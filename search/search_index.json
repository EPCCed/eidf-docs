{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EIDF User Documentation The Edinburgh International Data Facility (EIDF) is built and operated by EPCC at the University of Edinburgh . EIDF is a place to store, find and work with data of all kinds. You can find more information on the service and the research it supports on the EIDF website . For more information or for support with our services, please email eidf@epcc.ed.ac.uk in the first instance. What the documentation covers This documentation is in development and content will be added soon. Contributing to the documentation The source for this documentation is publicly available in the EIDF documentation Github repository so that anyone can contribute to improve the documentation for the service. Contributions can be in the form of improvements or additions to the content and/or addition of Issues providing suggestions for how it can be improved. Full details of how to contribute can be found in the README.md file of the repository. Further additions to be added. Credits This documentation draws on the ARCHER2 National Supercomputing Service documentation .","title":"Documentation overview"},{"location":"#eidf-user-documentation","text":"The Edinburgh International Data Facility (EIDF) is built and operated by EPCC at the University of Edinburgh . EIDF is a place to store, find and work with data of all kinds. You can find more information on the service and the research it supports on the EIDF website . For more information or for support with our services, please email eidf@epcc.ed.ac.uk in the first instance.","title":"EIDF User Documentation"},{"location":"#what-the-documentation-covers","text":"This documentation is in development and content will be added soon.","title":"What the documentation covers"},{"location":"#contributing-to-the-documentation","text":"The source for this documentation is publicly available in the EIDF documentation Github repository so that anyone can contribute to improve the documentation for the service. Contributions can be in the form of improvements or additions to the content and/or addition of Issues providing suggestions for how it can be improved. Full details of how to contribute can be found in the README.md file of the repository. Further additions to be added.","title":"Contributing to the documentation"},{"location":"#credits","text":"This documentation draws on the ARCHER2 National Supercomputing Service documentation .","title":"Credits"},{"location":"access/","text":"Accessing EIDF How to Access EIDF","title":"Access Overview"},{"location":"access/#accessing-eidf","text":"How to Access EIDF","title":"Accessing EIDF"},{"location":"access/virtualmachines-vdi/","text":".borderimg1 { border: 5px solid transparent; padding: 5px; /*margin: 15px;*/ border-color: rgba(192, 192, 192, 0.1); border-radius: 10px; } .bold { font-weight: bold; color: blue; } Virtual Machines (VMs) and the EIDF Virtual Desktop Interface (VDI) Using the EIDF VDI, members of EIDF projects can connect to VMs that they have been granted access to. The EIDF VDI is a web portal that displays the connections to VMs a user has available to them, and then those connections can be easily initiated by clicking on them in the user interface. Once connected to the target VM, all interactions are mediated through the user's web browser by the EIDF VDI. Accessing In order to access the EIDF VDI and connect to EIDF data science cloud VMs, you need to have an active SAFE account. If you already have a SAFE account, you can skip ahead to the Request Project Membership instructions. Otherwise, follow the Register Account in EPCC SAFE instructions immediately below to create the account. Register Account in EPCC SAFE Go to SAFE signup and complete the registration form Mandatory fields are: Email, Nationality, First name, Last name, Institution for reporting, Department, and Gender Your Email should be the one you used to register for the EIDF service (or Ed-DaSH workshop) Institution for reporting should always be 'University of Edinburgh' Department should always be ' EIDF ' Submit the form, then accept the SAFE Acceptable Use policy on the next page After you have completed the registration form and accepted the policy, you will receive an email from support@archer2.ac.uk with a password reset URL Visit the link in the email and generate a new password, then submit the form You will now be logged into your new account in SAFE Request Project Membership While logged into SAFE, select the \u2018Request Access\u2019 menu item from the 'Projects' menu in the top menu bar This will open the 'Apply for project membership' page Enter the appropriate project ID into the \u2018Project\u2019 field and click the \u2018Next\u2019 button In the 'Access route' drop down field that appears, select 'Request membership' (not 'Request machine account') The project owner will then receive notification of the application and accept your request Navigating the EIDF VDI Connecting to a VM Further information","title":"Virtual Desktop Interface"},{"location":"access/virtualmachines-vdi/#virtual-machines-vms-and-the-eidf-virtual-desktop-interface-vdi","text":"Using the EIDF VDI, members of EIDF projects can connect to VMs that they have been granted access to. The EIDF VDI is a web portal that displays the connections to VMs a user has available to them, and then those connections can be easily initiated by clicking on them in the user interface. Once connected to the target VM, all interactions are mediated through the user's web browser by the EIDF VDI.","title":"Virtual Machines (VMs) and the EIDF Virtual Desktop Interface (VDI)"},{"location":"access/virtualmachines-vdi/#accessing","text":"In order to access the EIDF VDI and connect to EIDF data science cloud VMs, you need to have an active SAFE account. If you already have a SAFE account, you can skip ahead to the Request Project Membership instructions. Otherwise, follow the Register Account in EPCC SAFE instructions immediately below to create the account.","title":"Accessing"},{"location":"access/virtualmachines-vdi/#register-account-in-epcc-safe","text":"Go to SAFE signup and complete the registration form Mandatory fields are: Email, Nationality, First name, Last name, Institution for reporting, Department, and Gender Your Email should be the one you used to register for the EIDF service (or Ed-DaSH workshop) Institution for reporting should always be 'University of Edinburgh' Department should always be ' EIDF ' Submit the form, then accept the SAFE Acceptable Use policy on the next page After you have completed the registration form and accepted the policy, you will receive an email from support@archer2.ac.uk with a password reset URL Visit the link in the email and generate a new password, then submit the form You will now be logged into your new account in SAFE","title":"Register Account in EPCC SAFE"},{"location":"access/virtualmachines-vdi/#request-project-membership","text":"While logged into SAFE, select the \u2018Request Access\u2019 menu item from the 'Projects' menu in the top menu bar This will open the 'Apply for project membership' page Enter the appropriate project ID into the \u2018Project\u2019 field and click the \u2018Next\u2019 button In the 'Access route' drop down field that appears, select 'Request membership' (not 'Request machine account') The project owner will then receive notification of the application and accept your request","title":"Request Project Membership"},{"location":"access/virtualmachines-vdi/#navigating-the-eidf-vdi","text":"","title":"Navigating the EIDF VDI"},{"location":"access/virtualmachines-vdi/#connecting-to-a-vm","text":"","title":"Connecting to a VM"},{"location":"access/virtualmachines-vdi/#further-information","text":"","title":"Further information"},{"location":"faq/","text":"FAQ EIDF Frequently Asked Questions","title":"EIDF Frequently Asked Questions"},{"location":"faq/#faq","text":"EIDF Frequently Asked Questions","title":"FAQ"},{"location":"known-issues/","text":"Known Issues EIDF Known Issues","title":"EIDF Known Issues"},{"location":"known-issues/#known-issues","text":"EIDF Known Issues","title":"Known Issues"},{"location":"overview/","text":"A Unique Service for Academia and Industry Built and operated by EPCC at the University of Edinburgh, EIDF is a place to store, find and work with data of all kinds. What is the Edinburgh International Data Facility? The Edinburgh International Data Facility (EIDF) supports learners, researchers and innovators across the spectrum, with services from basic data download, through simple learn-as-you-play-with-data notebooks, to GPU-enabled machine-learning platforms for driving AI application development. Most users of the EIDF work in the Data Service Cloud, which offers a rich set of data science and analytics tools: from browser-based notebooks to full desktop environments. The Data Service Cloud sits on top of an Analytics-Ready Data Layer (ARD Layer), where EIDF data can be shared and re-used for science and innovation. This ARD Layer will grow over time as more and more data are collected in the EIDF. Innovators and researchers looking for data can search and browse through the Data Catalogue to discover just what analytics-ready data EIDF has, and how they can get access. EIDF data managers work with data depositors at the Data Ingest Gateway, ensuring that incoming data are safely stored in the Data Lake Archive Layer, and well-described in the Data Catalogue. Data in the Data Lake are stored for the long term using best practices in digital preservation. EIDF data wranglers work in the Data Preparation Layer, often in collaboration with data depositors and others, to turn archived data from the Data Lake into analytics-ready data products in the ARD Layer. They are then ready for data innovators to create new, exciting datasets that can be stored and shared all over again. Safe Haven Services EIDF provides Safe Haven services to health and government users, following best practice in independent governance and supporting the linkage of complex personal data for public benefit research and policy making under national and regional safeguards. Safe Haven services can also be created for organisations wishing to host and govern access to their data assets in a highly secure environment. Safe Havens are isolated from the rest of EIDF, with user approvals, data ingress and egress, and permitted software all controlled by information governance bodies independent of the infrastructure itself. EIDF and the Data-Driven Innovation Initiative Launched at the end of 2018, the Data-Driven Innovation (DDI) initiative is one of six funded within the Edinburgh & South-East Scotland City Region Deal. The DDI initiative aims to make Edinburgh the \u201cData Capital of Europe\u201d, with ambitious targets to support, enhance and improve talent, research, commercial adoption and entrepreneurship across the region through better use of data. The initiative targets ten industry sectors, with interactions managed through five DDI Hubs: the Bayes Centre, the Usher Institute, Edinburgh Futures Institute, the National Robotarium, and Easter Bush. The activities of these Hubs are underpinned by EIDF.","title":"Introduction"},{"location":"overview/#a-unique-service-for-academia-and-industry","text":"Built and operated by EPCC at the University of Edinburgh, EIDF is a place to store, find and work with data of all kinds.","title":"A Unique Service for Academia and Industry"},{"location":"overview/#what-is-the-edinburgh-international-data-facility","text":"The Edinburgh International Data Facility (EIDF) supports learners, researchers and innovators across the spectrum, with services from basic data download, through simple learn-as-you-play-with-data notebooks, to GPU-enabled machine-learning platforms for driving AI application development. Most users of the EIDF work in the Data Service Cloud, which offers a rich set of data science and analytics tools: from browser-based notebooks to full desktop environments. The Data Service Cloud sits on top of an Analytics-Ready Data Layer (ARD Layer), where EIDF data can be shared and re-used for science and innovation. This ARD Layer will grow over time as more and more data are collected in the EIDF. Innovators and researchers looking for data can search and browse through the Data Catalogue to discover just what analytics-ready data EIDF has, and how they can get access. EIDF data managers work with data depositors at the Data Ingest Gateway, ensuring that incoming data are safely stored in the Data Lake Archive Layer, and well-described in the Data Catalogue. Data in the Data Lake are stored for the long term using best practices in digital preservation. EIDF data wranglers work in the Data Preparation Layer, often in collaboration with data depositors and others, to turn archived data from the Data Lake into analytics-ready data products in the ARD Layer. They are then ready for data innovators to create new, exciting datasets that can be stored and shared all over again.","title":"What is the Edinburgh International Data Facility?"},{"location":"overview/#safe-haven-services","text":"EIDF provides Safe Haven services to health and government users, following best practice in independent governance and supporting the linkage of complex personal data for public benefit research and policy making under national and regional safeguards. Safe Haven services can also be created for organisations wishing to host and govern access to their data assets in a highly secure environment. Safe Havens are isolated from the rest of EIDF, with user approvals, data ingress and egress, and permitted software all controlled by information governance bodies independent of the infrastructure itself.","title":"Safe Haven Services"},{"location":"overview/#eidf-and-the-data-driven-innovation-initiative","text":"Launched at the end of 2018, the Data-Driven Innovation (DDI) initiative is one of six funded within the Edinburgh & South-East Scotland City Region Deal. The DDI initiative aims to make Edinburgh the \u201cData Capital of Europe\u201d, with ambitious targets to support, enhance and improve talent, research, commercial adoption and entrepreneurship across the region through better use of data. The initiative targets ten industry sectors, with interactions managed through five DDI Hubs: the Bayes Centre, the Usher Institute, Edinburgh Futures Institute, the National Robotarium, and Easter Bush. The activities of these Hubs are underpinned by EIDF.","title":"EIDF and the Data-Driven Innovation Initiative"},{"location":"overview/acknowledgements/","text":"Acknowledgments Acknowledging EIDF Request Acknowledgment from EIDF","title":"Acknowledgements"},{"location":"overview/acknowledgements/#acknowledgments","text":"","title":"Acknowledgments"},{"location":"overview/acknowledgements/#acknowledging-eidf","text":"","title":"Acknowledging EIDF"},{"location":"overview/acknowledgements/#request-acknowledgment-from-eidf","text":"","title":"Request Acknowledgment from EIDF"},{"location":"overview/contacts/","text":"Contact The Edinburgh International Data Facility is located at the Advanced Computing Facility of EPCC, the supercomputing centre based at the University of Edinburgh. Email us Email EIDF: eidf@epcc.ed.ac.uk Sign up Join our mailing list to receive updates about EIDF.","title":"Contact"},{"location":"overview/contacts/#contact","text":"The Edinburgh International Data Facility is located at the Advanced Computing Facility of EPCC, the supercomputing centre based at the University of Edinburgh.","title":"Contact"},{"location":"overview/contacts/#email-us","text":"Email EIDF: eidf@epcc.ed.ac.uk","title":"Email us"},{"location":"overview/contacts/#sign-up","text":"Join our mailing list to receive updates about EIDF.","title":"Sign up"},{"location":"services/cs1/access/","text":"Cerebras CS-1 Getting Access Access to the Cerebras CS-1 system is currently by arrangement with EPCC. Please email eidf@epcc.ed.ac.uk with a short description of the work you would like to perform.","title":"Get Access"},{"location":"services/cs1/access/#cerebras-cs-1","text":"","title":"Cerebras CS-1"},{"location":"services/cs1/access/#getting-access","text":"Access to the Cerebras CS-1 system is currently by arrangement with EPCC. Please email eidf@epcc.ed.ac.uk with a short description of the work you would like to perform.","title":"Getting Access"},{"location":"services/cs1/run/","text":"Cerebras CS-1 Introduction The Cerebras CS-1 system is attached to the SDF-CS1 (Ultra2) system which serves as a host, provides access to files, the SLURM batch system etc. Login To login to the host system, use the username and password you obtain from SAFE , along with the SSH Key you registered when creating the account. You can then login directly to the host via: ssh <username>@sdf-cs1.epcc.ed.ac.uk Running Jobs All jobs must be run via SLURM to avoid inconveniencing other users of the system. The csrun_cpu and csrun_wse scripts themselves contain calls to srun to work with the SLURM system, so note the omission of srun in the below examples. Users can either copy these files from /home/y26/shared/bin to their own home directory should they wish, or use the centrally supplied vesion. In either case, ensure they are in your PATH before execution, eg: export PATH = $PATH :/home/y26/shared/bin Run on the host Jobs can be run on the host system (eg simulations, test scripts) using the csrun_cpu wrapper. Here is the example from the Cerebras documentation on PyTorch. Note that this assumes csrun_cpu is in your path. #!/bin/bash #SBATCH --job-name=Example # Job name #SBATCH --cpus-per-task=2 # Request 2 cores #SBATCH --output=example_%j.log # Standard output and error log csrun_cpu python-pt run.py --mode train --compile_only --params configs/<name-of-the-params-file.yaml> Run on the CS-1 The following will run the above PyTorch example on the CS-1 - note the --cs_ip argument with port number passed in via the command line, and the inclusion of the --gres option to request use of the CS-1 via SLURM. #!/bin/bash #SBATCH --job-name=Example # Job name #SBATCH --tasks-per-node=8 # There is only one node on SDF-CS1 #SBATCH --cpus-per-task=16 # Each cpu is a core #SBATCH --gres=cs:1 # Request CS-1 system #SBATCH --output=example_%j.log # Standard output and error log csrun_wse python-pt run.py --mode train --cs_ip 172 .24.102.121:9000 --params configs/<name-of-the-params-file.yaml>","title":"Running codes"},{"location":"services/cs1/run/#cerebras-cs-1","text":"","title":"Cerebras CS-1"},{"location":"services/cs1/run/#introduction","text":"The Cerebras CS-1 system is attached to the SDF-CS1 (Ultra2) system which serves as a host, provides access to files, the SLURM batch system etc.","title":"Introduction"},{"location":"services/cs1/run/#login","text":"To login to the host system, use the username and password you obtain from SAFE , along with the SSH Key you registered when creating the account. You can then login directly to the host via: ssh <username>@sdf-cs1.epcc.ed.ac.uk","title":"Login"},{"location":"services/cs1/run/#running-jobs","text":"All jobs must be run via SLURM to avoid inconveniencing other users of the system. The csrun_cpu and csrun_wse scripts themselves contain calls to srun to work with the SLURM system, so note the omission of srun in the below examples. Users can either copy these files from /home/y26/shared/bin to their own home directory should they wish, or use the centrally supplied vesion. In either case, ensure they are in your PATH before execution, eg: export PATH = $PATH :/home/y26/shared/bin","title":"Running Jobs"},{"location":"services/cs1/run/#run-on-the-host","text":"Jobs can be run on the host system (eg simulations, test scripts) using the csrun_cpu wrapper. Here is the example from the Cerebras documentation on PyTorch. Note that this assumes csrun_cpu is in your path. #!/bin/bash #SBATCH --job-name=Example # Job name #SBATCH --cpus-per-task=2 # Request 2 cores #SBATCH --output=example_%j.log # Standard output and error log csrun_cpu python-pt run.py --mode train --compile_only --params configs/<name-of-the-params-file.yaml>","title":"Run on the host"},{"location":"services/cs1/run/#run-on-the-cs-1","text":"The following will run the above PyTorch example on the CS-1 - note the --cs_ip argument with port number passed in via the command line, and the inclusion of the --gres option to request use of the CS-1 via SLURM. #!/bin/bash #SBATCH --job-name=Example # Job name #SBATCH --tasks-per-node=8 # There is only one node on SDF-CS1 #SBATCH --cpus-per-task=16 # Each cpu is a core #SBATCH --gres=cs:1 # Request CS-1 system #SBATCH --output=example_%j.log # Standard output and error log csrun_wse python-pt run.py --mode train --cs_ip 172 .24.102.121:9000 --params configs/<name-of-the-params-file.yaml>","title":"Run on the CS-1"},{"location":"services/datacatalogue/docs/","text":"Service Documentation Metadata For more information on metadata, please read the following: Metadata Online support","title":"Documentation"},{"location":"services/datacatalogue/docs/#service-documentation","text":"","title":"Service Documentation"},{"location":"services/datacatalogue/docs/#metadata","text":"For more information on metadata, please read the following: Metadata","title":"Metadata"},{"location":"services/datacatalogue/docs/#online-support","text":"","title":"Online support"},{"location":"services/datacatalogue/metadata/","text":"EIDF Metadata Information What is FAIR? FAIR stands for Findable, Accessible, Interoperable, and Reusable, and helps emphasise the best practices with publishing and sharing data (more details: FAIR Principles ) What is metadata? Metadata is data about data, to help describe the dataset. Common metadata fields are things like the title of the dataset, who produced it, where it was generated (if relevant), when it was generated, and some key words describing it What is CKAN? CKAN is a metadata catalogue - i.e. it is a database for metadata rather than data. This will help with all aspects of FAIR: it will be a signposting portal for where the data actually resides it will ensure that at least metadata (even if not the data) is in a format which is easily retrievable via an identifier the metadata (and hopefully data) use terms from vocabularies that are widely recognised in the relevant field the metadata has lots of attributes to help others use it, and there are clear licence conditions where necessary What metadata will we need to provide? the title of the dataset; if a short title is not particularly descriptive, then please add a longer, separate, description too. the name of the person who created the dataset if it has spatial relevance, the latitude and longitude of the location where the dataset was generated, if possible (e.g. if a sensor has collected data, then it should be straightforward to know the lat and long) the temporal period that the dataset covers it is important to standardise the licencing for all data and we will use Creative Commons 4.0 by default. If you want a different licence, please come and talk to us. If the dataset is from a third party, you must tell us the licence of that dataset As well as the theme you've picked for your WP directory, you can add other themes in the metadata file. For example, you might have decided your WP theme is geophysics, but a dataset is also related to geodesy. Again, please check that this term is in the FAST vocabulary. if there is likely to be more than 1 way that the data could be made available (e.g. netCDF and csv) Why do I need to use a controlled vocabulary? Using a standard vocabulary (such as the FAST Vocabulary) has many benefits: the terms are managed by an external body the hierarchy has been agreed (e.g. you will see for that for geophysics, it has \"skos broader\" topics of \"physics\" and \"earth sciences\", which I hope you agree with! Don't worry what \"skos\" means) using controlled vocabularies means that everybody who uses it knows they are using the same definitions as everybody else using it the vocabulary is updated at given intervals All of these advantages mean that we, as a project, don't need to think about this - there is no need to reinvent the wheel when other institutes (e.g. National Libraries) have created. You might recognise WorldCat - it is an organisation which manages a global catalogue of ~18000 libraries world-wide, so they are in a good position to generate a comprehensive vocabulary of academic topics! What about licensing? (What does CC-BY-SA 4.0 mean?) The R in FAIR stands for reusable - more specifically it includes this subphrase: \"(Meta)data are released with a clear and accessible data usage license\". This means that we have to tell anyone else who uses the data what they're allowed to do with it - and, under the FAIR philosophy, more freedom is better. CC-BY-SA 4.0 allows anyone to remix, adapt, and build upon your work (even for commercial purposes), as long as they credit you and license their new creations under the identical terms. It also explicitly includes Sui Generis Database Rights, giving rights to the curation of a database even if you don't have the rights to the items in a database (e.g. a Spotify playlist, even though you don't own the rights to each track). Human readable summary: Creative Commons 4.0 Human Readable Full legal code: Creative Commons 4.0 Legal Code I'm stuck! How do I get help? Contact the EIDF Service Team via eidf@epcc.ed.ac.uk","title":"Metadata information"},{"location":"services/datacatalogue/metadata/#eidf-metadata-information","text":"","title":"EIDF Metadata Information"},{"location":"services/datacatalogue/metadata/#what-is-fair","text":"FAIR stands for Findable, Accessible, Interoperable, and Reusable, and helps emphasise the best practices with publishing and sharing data (more details: FAIR Principles )","title":"What is FAIR?"},{"location":"services/datacatalogue/metadata/#what-is-metadata","text":"Metadata is data about data, to help describe the dataset. Common metadata fields are things like the title of the dataset, who produced it, where it was generated (if relevant), when it was generated, and some key words describing it","title":"What is metadata?"},{"location":"services/datacatalogue/metadata/#what-is-ckan","text":"CKAN is a metadata catalogue - i.e. it is a database for metadata rather than data. This will help with all aspects of FAIR: it will be a signposting portal for where the data actually resides it will ensure that at least metadata (even if not the data) is in a format which is easily retrievable via an identifier the metadata (and hopefully data) use terms from vocabularies that are widely recognised in the relevant field the metadata has lots of attributes to help others use it, and there are clear licence conditions where necessary","title":"What is CKAN?"},{"location":"services/datacatalogue/metadata/#what-metadata-will-we-need-to-provide","text":"the title of the dataset; if a short title is not particularly descriptive, then please add a longer, separate, description too. the name of the person who created the dataset if it has spatial relevance, the latitude and longitude of the location where the dataset was generated, if possible (e.g. if a sensor has collected data, then it should be straightforward to know the lat and long) the temporal period that the dataset covers it is important to standardise the licencing for all data and we will use Creative Commons 4.0 by default. If you want a different licence, please come and talk to us. If the dataset is from a third party, you must tell us the licence of that dataset As well as the theme you've picked for your WP directory, you can add other themes in the metadata file. For example, you might have decided your WP theme is geophysics, but a dataset is also related to geodesy. Again, please check that this term is in the FAST vocabulary. if there is likely to be more than 1 way that the data could be made available (e.g. netCDF and csv)","title":"What metadata will we need to provide?"},{"location":"services/datacatalogue/metadata/#why-do-i-need-to-use-a-controlled-vocabulary","text":"Using a standard vocabulary (such as the FAST Vocabulary) has many benefits: the terms are managed by an external body the hierarchy has been agreed (e.g. you will see for that for geophysics, it has \"skos broader\" topics of \"physics\" and \"earth sciences\", which I hope you agree with! Don't worry what \"skos\" means) using controlled vocabularies means that everybody who uses it knows they are using the same definitions as everybody else using it the vocabulary is updated at given intervals All of these advantages mean that we, as a project, don't need to think about this - there is no need to reinvent the wheel when other institutes (e.g. National Libraries) have created. You might recognise WorldCat - it is an organisation which manages a global catalogue of ~18000 libraries world-wide, so they are in a good position to generate a comprehensive vocabulary of academic topics!","title":"Why do I need to use a controlled vocabulary?"},{"location":"services/datacatalogue/metadata/#what-about-licensing-what-does-cc-by-sa-40-mean","text":"The R in FAIR stands for reusable - more specifically it includes this subphrase: \"(Meta)data are released with a clear and accessible data usage license\". This means that we have to tell anyone else who uses the data what they're allowed to do with it - and, under the FAIR philosophy, more freedom is better. CC-BY-SA 4.0 allows anyone to remix, adapt, and build upon your work (even for commercial purposes), as long as they credit you and license their new creations under the identical terms. It also explicitly includes Sui Generis Database Rights, giving rights to the curation of a database even if you don't have the rights to the items in a database (e.g. a Spotify playlist, even though you don't own the rights to each track). Human readable summary: Creative Commons 4.0 Human Readable Full legal code: Creative Commons 4.0 Legal Code","title":"What about licensing? (What does CC-BY-SA 4.0 mean?)"},{"location":"services/datacatalogue/metadata/#im-stuck-how-do-i-get-help","text":"Contact the EIDF Service Team via eidf@epcc.ed.ac.uk","title":"I'm stuck! How do I get help?"},{"location":"services/datacatalogue/quickstart/","text":"Quickstart Accessing First Task Further information","title":"QuickStart"},{"location":"services/datacatalogue/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"services/datacatalogue/quickstart/#accessing","text":"","title":"Accessing"},{"location":"services/datacatalogue/quickstart/#first-task","text":"","title":"First Task"},{"location":"services/datacatalogue/quickstart/#further-information","text":"","title":"Further information"},{"location":"services/datacatalogue/tutorial/","text":"Tutorial First Query","title":"Tutorial"},{"location":"services/datacatalogue/tutorial/#tutorial","text":"","title":"Tutorial"},{"location":"services/datacatalogue/tutorial/#first-query","text":"","title":"First Query"},{"location":"services/jhub/docs/","text":"Service Documentation Online support","title":"Documentation"},{"location":"services/jhub/docs/#service-documentation","text":"","title":"Service Documentation"},{"location":"services/jhub/docs/#online-support","text":"","title":"Online support"},{"location":"services/jhub/quickstart/","text":"Quickstart Accessing First Task Further information","title":"QuickStart"},{"location":"services/jhub/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"services/jhub/quickstart/#accessing","text":"","title":"Accessing"},{"location":"services/jhub/quickstart/#first-task","text":"","title":"First Task"},{"location":"services/jhub/quickstart/#further-information","text":"","title":"Further information"},{"location":"services/jhub/tutorial/","text":"Tutorial First notebook","title":"Tutorial"},{"location":"services/jhub/tutorial/#tutorial","text":"","title":"Tutorial"},{"location":"services/jhub/tutorial/#first-notebook","text":"","title":"First notebook"},{"location":"services/rstudioserver/docs/","text":"Service Documentation Online support","title":"Documentation"},{"location":"services/rstudioserver/docs/#service-documentation","text":"","title":"Service Documentation"},{"location":"services/rstudioserver/docs/#online-support","text":"","title":"Online support"},{"location":"services/rstudioserver/quickstart/","text":"Quickstart Accessing First Task Further information","title":"QuickStart"},{"location":"services/rstudioserver/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"services/rstudioserver/quickstart/#accessing","text":"","title":"Accessing"},{"location":"services/rstudioserver/quickstart/#first-task","text":"","title":"First Task"},{"location":"services/rstudioserver/quickstart/#further-information","text":"","title":"Further information"},{"location":"services/rstudioserver/tutorial/","text":"Tutorial First notebook","title":"Tutorial"},{"location":"services/rstudioserver/tutorial/#tutorial","text":"","title":"Tutorial"},{"location":"services/rstudioserver/tutorial/#first-notebook","text":"","title":"First notebook"},{"location":"services/ultra2/access/","text":"Ultra2 Large Memory System Getting Access Access to the Ultra2 system (also referred to as the SDF-CS1 system) is currently by arrangement with EPCC. Please email eidf@epcc.ed.ac.uk with a short description of the work you would like to perform.","title":"Get Access"},{"location":"services/ultra2/access/#ultra2-large-memory-system","text":"","title":"Ultra2 Large Memory System"},{"location":"services/ultra2/access/#getting-access","text":"Access to the Ultra2 system (also referred to as the SDF-CS1 system) is currently by arrangement with EPCC. Please email eidf@epcc.ed.ac.uk with a short description of the work you would like to perform.","title":"Getting Access"},{"location":"services/ultra2/run/","text":"Ultra2 High Memory System Introduction The Ultra2 system (also called the SDF-CS1) system, is a single logical CPU system based at EPCC. It is suitable for running jobs which require large volumes of non-distributed memory (as opposed to a cluster). Specifications The system is a HPE SuperDome Flex containing 576 individual cores in a SMT-1 arrangement (1 thread per core). The system has 18TB of memory available to users. Home direcories are network mounted from the EIDF e1000 Lustre filesystem, although some local NVMe storage is available for temporary file storage during runs. Login To login to the host system, use the username and password you obtain from SAFE , along with the SSH Key you registered when creating the account. You can then login directly to the host via: ssh <username>@sdf-cs1.epcc.ed.ac.uk Software The primary software provided is Intel's OneAPI suite containing mpi compilers and runtimes, debuggers and the vTune performance analyser. Standard GNU compilers are also available. The OneAPI suite can be loaded by sourcing the shell script: source /opt/intel/oneapi/setvars.sh Running Jobs All jobs must be run via SLURM to avoid inconveniencing other users of the system. Users should not run jobs directly. Note that the system has one logical processor with a large number of threads and thus appears to SLURM as a single node. This is intentional. Queue limits We kindly request that users limit their maximum total running job size to 288 cores and 4TB of memory, whether that be a divided into a single job, or a number of jobs. This may be enforced via SLURM in the future. MPI jobs An example script to run a multi-process MPI \"Hello world\" example is shown. #!/usr/bin/env bash #SBATCH -J HelloWorld #SBATCH --nodes=1 #SBATCH --tasks-per-node=4 #SBATCH --nodelist=sdf-cs1 #SBATCH --partition=standard ##SBATCH --exclusive echo \"Running on host ${ HOSTNAME } \" echo \"Using ${ SLURM_NTASKS_PER_NODE } tasks per node\" echo \"Using ${ SLURM_CPUS_PER_TASK } cpus per task\" let mpi_threads = ${ SLURM_NTASKS_PER_NODE } * ${ SLURM_CPUS_PER_TASK } echo \"Using ${ mpi_threads } MPI threads\" # Source oneAPI to ensure mpirun available if [[ -z \" ${ SETVARS_COMPLETED } \" ]] ; then source /opt/intel/oneapi/setvars.sh fi # mpirun invocation for Intel suite. mpirun -n ${ mpi_threads } ./helloworld.exe","title":"Running codes"},{"location":"services/ultra2/run/#ultra2-high-memory-system","text":"","title":"Ultra2 High Memory System"},{"location":"services/ultra2/run/#introduction","text":"The Ultra2 system (also called the SDF-CS1) system, is a single logical CPU system based at EPCC. It is suitable for running jobs which require large volumes of non-distributed memory (as opposed to a cluster).","title":"Introduction"},{"location":"services/ultra2/run/#specifications","text":"The system is a HPE SuperDome Flex containing 576 individual cores in a SMT-1 arrangement (1 thread per core). The system has 18TB of memory available to users. Home direcories are network mounted from the EIDF e1000 Lustre filesystem, although some local NVMe storage is available for temporary file storage during runs.","title":"Specifications"},{"location":"services/ultra2/run/#login","text":"To login to the host system, use the username and password you obtain from SAFE , along with the SSH Key you registered when creating the account. You can then login directly to the host via: ssh <username>@sdf-cs1.epcc.ed.ac.uk","title":"Login"},{"location":"services/ultra2/run/#software","text":"The primary software provided is Intel's OneAPI suite containing mpi compilers and runtimes, debuggers and the vTune performance analyser. Standard GNU compilers are also available. The OneAPI suite can be loaded by sourcing the shell script: source /opt/intel/oneapi/setvars.sh","title":"Software"},{"location":"services/ultra2/run/#running-jobs","text":"All jobs must be run via SLURM to avoid inconveniencing other users of the system. Users should not run jobs directly. Note that the system has one logical processor with a large number of threads and thus appears to SLURM as a single node. This is intentional.","title":"Running Jobs"},{"location":"services/ultra2/run/#queue-limits","text":"We kindly request that users limit their maximum total running job size to 288 cores and 4TB of memory, whether that be a divided into a single job, or a number of jobs. This may be enforced via SLURM in the future.","title":"Queue limits"},{"location":"services/ultra2/run/#mpi-jobs","text":"An example script to run a multi-process MPI \"Hello world\" example is shown. #!/usr/bin/env bash #SBATCH -J HelloWorld #SBATCH --nodes=1 #SBATCH --tasks-per-node=4 #SBATCH --nodelist=sdf-cs1 #SBATCH --partition=standard ##SBATCH --exclusive echo \"Running on host ${ HOSTNAME } \" echo \"Using ${ SLURM_NTASKS_PER_NODE } tasks per node\" echo \"Using ${ SLURM_CPUS_PER_TASK } cpus per task\" let mpi_threads = ${ SLURM_NTASKS_PER_NODE } * ${ SLURM_CPUS_PER_TASK } echo \"Using ${ mpi_threads } MPI threads\" # Source oneAPI to ensure mpirun available if [[ -z \" ${ SETVARS_COMPLETED } \" ]] ; then source /opt/intel/oneapi/setvars.sh fi # mpirun invocation for Intel suite. mpirun -n ${ mpi_threads } ./helloworld.exe","title":"MPI jobs"},{"location":"services/virtualmachines/docs/","text":"Service Documentation Online support","title":"Documentation"},{"location":"services/virtualmachines/docs/#service-documentation","text":"","title":"Service Documentation"},{"location":"services/virtualmachines/docs/#online-support","text":"","title":"Online support"},{"location":"services/virtualmachines/quickstart/","text":"Quickstart Accessing First VM Further information","title":"QuickStart"},{"location":"services/virtualmachines/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"services/virtualmachines/quickstart/#accessing","text":"","title":"Accessing"},{"location":"services/virtualmachines/quickstart/#first-vm","text":"","title":"First VM"},{"location":"services/virtualmachines/quickstart/#further-information","text":"","title":"Further information"},{"location":"services/virtualmachines/tutorial/","text":"Tutorial First VM","title":"Tutorial"},{"location":"services/virtualmachines/tutorial/#tutorial","text":"","title":"Tutorial"},{"location":"services/virtualmachines/tutorial/#first-vm","text":"","title":"First VM"},{"location":"status/","text":"Known Issues EIDF Service Status The below represents the broad status of each service. Service Status Notes OpenStack SSH Gateway UP Beta Test, not generally available OpenStack VDI Gateway UP Beta Test, not generally available OpenStack Platform UP Beta Test, not generally available Cerebras CS-1 UP Available for use SuperDome Flex (SDF-CS1 / Ultra2) UP Available for use","title":"Service Status"},{"location":"status/#known-issues","text":"EIDF Service Status The below represents the broad status of each service. Service Status Notes OpenStack SSH Gateway UP Beta Test, not generally available OpenStack VDI Gateway UP Beta Test, not generally available OpenStack Platform UP Beta Test, not generally available Cerebras CS-1 UP Available for use SuperDome Flex (SDF-CS1 / Ultra2) UP Available for use","title":"Known Issues"}]}