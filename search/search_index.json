{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EIDF User Documentation","text":"<p>The Edinburgh International Data Facility (EIDF) is built and operated by EPCC at the University of Edinburgh. EIDF is a place to store, find and work with data of all kinds. You can find more information on the service and the research it supports on the EIDF website.</p> <p>If you are an existing user of EIDF or other EPCC systems then please submit your queries via our EIDF Helpdesk otherwise send your query by email to eidf@epcc.ed.ac.uk.</p>"},{"location":"#what-the-documentation-covers","title":"What the documentation covers","text":"<p>This documentation gives more in-depth coverage of current EIDF services. It is aimed primarily at developers or power users.</p>"},{"location":"#contributing-to-the-documentation","title":"Contributing to the documentation","text":"<p>The source for this documentation is publicly available in the EIDF documentation Github repository so that anyone can contribute to improve the documentation for the service. Contributions can be in the form of improvements or additions to the content and/or addition of Issues providing suggestions for how it can be improved.</p> <p>Full details of how to contribute can be found in the <code>README.md</code> file of the repository.</p> <p>This documentation set is a work in progress.</p>"},{"location":"#credits","title":"Credits","text":"<p>This documentation draws on the ARCHER2 National Supercomputing Service documentation.</p>"},{"location":"access/","title":"Accessing EIDF","text":"<p>Some EIDF services are accessed via a Web browser and some by \"traditional\" command-line <code>ssh</code>.</p> <p>All EIDF services use the EPCC SAFE service management back end, to ensure compatibility with other EPCC high-performance computing services.</p>"},{"location":"access/#web-access-to-virtual-machines","title":"Web Access to Virtual Machines","text":"<p>The Virtual Desktop VM service is browser-based, providing a virtual desktop interface (Apache Guacamole) for \"desktop-in-a-browser\" access. Applications to use the VM service are made through the EIDF Portal.</p> <p>EIDF Portal: how to ask to join an existing EIDF project and how to apply for a new project</p> <p>VDI access to virtual machines: how to connect to the virtual desktop interface.</p>"},{"location":"access/#ssh-access-to-virtual-machines","title":"SSH Access to Virtual Machines","text":"<p>Users with the appropriate permissions can also use <code>ssh</code> to login to Virtual Desktop VMs</p>"},{"location":"access/#ssh-access-to-computing-services","title":"SSH Access to Computing Services","text":"<p>Includes access to the following services:</p> <ul> <li>Ultra2</li> <li>EIDF Interim and Experimental Systems</li> </ul> <p>To login to most command-line services with <code>ssh</code> you should use the username and password you obtained from SAFE when you applied for access, along with the SSH Key you registered when creating the account. You can then login to the host following the appropriately linked instructions above.</p>"},{"location":"access/project/","title":"EIDF Portal","text":"<p>Projects using the Virtual Desktop cloud service are accessed via the EIDF Portal.</p> <p>The EIDF Portal uses EPCC's SAFE service management software to manage user accounts across all EPCC services. To log in to the Portal you will first be redirected to the SAFE log on page.  If you do not have a SAFE account follow the instructions in the SAFE documentation how to register and receive your password.</p>"},{"location":"access/project/#how-to-request-to-join-a-project","title":"How to request to join a project","text":"<p>Log in to the EIDF Portal and navigate to \"Projects\" and choose \"Request access\". Select the project that you want to join in the \"Project\" dropdown list - you can search for the project name or the project code, e.g. \"eidf0123\".</p> <p>Now you have to wait for your PI or project manager to accept your request to join.</p> <p>Once your request is accepted, the project manager creates an account for you and enables access to the machines that you require in your project. You cannot request accounts, this has to be done by your project manager. If you cannot see any accounts in the project after joining please get in touch with the PI or a project manager and ask them to create one for you.</p>"},{"location":"access/project/#how-to-apply-for-a-project-as-a-principal-investigator","title":"How to apply for a project as a Principal Investigator","text":""},{"location":"access/project/#create-a-new-project-application","title":"Create a new project application","text":"<p>Navigate to the EIDF Portal and log in via SAFE if necessary (see above).</p> <p>Once you have logged in click on \"Applications\" in the menu and choose \"New Application\".</p> <ol> <li>Fill in the Application Title - this will be the name of the project once it is approved.</li> <li>Choose a start date and an end date for your project.</li> <li>Select the services for which you are applying.</li> <li>Click \"Create\" to create your project application.</li> </ol> <p>Once the application has been created you see an overview of the form you are required to fill in. You can revisit the application at any time by clicking on \"Applications\" and choosing \"Your applications\" to display all your current and past applications and their status, or follow the link https://portal.eidf.ac.uk/proposal/.</p>"},{"location":"access/project/#populate-a-project-application","title":"Populate a project application","text":"<p>Fill in each section of the application as required:</p> <ul> <li>Case for Support</li> <li>EIDF Services Required</li> </ul> <p>You can edit and save each section separately and revisit the application at a later time.</p>"},{"location":"access/project/#editors","title":"Editors","text":"<p>You may invite other editors to your application. Editors must have a valid SAFE login and they will receive an email with your invitation and a link. After accepting, they have write access to all parts of the proposal. However, only the owner of the application has permission to submit the application.</p>"},{"location":"access/project/#datasets","title":"Datasets","text":"<p>You are required to fill in a \"Dataset\" form for each dataset that you are planning to store and process as part of your project.</p> <p>We are required to ensure that projects involving \"sensitive\" data have the necessary permissions in place. The answers to these questions will enable us to decide what additional documentation we may need, and whether your project may need to be set up in an independently governed Safe Haven. There may be some projects we are simply unable to host for data protection reasons.</p>"},{"location":"access/project/#virtual-desktop-requirements","title":"Virtual Desktop Requirements","text":"<p>Add an estimate for each size and type of VM that is required.</p>"},{"location":"access/project/#submission","title":"Submission","text":"<p>When you are happy with your application, click \"Submit\". If there are missing fields that are required these are highlighted and your submission will fail.</p> <p>When your submission was successful the application status is marked as \"Submitted\" and now you have to wait while the EIDF approval team considers your application. You may be contacted if there are any questions regarding your application or further information is required, and you will be notified of the outcome of your application.</p>"},{"location":"access/project/#approved-project","title":"Approved Project","text":"<p>If your application was approved, refer to Data Science Virtual Desktops: Quickstart how to view your project and to Data Science Virtual Desktops: Managing VMs how to manage a project and how to create virtual machines and user accounts.</p>"},{"location":"access/ssh/","title":"SSH Access to Virtual Machines using the EIDF-Gateway Jump Host","text":"<p>The EIDF-Gateway is an SSH gateway suitable for accessing EIDF Services via a console or terminal. As the gateway cannot be 'landed' on, a user can only pass through it and so the destination (the VM IP) has to be known for the service to work. Users connect to their VM through the jump host using their given accounts. You will require three things to use the gateway:</p> <ol> <li>A user within a project allowed to access the gateway and a password set.</li> <li>An SSH-key linked to this account, used to authenticate against the gateway.</li> <li>Have MFA setup with your project account via SAFE.</li> </ol> <p>Steps to meet all of these requirements are explained below.</p>"},{"location":"access/ssh/#generating-and-adding-an-ssh-key","title":"Generating and Adding an SSH Key","text":"<p>In order to make use of the EIDF-Gateway, your EIDF account needs an SSH-Key associated with it. If you added one while creating your EIDF account, you can skip this step.</p>"},{"location":"access/ssh/#check-for-an-existing-ssh-key","title":"Check for an existing SSH Key","text":"<p>To check if you have an SSH Key associated with your account:</p> <ol> <li>Login to the Portal</li> <li>Select 'Your Projects'</li> <li>Select your project name</li> <li>Select your username</li> </ol> <p>If there is an entry under 'Credentials', then you're all setup. If not, you'll need to generate an SSH-Key, to do this:</p>"},{"location":"access/ssh/#generate-a-new-ssh-key","title":"Generate a new SSH Key","text":"<ol> <li>Open a new window of whatever terminal you will use to SSH to EIDF.</li> <li> <p>Generate a new SSH Key:</p> <pre><code>ssh-keygen\n</code></pre> </li> <li> <p>It is fine to accept the default name and path for the key unless you manage a number of keys.</p> </li> <li>Press enter to finish generating the key</li> </ol>"},{"location":"access/ssh/#adding-the-new-ssh-key-to-your-account-via-the-portal","title":"Adding the new SSH Key to your account via the Portal","text":"<ol> <li>Login into the Portal</li> <li>Select 'Your Projects'</li> <li>Select the relevant project</li> <li>Select your username</li> <li>Select the plus button under  'Credentials'</li> <li>Select 'Choose File' to upload the PUBLIC (.pub) ssh key generated in the last step, or open the <code>&lt;ssh-key&gt;.pub</code> file you just created and copy its contents into the text box.</li> <li>Click 'Upload Credential' - it should look something like this:</li> </ol>"},{"location":"access/ssh/#adding-a-new-ssh-key-via-safe","title":"Adding a new SSH Key via SAFE","text":"<p>This should not be necessary for most users, so only follow this process if you have an issue or have been told to by the EPCC Helpdesk. If you need to add an SSH Key directly to SAFE, you can follow this guide. However, select your '[username]@EIDF' login account, not 'Archer2' as specified in that guide.</p>"},{"location":"access/ssh/#enabling-mfa-via-the-portal","title":"Enabling MFA via the Portal","text":"<p>A multi-factor Time-Based One-Time Password is now required to access the SSH Gateway.</p> <p>To enable this for your EIDF account:</p> <ol> <li>Login to the portal.</li> <li>Select 'Projects' then 'Your Projects'</li> <li>Select the project containing the account you'd like to add MFA to.</li> <li>Under 'Your Accounts', select the account you would like to add MFA to.</li> <li>Select 'Set MFA Token'</li> <li>Within your chosen MFA application, scan the QR Code or enter the key and add the token.</li> <li>Enter the code displayed in the app into the 'Verification Code' box and select 'Set Token'</li> <li>You will be redirected to the User Account page and a green 'Added MFA Token' message will confirm the token has been added successfully.</li> </ol> <p>Note</p> <p>TOTP is only required for the SSH Gateway, not to the VMs themselves, and not through the VDI. An MFA token will have to be set for each account you'd like to use to access the EIDF SSH Gateway.</p>"},{"location":"access/ssh/#using-the-ssh-key-and-totp-code-to-access-eidf-windows-and-linux","title":"Using the SSH-Key and TOTP Code to access EIDF - Windows and Linux","text":"<ol> <li> <p>From your local terminal, import the SSH Key you generated above:     <code>ssh-add /path/to/ssh-key</code></p> </li> <li> <p>This should return \"Identity added [Path to SSH Key]\" if successful. You can then follow the steps below to access your VM.</p> </li> </ol>"},{"location":"access/ssh/#accessing-from-macoslinux","title":"Accessing From MacOS/Linux","text":"<p>OpenSSH is installed on Linux and MacOS usually by default, so you can access the gateway natively from the terminal.</p> <p>Ensure you have created and added an ssh key as specified in the 'Generating and Adding an SSH Key' section above, then run the commands below:</p> <pre><code>ssh-add /path/to/ssh-key\nssh -J [username]@eidf-gateway.epcc.ed.ac.uk [username]@[vm_ip]\n</code></pre> <p>For example:</p> <pre><code>ssh-add ~/.ssh/keys/id_ed25519\nssh -J alice@eidf-gateway.epcc.ed.ac.uk alice@10.24.1.1\n</code></pre> <p>Info</p> <p>If the <code>ssh-add</code> command fails saying the SSH Agent is not running, run the below command:</p> <pre><code>eval `ssh-agent`\n</code></pre> <p>Then re-run the ssh-add command above.</p> <p>The <code>-J</code> flag is use to specify that we will access the second specified host by jumping through the first specified host.</p> <p>You will be prompted for a 'TOTP' code upon successful public key authentication to the gateway. At the TOTP prompt, enter the code displayed in your MFA Application.</p>"},{"location":"access/ssh/#accessing-from-windows","title":"Accessing from Windows","text":"<p>Windows will require the installation of OpenSSH-Server to use SSH. Putty or MobaXTerm can also be used but won\u2019t be covered in this tutorial.</p>"},{"location":"access/ssh/#installing-and-using-openssh","title":"Installing and using OpenSSH","text":"<ol> <li>Click the \u2018Start\u2019 button at the bottom of the screen</li> <li>Click the \u2018Settings\u2019 cog icon</li> <li>Select 'System'</li> <li>Select the \u2018Optional Features\u2019 option at the bottom of the list</li> <li>If \u2018OpenSSH Client\u2019 is not under \u2018Installed Features\u2019, click the \u2018View Features\u2019 button</li> <li>Search \u2018OpenSSH Client\u2019</li> <li>Select the check box next to \u2018OpenSSH Client\u2019 and click \u2018Install\u2019</li> </ol>"},{"location":"access/ssh/#accessing-eidf-via-a-terminal","title":"Accessing EIDF via a Terminal","text":"<ol> <li>Open either Powershell or the Windows Terminal</li> <li> <p>Import the SSH Key you generated above:</p> <pre><code>ssh-add \\path\\to\\sshkey\n</code></pre> <p>For Example:</p> <pre><code>ssh-add .\\.ssh\\id_ed25519\n</code></pre> </li> <li> <p>This should return \"Identity added [Path to SSH Key]\" if successful. If it doesn't, run the following in Powershell:</p> <pre><code>Get-Service -Name ssh-agent | Set-Service -StartupType Manual\nStart-Service ssh-agent\nssh-add \\path\\to\\sshkey\n</code></pre> </li> <li> <p>Login by jumping through the gateway.</p> <pre><code>ssh -J [EIDF username]@eidf-gateway.epcc.ed.ac.uk [EIDF username]@[vm_ip]\n</code></pre> <p>For Example:</p> <pre><code>ssh -J alice@eidf-gateway.epcc.ed.ac.uk alice@10.24.1.1\n</code></pre> </li> </ol> <p>You will be prompted for a 'TOTP' code upon successful public key authentication to the gateway. At the TOTP prompt, enter the code displayed in your MFA Application.</p>"},{"location":"access/ssh/#accessing-with-putty","title":"Accessing with PuTTY","text":""},{"location":"access/ssh/#creating-the-gateway-config","title":"Creating the Gateway Config","text":"<ol> <li>Install PuTTY v0.81 or greater and launch it</li> <li>From the default 'Session' menu, input the gateway URL: <code>eidf-gateway.epcc.ed.ac.uk</code></li> <li>Set the port to <code>22</code></li> <li>Naviagte to 'Connection' &gt; 'SSH' &gt; 'Auth' &gt; 'Credentials'</li> <li>Specify your gateway private key by selecting the 'Browse' button next to 'Private key file for authentication'</li> <li>Select the path to your SSH key for your gateway account</li> <li>Return to the main 'Session' tab and in the empty box under 'Saved Sessions' input a memorable name, for example 'eidf-gateway'</li> <li>Click Save</li> </ol>"},{"location":"access/ssh/#adding-a-clientvm-config","title":"Adding a client/VM config","text":"<p>Now we can create a config session for a VM which can be connected to through the gateway</p> <ol> <li>From the 'Sessions' tab, select 'Default Settings' in the 'Saved Settings' box and select 'Load'</li> <li>Input the hostname/IP of the VM, e.g. 10.24.X.Y.</li> <li>Ensure the port is <code>22</code></li> <li>From the navigation menu, move to 'Connection' &gt; 'Proxy'</li> <li>Select 'Proxy Type' at the top to 'SSH to proxy and use port forward'. Note this doesn't exist in older versions of PuTTY</li> <li>Under 'Proxy Hostname', set the name of the gateway session you created above. In the SOP this was called 'eidf-gateway'</li> <li>Set the Port to <code>22</code></li> <li>Put your eidf-gateway username into the 'Username' box</li> <li>Set 'Do DNS name lookup at proxy end' to 'Yes'</li> <li>Navigate to 'Connection' &gt; 'SSH' &gt; 'Auth' &gt; 'Credentials' and select 'Browse' next to the 'Private key file for authentication' box</li> <li>Select your private key for your VM account (this may be the same as the gateway)</li> <li>Navigate to 'Connection' &gt; 'Data'</li> <li>Put your VM username into the 'Auto-Login username' box</li> <li>Return to the original 'Sesstion' tab</li> <li>Put the VM name or a nickname into the 'Saved Settings' name box</li> <li>Select 'Save'</li> <li>Select 'Open' and you should connect to your VM.</li> </ol>"},{"location":"access/ssh/#ssh-aliases","title":"SSH Aliases","text":"<p>You can use SSH Aliases to access your VMs with a single command.</p> <ol> <li> <p>Create a new entry for the EIDF-Gateway in your ~/.ssh/config file. Using the text editor of your choice (vi used as an example), edit the .ssh/config file:</p> <pre><code>vi ~/.ssh/config\n</code></pre> </li> <li> <p>Insert the following lines:</p> <pre><code>Host eidf-gateway\n  Hostname eidf-gateway.epcc.ed.ac.uk\n  User &lt;eidf project username&gt;\n  IdentityFile /path/to/ssh/key\n</code></pre> <p>For example:</p> <pre><code>Host eidf-gateway\n  Hostname eidf-gateway.epcc.ed.ac.uk\n  User alice\n  IdentityFile ~/.ssh/id_ed25519\n</code></pre> </li> <li> <p>Save and quit the file.</p> </li> <li> <p>Now you can ssh to your VM using the below command:</p> <pre><code>ssh -J eidf-gateway [EIDF username]@[vm_ip] -i /path/to/ssh/key\n</code></pre> <p>For Example:</p> <pre><code>ssh -J eidf-gateway alice@10.24.1.1 -i ~/.ssh/id_ed25519\n</code></pre> </li> <li> <p>You can add further alias options to make accessing your VM quicker. For example, if you use the below template to create an entry below the EIDF-Gateway entry in ~/.ssh/config, you can use the alias name to automatically jump through the EIDF-Gateway and onto your VM:</p> <pre><code>Host &lt;vm name/alias&gt;\n  HostName 10.24.VM.IP\n  User &lt;vm username&gt;\n  IdentityFile /path/to/ssh/key\n  ProxyCommand ssh eidf-gateway -W %h:%p\n</code></pre> <p>For Example:</p> <pre><code>Host demo\n  HostName 10.24.1.1\n  User alice\n  IdentityFile ~/.ssh/id_ed25519\n  ProxyCommand ssh eidf-gateway -W %h:%p\n</code></pre> <p>If your key is RSA, you will need to add the following line to the bottom of the 'demo' alias: <code>HostKeyAlgorithms +ssh-rsa</code></p> </li> <li> <p>Now, by running <code>ssh demo</code> your ssh agent will automatically follow the 'ProxyCommand' section in the 'demo' alias and jump through the gateway before following its own instructions to reach your VM.</p> </li> </ol> <p>Info</p> <p>This has added an 'Alias' entry to your ssh config, so whenever you ssh to 'eidf-gateway' your ssh agent will automatically fill the hostname, your username and ssh key. This method allows for a much less complicated ssh command to reach your VMs. You can replace the alias name with whatever you like, just change the 'Host' line from saying 'eidf-gateway' to the alias you would like. The <code>-J</code> flag is use to specify that we will access the second specified host by jumping through the first specified host.</p>"},{"location":"access/ssh/#sudo-password-setting-and-password-resets","title":"sudo, Password Setting and Password Resets","text":"<p>You do not have to set a password to log into virtual machines. However, if you have been given sudo permission, you will need to set a password to be able to make use of sudo. You can set (or reset) a password using the web form in the EIDF Portal following the instructions in Set or change the password for a user account.</p>"},{"location":"access/virtualmachines-vdi/","title":"Virtual Machines (VMs) and the EIDF Virtual Desktop Interface (VDI)","text":"<p>Using the EIDF VDI, members of EIDF projects can connect to VMs that they have been granted access to. The EIDF VDI is a web portal that displays the connections to VMs a user has available to them, and then those connections can be easily initiated by clicking on them in the user interface. Once connected to the target VM, all interactions are mediated through the user's web browser by the EIDF VDI.</p>"},{"location":"access/virtualmachines-vdi/#login-to-the-eidf-vdi","title":"Login to the EIDF VDI","text":"<p>Once your membership request to join the appropriate EIDF project has been approved, you will be able to login to the EIDF VDI at https://eidf-vdi.epcc.ed.ac.uk/vdi.</p> <p>Authentication to the VDI is provided by SAFE, so if you do not have an active web browser session in SAFE, you will be redirected to the SAFE log on page. If you do not have a SAFE account follow the instructions in the SAFE documentation how to register and receive your password.</p>"},{"location":"access/virtualmachines-vdi/#navigating-the-eidf-vdi","title":"Navigating the EIDF VDI","text":"<p>After you have been authenticated through SAFE and logged into the EIDF VDI, if you have multiple connections available to you that have been associated with your user (typically in the case of research projects), you will be presented with the VDI home screen as shown below:</p> <p> VDI home page with list of available VM connections</p> <p>Adding connections</p> <p>Note that if a project manager has added a new connection for you it may not appear in the list of connections immediately. You must log out and log in again to refresh your connections list.</p>"},{"location":"access/virtualmachines-vdi/#connecting-to-a-vm","title":"Connecting to a VM","text":"<p>If you have only one connection associated with your VDI user account (typically in the case of workshops), you will be automatically connected to the target VM's virtual desktop. Once you are connected to the VM, you will be asked for your username and password as shown below (if you are participating in a workshop, then you may not be asked for credentials)</p> <p>Warning</p> <p>If this is your first time connecting to EIDF using a new account, you have to set a password as described in Set or change the password for a user account.</p> <p> VM virtual desktop connection user account login screen</p> <p>Once your credentials have been accepted, you will be connected to your VM's desktop environment. For instance, the screenshot below shows a resulting connection to a Xubuntu 20.04 VM with the Xfce desktop environment.</p> <p> VM virtual desktop</p>"},{"location":"access/virtualmachines-vdi/#vdi-features-for-the-virtual-desktop","title":"VDI Features for the Virtual Desktop","text":"<p>The EIDF VDI is an instance of the Apache Guacamole clientless remote desktop gateway. Since the connection to your VM virtual desktop is entirely managed through Guacamole in the web browser, there are some additional features to be aware of that may assist you when using the VDI.</p>"},{"location":"access/virtualmachines-vdi/#the-vdi-menu","title":"The VDI Menu","text":"<p>The Guacamole menu is a sidebar which is hidden until explicitly shown. On a desktop or other device which has a hardware keyboard, you can show this menu by pressing &lt;Ctrl&gt; + &lt;Alt&gt; + &lt;Shift&gt; on a Windows PC client, or &lt;Ctrl&gt; + &lt;Command&gt; + &lt;Shift&gt; on a Mac client. To hide the menu, you press the same key combination once again. The menu provides various options, including:</p> <ul> <li>Reading from (and writing to) the clipboard of the remote desktop</li> <li>Zooming in and out of the remote display</li> </ul>"},{"location":"access/virtualmachines-vdi/#clipboard-copy-and-paste-functionality","title":"Clipboard Copy and Paste Functionality","text":"<p>After you have activated the Guacamole menu using the key combination above, at the top of the menu is a text area labeled \u201cclipboard\u201d along with some basic instructions:</p> <p>Text copied/cut within Guacamole will appear here. Changes to the text below will affect the remote clipboard.</p> <p>The text area functions as an interface between the remote clipboard and the local clipboard. Text from the local clipboard can be pasted into the text area, causing that text to be sent to the clipboard of the remote desktop. Similarly, if you copy or cut text within the remote desktop, you will see that text within the text area, and can manually copy it into the local clipboard if desired.</p> <p>You can use the standard keyboard shortcuts to copy text from your client PC or Mac to the Guacamole menu clipboard, then again copy that text from the Guacamole menu clipboard into an application or CLI terminal on the VM's remote desktop. An example of using the copy and paste clipboard is shown in the screenshot below.</p> <p> The EIDF VDI Clipboard</p>"},{"location":"access/virtualmachines-vdi/#keyboard-language-and-layout-settings","title":"Keyboard Language and Layout Settings","text":"<p>For users who do not have standard <code>English (UK)</code> keyboard layouts, key presses can have unexpected translations as they are transmitted to your VM. Please contact the EIDF helpdesk at EIDF Helpdesk if you are experiencing difficulties with your keyboard mapping, and we will help to resolve this by changing some settings in the Guacamole VDI connection configuration.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#eidf-frequently-asked-questions","title":"EIDF Frequently Asked Questions","text":""},{"location":"faq/#how-do-i-contact-the-eidf-helpdesk","title":"How do I contact the EIDF Helpdesk?","text":"<p>If you are an existing user of EIDF or other EPCC systems then please submit your queries via our EIDF Helpdesk otherwise send your query by email to eidf@epcc.ed.ac.uk.</p>"},{"location":"faq/#how-do-i-request-more-resources-for-my-project-can-i-extend-my-project","title":"How do I request more resources for my project? Can I extend my project?","text":"<p>Submit a support request: In the form select the project that your request relates to and select \"EIDF Project extension: duration and quota\" from the dropdown list of categories. Then enter the new quota or extension date in the description text box below and submit the request.</p> <p>The EIDF approval team will consider the extension and you will be notified of the outcome.</p>"},{"location":"faq/#new-vms-and-vdi-connections","title":"New VMs and VDI connections","text":"<p>My project manager gave me access to a VM but the connection doesn't show up in the VDI connections list?</p> <p>This may happen when a machine/VM was added to your connections list while you were logged in to the VDI. Please refresh the connections list by logging out and logging in again, and the new connections should appear.</p>"},{"location":"faq/#non-default-ssh-keys","title":"Non-default SSH Keys","text":"<p>I have different SSH keys for the SSH gateway and my VM, or I use a key which does not have the default name (~/.ssh/id_rsa) and I cannot login.</p> <p>The command syntax shown in our SSH documentation (using the <code>-J &lt;username&gt;@eidf-gateway</code> stanza) makes assumptions about SSH keys and their naming. You should try the full version of the command:</p> <pre><code>ssh -o ProxyCommand=\"ssh -i ~/.ssh/&lt;gateway_private_key&gt; -W %h:%p &lt;gateway_username&gt;@eidf-gateway.epcc.ed.ac.uk\" -i ~/.ssh/&lt;vm_private_key&gt; &lt;vm_username&gt;@&lt;vm_ip&gt;\n</code></pre> <p>Note that for the majority of users, gateway_username and vm_username are the same, as are gateway_private_key and vm_private_key</p>"},{"location":"faq/#username-policy","title":"Username Policy","text":"<p>I already have an EIDF username for project Y, can I use this for project X?</p> <p>We mandate that every username must be unique across our estate. EPCC machines including EIDF services such as the SDF and DSC VMs, and HPC services such as Cirrus require you to create a new machine account with a unique username for each project you work on. Usernames cannot be used on multiple projects, even if the previous project has finished. However, some projects span multiple machines so you may be able to login to multiple machines with the same username.</p>"},{"location":"known-issues/","title":"Known Issues","text":""},{"location":"known-issues/#virtual-desktops","title":"Virtual desktops","text":"<p>No known issues.</p>"},{"location":"overview/","title":"A Unique Service for Academia and Industry","text":"<p>The Edinburgh International Data Facility (EIDF) is a growing set of data and compute services developed to support the Data Driven Innovation Programme at the University of Edinburgh.</p> <p>Our goal is to support learners, researchers and innovators across the spectrum, with services from data discovery through simple learn-as-you-play-with-data notebooks to GPU-enabled machine-learning platforms for driving AI application development.</p>"},{"location":"overview/#eidf-and-the-data-driven-innovation-initiative","title":"EIDF and the Data-Driven Innovation Initiative","text":"<p>Launched at the end of 2018, the Data-Driven Innovation (DDI) programme is one of six funded within the Edinburgh &amp; South-East Scotland City Region Deal. The DDI programme aims to make Edinburgh the \u201cData Capital of Europe\u201d, with ambitious targets to support, enhance and improve talent, research, commercial adoption and entrepreneurship across the region through better use of data.</p> <p>The programme targets ten industry sectors, with interactions managed through five DDI Hubs: the Bayes Centre, the Usher Institute, Edinburgh Futures Institute, the National Robotarium, and Easter Bush. The activities of these Hubs are underpinned by EIDF.</p>"},{"location":"overview/acknowledgements/","title":"Acknowledging EIDF","text":"<p>If you make use of EIDF services in your work, we encourage you to acknowledge us in any publications.</p> <p>Acknowledgement of using the facility in publications can be used as an identifiable metric to evaluate the scientific support provided, and helps promote the impact of the wider DDI Programme.</p> <p>We encourage our users to ensure that an acknowledgement of EIDF is included in the relevant section of their manuscript. We would suggest:</p> <p>This work was supported by the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh.</p>"},{"location":"overview/contacts/","title":"Contact","text":"<p>The Edinburgh International Data Facility is located at the Advanced Computing Facility of EPCC, the supercomputing centre based at the University of Edinburgh.</p>"},{"location":"overview/contacts/#send-us-a-query","title":"Send us a query","text":"<p>If you are an existing user of EIDF or other EPCC systems then please submit your queries via our EIDF Helpdesk otherwise send your query by email to eidf@epcc.ed.ac.uk.</p>"},{"location":"overview/contacts/#sign-up","title":"Sign up","text":"<p>Join our mailing list to receive updates about EIDF.</p>"},{"location":"safe-haven-services/network-access-controls/","title":"Safe Haven Network Access Controls","text":"<p>The TRE Safe Haven services are protected against open, global access by IPv4 source address filtering. These network access controls ensure that connections are permitted only from Safe Haven controller partner networks and collaborating research institutions.</p> <p>The network access controls are managed by the Safe Haven service controllers who instruct EPCC to add and remove the IPv4 addresses allowed to connect to the service gateway. Researchers must connect to the Safe Haven service by first connecting to their institution or corporate VPN and then connecting to the Safe Haven.</p> <p>The Safe Haven IG controller and research project co-ordination teams must submit and confirm IPv4 address filter changes to their service help desk via email.</p>"},{"location":"safe-haven-services/overview/","title":"Safe Haven Services","text":"<p>The EIDF Trusted Research Environment (TRE) hosts several Safe Haven services that enable researchers to work with sensitive data in a secure environment. These services are operated by EPCC in partnership with Safe Haven controllers who manage the Information Governance (IG) appropriate for the research activities and the data access of their Safe Haven service.</p> <p>It is the responsibility of EPCC as the Safe Haven operator to design, implement and administer the technical controls required to deliver the Safe Haven security regime demanded by the Safe Haven controller.</p> <p>The role of the Safe Haven controller is to satisfy the needs of the researchers and the data suppliers. The controller is responsible for guaranteeing the confidentiality needs of the data suppliers and matching these with the availability needs of the researchers.</p> <p>The service offers secure data sharing and analysis environments allowing researchers access to sensitive data under the terms and conditions prescribed by the data providers. The service prioritises the requirements of the data provider over the demands of the researcher and is an academic TRE operating under the guidance of the Five Safes framework.</p> <p>The TRE has dedicated, private cloud infrastructure at EPCC's Advanced Computing Facility (ACF) data centre and has its own HPC cluster and high-performance file systems. When a new Safe Haven service is commissioned in the TRE it is created in a new virtual private cloud providing the Safe Haven service controller with an independent IG domain separate from other Safe Havens in the TRE. All TRE service infrastructure and all TRE project data are hosted at ACF.</p> <p>If you have any questions about the EIDF TRE or about Safe Haven services, please contact us.</p>"},{"location":"safe-haven-services/safe-haven-access/","title":"Safe Haven Service Access","text":"<p>Safe Haven services are accessed from a registered network connection address using a browser. The service URL will be \"https://shs.epcc.ed.ac.uk/&lt;service&gt;\" where &lt;service&gt; is the Safe Haven service name.</p> <p>The Safe Haven access process is in three stages from multi-factor authentication to project desktop login.</p> <p>Researchers who are active in many research projects and in more than one Safe Haven will need to pay attention to the service they connect to, the project desktop they login to, and the accounts and identities they are using.</p>"},{"location":"safe-haven-services/safe-haven-access/#safe-haven-login","title":"Safe Haven Login","text":"<p>The first step in the process prompts the user for a Safe Haven username and then for a session PIN code sent via SMS text to the mobile number registered for the username.</p> <p>Valid PIN code entry allows the user access to all of the Safe Haven service remote desktop gateways for up to 24 hours without entry of a new PIN code. A user who has successfully entered a PIN code once can access shs.epcc.ed.ac.uk/haven1 and shs.epcc.ed.ac.uk/haven2 without repeating PIN code identity verification.</p> <p>When a valid PIN code is accepted, the user is prompted to accept the service use terms and conditions.</p> <p>Registration of the user mobile phone number is managed by the Safe Haven IG controller and research project co-ordination teams by submitting and confirming user account changes through the dedicated service help desk via email.</p>"},{"location":"safe-haven-services/safe-haven-access/#remote-desktop-gateway-login","title":"Remote Desktop Gateway Login","text":"<p>The second step in the access process is for the user to login to the Safe Haven service remote desktop gateway so that a project desktop connection can be chosen. The user is prompted for a Safe Haven service account identity.</p> <p> VDI Safe Haven Service Login Page</p> <p>Safe Haven accounts are managed by the Safe Haven IG controller and research project co-ordination teams by submitting and confirming user account changes through the dedicated service help desk via email.</p>"},{"location":"safe-haven-services/safe-haven-access/#project-desktop-connection","title":"Project Desktop Connection","text":"<p>The third stage in the process is to select the virtual connection from those available on the account's home page. An example home page is shown below offering two connection options to the same virtual machine. Remote desktop connections will have an _rdp suffix and SSH terminal connections have an _ssh suffix. The most recently used connections are shown as screen thumbnails at the top of the page and all the connections available to the user are shown in a tree list below this.</p> <p> VM connections available home page</p> <p>The remote desktop gateway software used in the Safe Haven services in the TRE is the Apache Guacamole web application. Users new to this application can find the user manual here. It is recommended that users read this short guide, but note that the data sharing features such as copy and paste, connection sharing, and file transfers are disabled on all connections in the TRE Safe Havens.</p> <p>A remote desktop or SSH connection is used to access data provided for a specific research project. If a researcher is working on multiple projects within a Safe Haven they can only login to one project at a time. Some connections may allow the user to login to any project and some connections will only allow the user to login into one specific project. This depends on project IG restrictions specified by the Safe Haven and project controllers.</p> <p>Project desktop accounts are managed by the Safe Haven IG controller and research project co-ordination teams by submitting and confirming user account changes through the dedicated service help desk via email.</p>"},{"location":"safe-haven-services/using-the-hpc-cluster/","title":"Using the TRE HPC Cluster","text":""},{"location":"safe-haven-services/using-the-hpc-cluster/#introduction","title":"Introduction","text":"<p>The TRE HPC system, also called the SuperDome Flex, is a single node, large memory HPC system. It is provided for compute and data intensive workloads that require more CPU, memory, and better IO performance than can be provided by the project VMs, which have the performance equivalent of small rack mount servers.</p>"},{"location":"safe-haven-services/using-the-hpc-cluster/#specifications","title":"Specifications","text":"<p>The system is an HPE SuperDome Flex configured with 1152 hyper-threaded cores (576 physical cores) and 18TB of memory, of which 17TB is available to users. User home and project data directories are on network mounted storage pods running the BeeGFS parallel filesystem. This storage is built in blocks of 768TB per pod. Multiple pods are available in the TRE for use by the HPC system and the total storage available will vary depending on the project configuration.</p> <p>The HPC system runs Red Hat Enterprise Linux, which is not the same flavour of Linux as the Ubuntu distribution running on the desktop VMs. However, most jobs in the TRE run Python and R, and there are few issues moving between the two version of Linux. Use of virtual environments is strongly encouraged to ensure there are no differences between the desktop and HPC runtimes.</p>"},{"location":"safe-haven-services/using-the-hpc-cluster/#software-management","title":"Software Management","text":"<p>All system level software installed and configured on the TRE HPC system is managed by the TRE admin team. Software installation requests may be made by the Safe Haven IG controllers, research project co-ordinators, and researchers by submitting change requests through the dedicated service help desk via email.</p> <p>Minor software changes will be made as soon as admin effort can be allocated. Major changes are likely to be scheduled for the TRE monthly maintenance session on the first Thursday of each month.</p>"},{"location":"safe-haven-services/using-the-hpc-cluster/#hpc-login","title":"HPC Login","text":"<p>Login to the HPC system is from the project VM using SSH and is not direct from the VDI. The HPC cluster accounts are the same accounts used on the project VMs, with the same username and password. All project data access on the HPC system is private to the project accounts as it is on the VMs, but it is important to understand that the TRE HPC cluster is shared by projects in other TRE Safe Havens.</p> <p>One-Way SSH Access Only</p> <p>SSH access to shs-sdf01 is strictly one-way from the project VM to the HPC system. Reverse SSH from the HPC system back to the project VM or any other system is not permitted.</p> <p>To login to the HPC cluster from the project VMs use <code>ssh shs-sdf01</code> from an xterm. If you wish to avoid entry of the account password for every SSH session or remote command execution you can use SSH key authentication by following the SSH key configuration instructions here. SSH key passphrases are not strictly enforced within the Safe Haven but are strongly encouraged.</p>"},{"location":"safe-haven-services/using-the-hpc-cluster/#running-jobs","title":"Running Jobs","text":"<p>To use the HPC system fully and fairly, all jobs must be run using the SLURM job manager. More information about SLURM, running batch jobs and running interactive jobs can be found here. Please read this carefully before using the cluster if you have not used SLURM before. The SLURM site also has a set of useful tutorials on HPC clusters and job scheduling.</p> <p>All analysis and processing jobs must be run via SLURM. SLURM manages access to all the cores on the system beyond the first 32. If SLURM is not used and programs are run directly from the command line, then there are only 32 cores available, and these are shared by the other users. Normal code development, short test runs, and debugging can be done from the command line without using SLURM.</p> <p>There is only one node</p> <p>The HPC system is a single node with all cores sharing all the available memory. SLURM jobs should always specify '#SBATCH --nodes=1' to run correctly.</p> <p>SLURM email alerts for job status change events are not supported in the TRE.</p>"},{"location":"safe-haven-services/using-the-hpc-cluster/#resource-limits","title":"Resource Limits","text":"<p>There are no resource constraints imposed on the default SLURM partition at present. There are user limits (see the output of <code>ulimit -a</code>). If a project has a requirement for more than 200 cores, more than 4TB of memory, or an elapsed runtime of more than 96 hours, a resource reservation request should be made by the researchers through email to the service help desk.</p> <p>There are no storage quotas enforced in the HPC cluster storage at present. The project storage requirements are negotiated, and space allocated before the project accounts are released. Storage use is monitored, and guidance will be issued before quotas are imposed on projects.</p> <p>The HPC system is managed in the spirit of utilising it as fully as possible and as fairly as possible. This approach works best when researchers are aware of their project workload demands and cooperate rather than compete for cluster resources.</p>"},{"location":"safe-haven-services/using-the-hpc-cluster/#python-jobs","title":"Python Jobs","text":"<p>A basic script to run a Python job in a virtual environment is shown below.</p> <pre><code>#!/bin/bash\n#\n#SBATCH --export=ALL                  # Job inherits all env vars\n#SBATCH --job-name=my_job_name        # Job name\n#SBATCH --mem=512G                    # Job memory request\n#SBATCH --output=job-%j.out           # Standard output file\n#SBATCH --error=job-%j.err            # Standard error file\n#SBATCH --nodes=1                     # Run on a single node\n#SBATCH --ntasks=1                    # Run one task per node\n#SBATCH --time=02:00:00               # Time limit hrs:min:sec\n#SBATCH --partition standard          # Run on partition (queue)\n\npwd\nhostname\ndate \"+DATE: %d/%m/%Y TIME: %H:%M:%S\"\necho \"Running job on a single CPU core\"\n\n# Create the job\u2019s virtual environment\nsource ${HOME}/my_venv/bin/activate\n\n# Run the job code\npython3 ${HOME}/my_job.py\n\ndate \"+DATE: %d/%m/%Y TIME: %H:%M:%S\"\n</code></pre>"},{"location":"safe-haven-services/using-the-hpc-cluster/#mpi-jobs","title":"MPI Jobs","text":"<p>An example script for a multi-process MPI example is shown. The system currently supports MPICH MPI.</p> <pre><code>#!/bin/bash\n#\n#SBATCH --export=ALL\n#SBATCH --job-name=mpi_test\n#SBATCH --output=job-%j.out\n#SBATCH --error=job-%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=5\n#SBATCH --time=05:00\n#SBATCH --partition standard\n\necho \"Submitted Open MPI job\"\necho \"Running on host ${HOSTNAME}\"\necho \"Using ${SLURM_NTASKS_PER_NODE} tasks per node\"\necho \"Using ${SLURM_CPUS_PER_TASK} cpus per task\"\nlet mpi_threads=${SLURM_NTASKS_PER_NODE}*${SLURM_CPUS_PER_TASK}\necho \"Using ${mpi_threads} MPI threads\"\n\n# load Open MPI module\nmodule purge\nmodule load mpi/mpich-x86_64\n\n# run mpi program\nmpirun ${HOME}/test_mpi\n</code></pre>"},{"location":"safe-haven-services/using-the-hpc-cluster/#managing-files-and-data","title":"Managing Files and Data","text":"<p>There are three file systems to manage in the VM and HPC environment.</p> <ol> <li>The desktop VM /home file system. This can only be used when you login to the VM remote desktop. This file system is local to the VM and is not backed up.</li> <li>The HPC system /home file system. This can only be used when you login to the HPC system using SSH from the desktop VM. This file system is local to the HPC cluster and is not backed up.</li> <li>The project file and data space in the /safe_data file system. This file system can only be used when you login to a VM remote desktop session. This file system is backed up.</li> </ol> <p>The /safe_data file system with the project data cannot be used by the HPC system. The /safe_data file system has restricted access and a relatively slow IO performance compared to the parallel BeeGFS file system storage on the HPC system.</p> <p>The process to use the TRE HPC service is to copy and synchronise the project code and data files on the /safe_data file system with the HPC /home file system before and after login sessions and job runs on the HPC cluster. Assuming all the code and data required for the job is in a directory 'current_wip' on the project VM, the workflow is as follows:</p> <ol> <li>Copy project code and data to the HPC cluster (from the desktop VM) <code>rsync -avPz -e ssh /safe_data/my_project/current_wip shs-sdf01:</code></li> <li>Run jobs/tests/analysis <code>ssh shs-sdf01</code>, <code>cd current_wip</code>, <code>sbatch/srun my_job</code></li> <li>Copy any changed project code and data back to /safe_data (from the desktop VM) <code>rsync -avPz -e ssh shs-sdf01:current_wip /safe_data/my_project</code></li> <li>Optionally delete the code and data from the HPC cluster working directory.</li> </ol>"},{"location":"safe-haven-services/virtual-desktop-connections/","title":"Virtual Machine Connections","text":"<p>Sessions on project VMs may be either remote desktop (RDP) logins or SSH terminal logins. Most users will prefer to use the remote desktop connections, but the SSH terminal connection is useful when remote network performance is poor and it must be used for account password changes.</p>"},{"location":"safe-haven-services/virtual-desktop-connections/#first-time-login-and-account-password-changes","title":"First Time Login and Account Password Changes","text":"<p>Account Password Changes</p> <p>Note that first time account login cannot be through RDP as a password change is required. Password reset logins must be SSH terminal sessions as password changes can only be made through SSH connections.</p>"},{"location":"safe-haven-services/virtual-desktop-connections/#connecting-to-a-remote-ssh-session","title":"Connecting to a Remote SSH Session","text":"<p>When a VM SSH connection is selected the browser screen becomes a text terminal and the user is prompted to \"Login as: \" with a project account name, and then prompted for the account password. This connection type is equivalent to a standard xterm SSH session.</p>"},{"location":"safe-haven-services/virtual-desktop-connections/#connecting-to-a-remote-desktop-session","title":"Connecting to a Remote Desktop Session","text":"<p>Remote desktop connections work best by first placing the browser in Full Screen mode and leaving it in this mode for the entire duration of the Safe Haven session.</p> <p>When a VM RDP connection is selected the browser screen becomes a remote desktop presenting the login screen shown below.</p> <p> VM virtual desktop connection user account login screen</p> <p>Once the project account credentials have been accepted, a remote dekstop similar to the one shown below is presented. The default VM environment in the TRE is Ubuntu 22.04 with the Xfce desktop.</p> <p> VM virtual desktop</p>"},{"location":"safe-haven-services/open-ondemand/about/","title":"About the TRE Open OnDemand service","text":"<p>The TRE Open OnDemand service is a web service that runs within a safe haven. The service allows you to run compute and data-related tasks on compute resources available to your safe haven.</p> <p>The service provides a number of apps that allow for researchers' containers - which package up software, services and their dependencies - to be executed via the TRE Container Execution Service as well as providing apps supporting the execution of standard containers with useful services such as JupyterLab and RStudio Server.</p> <p> Open OnDemand home page</p> <p>To quickly get started with the TRE Open OnDemand Service (hereon referred to as Open OnDemand for brevity):</p> <ul> <li>Access Open OnDemand.</li> <li>Follow Getting started to walk through the key points of the service and using it to run containers via the TRE Container Execution Service.</li> </ul>"},{"location":"safe-haven-services/open-ondemand/access/","title":"Access Open OnDemand","text":"<p>Open OnDemand is accessed by running a browser on a VM within your safe haven.</p>"},{"location":"safe-haven-services/open-ondemand/access/#access-open-ondemand-within-your-safe-haven","title":"Access Open OnDemand within your safe haven","text":"<p>To access Open OnDemand:</p> <ol> <li>Start a remote desktop (RDP) session with a VM within your safe haven, as described in Safe Haven Service Access.</li> <li>Start a web browser within your safe haven VM.</li> <li>Enter the Open OnDemand URL for your safe haven:<ul> <li>National Safe Haven, <code>https://nsh-ondemand.nsh.loc</code></li> <li>ODAP, <code>https://odp-ondemand.nsh.loc</code></li> <li>Smart Data Foundry, <code>https://smartdf-ondemand.nsh.loc</code></li> <li>DataLoch, <code>https://dap-ondemand.nsh.loc</code></li> </ul> </li> <li>If using Falkon and an SSL Certificate Error! dialog appears with text 'Would you like to make an exception for this certificate?', then:<ul> <li>Click Yes. It is OK to ignore this error in this specific case, as you are within your safe haven within the TRE, contacting a Safe Haven Services service also running within your safe haven. This error arises as, at time of writing, the certificates for the TRE services have not been installed on your VM.</li> </ul> </li> <li>If using Firefox and a Warning: Potential Security Risk Ahead page appears with text 'Error code: SEC_ERROR_UNKNOWN_ISSUER', then<ol> <li>Click Advanced...</li> <li>Click Accept the Risk and Continue. It is OK to accept this risk in this specific case, as you are within your safe haven within the TRE, contacting a Safe Haven Services service also running within your safe haven. This warning arises as, at time of writing, the certificates for the TRE services have not been installed on your VM.</li> </ol> </li> <li> <p>The Open OnDemand log in page will appear.</p> <p> Open OnDemand log in page</p> </li> <li> <p>Enter your project username and password. These are the same username and password that you used when logging into your safe haven VM.</p> </li> <li>Click Log in</li> <li>The Open OnDemand home page will open.</li> </ol>"},{"location":"safe-haven-services/open-ondemand/access/#troubleshooting-bad-request","title":"Troubleshooting: Bad Request","text":"<p>If you see a page with text:</p> <p>Your browser sent a request that this server could not understand.</p> <p>then revisit the URL and try to log in again. This can arise if there is information in your browser cache from a previous Open OnDemand session.</p>"},{"location":"safe-haven-services/open-ondemand/access/#troubleshooting-cannot-access-open-ondemand","title":"Troubleshooting: Cannot access Open OnDemand","text":"<p>'For any other problems logging into Open OnDemand, first double-check your username and password. If you still have no success, then please contact your Research Coordinator (or equivalent).</p>"},{"location":"safe-haven-services/open-ondemand/apps/","title":"View and run apps","text":""},{"location":"safe-haven-services/open-ondemand/apps/#introduction","title":"Introduction","text":"<p>The following apps are available on Open OnDemand.</p> App Type Description Active Jobs System Installed App Open OnDemand app that allows you to browse and manage jobs created via both apps and the Job Composer app Run Container Tenant and HPC Service Container Execution Service app that allows you to run a container on a back-end Run JupyterLab Tenant Service Container Execution Service app that allows you to run a JupyterLab container on a back-end Run RStudio Server Tenant Service Container Execution Service app that allows you to run an RStudio Server container on a back-end Job Composer System Installed App Open OnDemand app that allows you to submit a Slurm batch job to a back-end <p>The app types are:</p> <ul> <li>'System Installed App': apps provided with Open OnDemand.</li> <li>'Tenant Service': apps to run jobs on back-ends within your safe haven.</li> <li>'Tenant and HPC Service': apps to run jobs on back-ends within both your safe haven and on TRE-level resources.</li> </ul> <p>Open OnDemand supports a number of ways by which you can see the apps available to you and select an app to run.</p> <p>In each case, selecting an app will open an app-specific page, from which you can run the app, for which Open OnDemand will create and run a job.</p>"},{"location":"safe-haven-services/open-ondemand/apps/#jobs-panel","title":"Jobs panel","text":"<p>The Jobs panel on the Open OnDemand home page has a selection of the apps available to you.</p> <p>Click on an app button to access that app.</p>"},{"location":"safe-haven-services/open-ondemand/apps/#apps-menu","title":"Apps menu","text":"<p>The Apps menu provides access to all of the apps available to you.</p> <p>Select an app-specific menu option to access that app. The app-specific menu options correspond to the apps available via the Jobs panel.</p> <p>To see all of the apps available to you, select the All Apps option to go to the All Apps page.</p>"},{"location":"safe-haven-services/open-ondemand/apps/#jobs-menu","title":"Jobs menu","text":"<p>The Jobs menu shows all of the apps available to you.</p> <p>Select an app-specific menu option to access that app.</p> <p>Info</p> <p>If you are wondering why the Apps menu and Jobs menu overlap, this is because each app has a 'category' label. This is used by Open OnDemand both to group app buttons on the home page and to create menus with apps belonging to a specific category. All Container Execution Service apps are within a 'Jobs' category, hence the overlap.</p>"},{"location":"safe-haven-services/open-ondemand/apps/#all-apps-page","title":"All Apps page","text":"<p>Select the Apps menu, All Apps option to go to the All Apps page.</p> <p>The All Apps page shows all of the apps available to you.</p> <p>Click an app-specific link to access that app.</p>"},{"location":"safe-haven-services/open-ondemand/apps/#my-interactive-sessions-page-apps-list","title":"My Interactive Sessions page apps list","text":"<p>Click My Interactive Sessions (overlaid squares icon) on the menu bar to opens the My Interactive Sessions page.</p> <p> My Interactive Sessions menu button</p> <p>The My Interactive Sessions page has a selection of the apps available to you on the left-hand-side.</p> <p>Click an app-specific button to access that app.</p>"},{"location":"safe-haven-services/open-ondemand/containers/","title":"Run containers","text":""},{"location":"safe-haven-services/open-ondemand/containers/#introduction","title":"Introduction","text":"<p>The TRE Open OnDemand service is intended to allow you to run jobs that run containers, which package up software, services a nd their dependencies. This includes both your own project-specific containers that you have been authorised to run within your safe haven as well as standard containers with useful services such as JupyterLab and RStudio Server.</p> <p>Container are typically run using Podman or Apptainer, depending on which of these is available on a back-end. Some Open OnDemand apps will select which to use, others allow you to choose.</p> <p>Run jobs introduces how Open OnDemand runs tasks, and information you need to know about when running tasks. The page focuses on aspects of running containers within this job execution environment.</p>"},{"location":"safe-haven-services/open-ondemand/containers/#container-requirements","title":"Container requirements","text":"<p>Open OnDemand uses the Container Execution Service tools to run containers. Consequently, containers run via Open OnDemand must conform to the requirements of the Container Execution Service. See the TRE Container User Guide for details of these requirements.</p>"},{"location":"safe-haven-services/open-ondemand/containers/#container-registries","title":"Container registries","text":"<p>The container registries supported by the Container Execution Service, and so accessible from within Open OnDemand, are as follows:</p> Container Registry URL prefix Example GitHub <code>ghcr.io</code> <code>ghcr.io/mikej888/hello-tre:1.0</code> University of Edinburgh ECDF GitLab <code>git.ecdf.ed.ac.uk</code> <code>git.ecdf.ed.ac.uk/tre-container-execution-service/containers/hello-tre:1.0</code> <p>Note</p> <p>For ECDF GitLab, do not put the port number, 5050, into the URL. The Container Execution Service tools will automatically insert this into ECDF GitLab URLs.</p>"},{"location":"safe-haven-services/open-ondemand/containers/#sharing-files-between-a-back-end-and-a-container","title":"Sharing files between a back-end and a container","text":"<p>When the container is run, three directories on the back-end are mounted into the container:</p> Back-end directory Container directory Description Project-specific <code>/safe_data</code> subdirectory <code>$HOME/safe_data</code> OR <code>/safe_data/PROJECT_SUBDIRECTORY</code> If <code>$HOME/safe_data</code> exists in your home directory on the back-end, then that is mounted. Otherwise, a subdirectory of <code>/safe_data</code> corresponding to your project (and inferred from your user group) is mounted, if such a subdirectory can be found. <code>$HOME/safe_outputs/APP_SHORT_NAME/SESSION_ID</code> <code>/safe_outputs</code> <code>APP_SHORT_NAME</code> is a short-name for an app (e.g., <code>jupyter</code> for Run JupyterLab). <code>SESSION_ID</code> a unique session identifier created when an app is run. This directory is created in your home directory on the back-end when your container runs. The directory persists after the job which created the container ends. <code>$HOME/scratch/APP_SHORT_NAME/SESSION_ID</code> <code>/scratch</code> <code>APP_SHORT_NAME</code> and <code>SESSION_ID</code> are as above. This directory is also created in your home directory on the back-end when your container runs. This directory exists for the duration of the job which created the container. The <code>SESSION_ID</code> sub-directory is deleted when the job which created the container ends. <p>Together, these mounts provides a means for data, configuration files, scripts and code to be shared between the back-end on which the container is running and the environment within the container itself. Creating or editing a file within any of these directories on the back-end means that the changes will be available within the container, and vice-versa.</p> <p>You can interact with your project's <code>/safe_data</code> subdirectory on the back-end, by logging into the back-end, see Log into back-ends.</p> <p>When using a back-end where your home directory is common to both the Open OnDemand VM and the back-end, then you can interact with both <code>safe_outputs/APP_SHORT_NAME/SESSION_ID</code> and <code>scratch/APP_SHORT_NAME/SESSION_ID</code> (and <code>$HOME/safe_data</code>, if applicable) via the File Manager and/or by logging into the back-end, see Log into back-ends.</p> <p>When using a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, then you can interact with <code>/safe_data/PROJECT_SUBDIRECTORY</code> (or <code>$HOME/safe_data</code>, if applicable), <code>safe_outputs/APP_SHORT_NAME/SESSION_ID</code> and <code>scratch/APP_SHORT_NAME/SESSION_ID</code> by logging into the back-end, see Log into back-ends.</p> <p>Note</p> <p>Your project data files, in a project-specific directory under <code>/safe_data</code> are not available on the Open OnDemand VM.</p> <p>Note</p> <p>Your project data files, in a project-specific directory under <code>/safe_data</code> are not available on TRE-level back-ends (e.g., the Superdome Flex). For these, you will need to stage your data to the TRE-level back-end following your project- and safe haven-specific processes for the use of TRE-level services.</p>"},{"location":"safe-haven-services/open-ondemand/containers/#troubleshooting-cannot-open-project-data-safe_datacannot_determine_project_from_groups","title":"Troubleshooting: 'Cannot open project data: /safe_data/cannot_determine_project_from_groups'","text":"<p>If your project cannot be inferred from your user group or there is no subdirectory of <code>/safe_data</code> for your project group, and you are not using a <code>$HOME/safe_data</code> directory, then the job running the container will fail.</p> <p>As described in Job cards, app job cards will only show such jobs as having 'Completed'. Whether a job succeeded or failed can be seen in the job details for the job which can be seen via the Active Jobs app.</p> <p>In such cases, the log file for the app's job, in the job context directory, <code>ondemand/data/sys/dashboard/batch_connect/sys/APP_NAME/output/SESSION_ID</code>, will include a message:</p> <pre><code>Cannot open project data: /safe_data/cannot_determine_project_from_groups\n</code></pre> <p>If this problem occurs, then please contact your Research Coordinator (or equivalent).</p>"},{"location":"safe-haven-services/open-ondemand/containers/#containers-and-root-users","title":"Containers and 'root' users","text":"<p>For some containers run using Podman that you will find that you the 'root' user within the container but only within the container. You do not have 'root' access to the back-end on which the container is running!</p> <p>Any files you create in the directories mounted into the container will be owned by your own user, and user group, on the back-end.</p> <p>For containers run using Apptainer, you will be your own user within the  container.</p> <p>As a concrete example, consider the <code>hello-tre</code> example container (described in Getting started) which outputs in a log file the permissions of the directories mounted into the a container (as described above).</p> <p>If <code>hello-tre</code> is run via Podman, then you will be the 'root' user within the container and the directory permissions logged will be:</p> <pre><code>/safe_data: nobody (65534) root(0) drwxrwx--- nfs\n/scratch: root (0) root(0) drwxr-xr-x ext2/ext3\n/safe_outputs: root (0) root(0) drwxr-xr-x ext2/ext3\n</code></pre> <p><code>/safe_data</code> has user <code>nobody</code> as typically the user that owns <code>/safe_data</code> on the back-end won't be known within the container. If using <code>$HOME/safe_data</code> then the permissions logged would be:</p> <pre><code>/safe_data: root (0) root(0) drwxr-xr-x ext2/ext3\n</code></pre> <p>as this is in your home directory, and, again, you are <code>root</code> but only within the container.</p> <p>The other directories, mounted from directories in your home directory, likewise have user, and group, <code>root</code>.</p> <p>In contrast, if <code>hello-tre</code> is run via Apptainer, then the directory permissions logged are:</p> <pre><code>/safe_data: nobody (65534) your_project_group(4797) drwxrwx--- nfs\n/scratch: you (36177) your_project_group(4797) drwxr-xr-x ext2/ext3\n/safe_outputs: you (36177) your_project_group(4797) drwxr-xr-x ext2/ext3\n</code></pre> <p>Again <code>/safe_data</code> has user <code>nobody</code> as typically the user that owns <code>/safe_data</code> on the back-end won't be known within the container. However, its group will be your user group. If using <code>$HOME/safe_data</code> then the permissions logged would be:</p> <pre><code>/safe_data: you (36177) your_project_group(4797) drwxr-xr-x ext2/ext3\n</code></pre> <p>as this is in your home directory.</p> <p>Similarly, the other directories, mounted from directories in your home directory likewise have your user, and user group.</p>"},{"location":"safe-haven-services/open-ondemand/files/","title":"Browse and manage files","text":""},{"location":"safe-haven-services/open-ondemand/files/#introduction","title":"Introduction","text":"<p>Open OnDemand allows you to browse and manage files via the File Manager which can be accessed in various ways.</p>"},{"location":"safe-haven-services/open-ondemand/files/#file-manager","title":"File Manager","text":"<p>Select the Files menu, Home Directory option to open the File Manager.</p> <p>You can browse your home directory and there are buttons to create new files and directories, upload or download files, copy or move files or directories and delete files or directories and to change directory.</p> <p> The File Manager</p> <p>Note</p> <p>The File Manager allows you to only manipulate files on the Open OnDemand VM. For most back-ends, your home directory is common to both the Open OnDemand VM and the back-ends so your directories and files on the Open OnDemand VM, and changes to these, are reflected on the back-ends and vice-versa.</p> <p>However, you may have access to back-ends where your home directory is not common to both the Open OnDemand VM and the back-end i.e., you have unsynchronised, separate, home directories on each VM, and you will have to log into a back-end to view files created on the back-end when you run jobs. For more information, see Unsynched home directories and Log into back-ends.</p> <p>Note</p> <p>Your project data files, in a project-specific directory under <code>/safe_data</code> are not available on the Open OnDemand VM.</p> <p>Note</p> <p>Your project data files, in a project-specific directory under <code>/safe_data</code> are not available on TRE-level back-ends (e.g., the Superdome Flex). For these, you will need to stage your data to the TRE-level back-end following your</p>"},{"location":"safe-haven-services/open-ondemand/files/#open-in-terminal","title":"Open in Terminal","text":"<p>Click Open in Terminal to log into a specific VM:</p> <ul> <li>The default is the first back-end you have access to in alphabetical order.</li> <li>To select a specific back-end, click the Open in Terminal button's &gt; side-button to open a drop down-menu to allow you to choose a specific VM.</li> </ul> <p> Open in Terminal button and menu</p> <p>Once logged in, your current directory will be changed to match the job context directory.</p>"},{"location":"safe-haven-services/open-ondemand/files/#troubleshooting-cd-no-such-file-or-directory","title":"Troubleshooting: 'cd ... No such file or directory'","text":"<p>If, after you have logged into the back-end, you see an error like:</p> <pre><code>bash: line 1: cd: /home/user/ondemand: No such file or directory\n</code></pre> <p>then this means that the directory you are currently viewing in the File Manager on the Open OnDemand VM is not available on the back-end.</p> <p>This can arise if you select a back-end where your home directory is not common to both the Open OnDemand VM and the back-end and you have not yet run a job on that back-end. See Unsynched home directories for back-ends this relates to.</p>"},{"location":"safe-haven-services/open-ondemand/files/#my-interactive-sessions-page-job-cards-and-job-context-directory","title":"My Interactive Sessions page job cards and job context directory","text":"<p>Click My Interactive Sessions (overlaid squares icon) on the menu bar to open the My Interactive Sessions page.</p> <p> My Interactive Sessions menu button</p> <p>On a job's job card, click the Session ID link to open the File Manager, pointing at the job context directory for the job on the Open OnDemand VM.</p>"},{"location":"safe-haven-services/open-ondemand/files/#active-jobs-app-and-job-context-directory","title":"Active Jobs app and job context directory","text":"<p>Open the Active Jobs app.</p> <p>Click the &gt; button, by the job of interest, to open the job details.</p> <p>Click Open in File Manager to open the File Manager pointing at the job context directory for the job on the Open OnDemand VM.</p>"},{"location":"safe-haven-services/open-ondemand/files/#job-composer-app-and-job-context-directory","title":"Job Composer app and job context directory","text":"<p>Open the Job Composer app.</p> <p>Select a job.</p> <p>Click Open Dir or click Edit Files to open the File Manager pointing at the job context directory for the currently selected job.</p>"},{"location":"safe-haven-services/open-ondemand/files/#app-data-root-directory","title":"App data root directory","text":"<p>Within an app's form, click the data root directory link to open the File Manager pointing at the app directory, <code>ondemand/data/sys/dashboard/batch_connect/sys/APP_NAME</code>, under which the job's files will be created.</p>"},{"location":"safe-haven-services/open-ondemand/files/#troubleshooting-error-occurred-when-attempting-to-access-ondemanddatasysdashboardbatch_connectsysapp_name","title":"Troubleshooting: 'Error occurred when attempting to access ondemand/data/sys/dashboard/batch_connect/sys/APP_NAME'","text":"<p>If a dialog pops up with error:</p> <p>'Error occurred when attempting to access /pun/sys/dashboard/files/fs/.../ondemand/data/sys/dashboard/batch_connect/sys/APP_NAME'</p> <p>and</p> <p>'Cannot read file /.../ondemand/data/sys/dashboard/batch_connect/sys/APP_NAME'</p> <p>then click OK.</p> <p>This error can arise if you have not used the app before and, so, its <code>APP_NAME</code> subdirectory will not exist under your <code>ondemand</code> directory.</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/","title":"Getting started","text":"<p>The TRE Open OnDemand service allows you to run compute and data-related tasks on compute resources available to your safe haven. Here, we introduce using Open OnDemand to run containers - which package up software, services, and their dependencies - via the TRE Container Execution Service - on compute resources available to your safe haven.</p> <p>First, some Open OnDemand terminology. A compute resource upon which tasks can be run is called a back-end, or, in some parts of Open OnDemand, a cluster. Each run of a task on a back-end is called a job. An Open OnDemand component that allows you to run jobs, or other useful functions, is called an app.</p> <p>Within the TRE Open OnDemand service, apps are provided to run containers on back-ends. This walkthrough is centred around three apps:</p> <ul> <li>Run Container allows you to run a container on a back-end. This app is designed to run batch containers, those that perform some computational or data-related task, those that perform some computational or data-related task without human interaction when they are running.</li> <li>Run JupyterLab allows you to run a JupyterLab container on a back-end, which creates an interactive JupyterLab service that you can use. Please be reassured that no Python knowledge is assumed or required!</li> <li>Active Jobs allows you to see which of your jobs have been submitted, are running, or have completed.</li> </ul>"},{"location":"safe-haven-services/open-ondemand/getting-started/#where-open-ondemand-stores-your-information-your-ondemand-directory","title":"Where Open OnDemand stores your information - your <code>ondemand</code> directory","text":"<p>Within your home directory on the Open OnDemand VM, Open OnDemand creates an <code>ondemand</code> directory. This is where Open OnDemand stores information about your current session and previous sessions.</p> <p>Every time a job is created by an app, Open OnDemand creates the job files the app needs for it to run, and log files when it is running, within a job-specific job context directory in an app-specific directory.</p> <p>For most back-ends, your home directory is common to both the Open OnDemand VM and the back-ends so your directories and files on the Open OnDemand VM, and changes to these, are reflected on the back-ends and vice-versa.</p> <p>However, you may have access to back-ends where your home directory is not common to both the Open OnDemand VM and the back-end i.e., you have unsynchronised, separate, home directories on each VM. To use such back-ends, you need to do some set up to allow Open OnDemand to automatically copy your <code>ondemand</code> directory, and so your job files, to the back-end when you submit a job.</p> <p>For the back-ends used by this walkthrough this only needs to be done by users of the DataLoch safe haven. Users of other safe havens can skip to the next section, Run the Run Container app below.</p> <p>To use a DataLoch VM to run Open OnDemand apps, please follow the instructions in Enable copy of <code>ondemand</code> directory to a back-end to enable this for the 'desktop' VM on which you are running the browser in which you are using Open OnDemand, then return to this page.</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#run-the-run-container-app","title":"Run the Run Container app","text":"<p>Run Container allows you to run a batch container on a back-end. By batch container we mean a container that performs some computational or data-related task without human interaction when it is running.</p> <p>Click the 'Run Container' app on the Open OnDemand home page.</p> <p>The 'Run Container' app form will open.</p> <p> Excerpt of Run Container app form</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#review-and-complete-the-run-container-app-form","title":"Review and complete the Run Container app form","text":"<p>The app form is prepopulated with the configuration to pull and run a 'hello TRE' container. When run, the container logs a greeting and information about directories mounted into the container.</p> <p>Read the form entries in conjunction with the explanations below and make the suggested changes:</p> <ul> <li>Cluster selects a back-end (cluster) within your safe haven on which to run the container. Back-end-specific short-names are used in the drop-down list, and safe haven-specific back-ends include the text 'tenant', to distinguish them from any TRE-level back-ends to which you might have access.<ul> <li>Select the 'desktop' VM on which you are running the browser in which you are using Open OnDemand.</li> </ul> </li> <li>Container/image URL in container registry cites a URL specifying both the container to run and the container registry from which it is to be pulled.<ul> <li>Leave this value as-is to use the <code>ghcr.io/mikej888/hello-tre:1.0</code> container, hereon termed <code>hello-tre</code>.</li> </ul> </li> <li>Container registry username is a username to access the container registry.<ul> <li>Leave this value as-is.</li> </ul> </li> <li>Container registry access token is an access token to access to the container registry. An access token granting read-only access to the container registry is strongly recommended.<ul> <li>Leave this value as-is, the access token provides read-only access to pull the container.</li> </ul> </li> <li>Container runner is the container runner - 'podman' or 'apptainer' - with which to run container on the back-end. The selected runner must be available on the selected back-end.<ul> <li>Leave this value as-is i.e., 'podman', as this is available on all back-ends.</li> </ul> </li> <li>Container name is the name to be given to the container when it is run. Your job will fail if there is already a running container with that name. If omitted, then the default is <code>CONTAINER_NAME-SESSION_ID</code>, where <code>CONTAINER_NAME</code> is derived from the image name (if the image name is <code>my-container:1.0</code> then <code>CONTAINER_NAME</code> is <code>my-container</code>) and <code>SESSION_ID</code> is a unique session identifier for the app's job.<ul> <li>Leave this value as-is.</li> </ul> </li> <li>Cores is the number of cores/CPUs requested for this job. To run jobs via Open OnDemand requires you to select the resources you think your job will need, including the number of cores/CPUs. Your selected back-end must have at least that number of cores/CPUs request.<ul> <li>Leave this value as-is as the all back-ends can provide the default number of cores, and the <code>hello-tre</code> container does not need any more.</li> </ul> </li> <li>Memory in GiB is the memory requested for this job. Your selected back-end must have at least that amount of memory available.<ul> <li>Leave this value as-is as the all back-ends can provide the default memory, and the <code>hello-tre</code> container does not need any more.</li> </ul> </li> <li>Use GPU? requests that the container use a GPU. If selected, then your selected back-end must have a GPU.<ul> <li>Leave this value as-is, as the <code>hello-tre</code> container does not require a GPU.</li> </ul> </li> <li>Command-line options to pass to container runner are container runner-specific options to control the container runner's behaviour.<ul> <li>Leave this value as-is, as the container does not require any such options to be set.</li> </ul> </li> <li> <p>Environment variables to pass to container are environment variables to be passed on by the container runner and set within the container when it runs. The <code>hello-tre</code> container looks for a <code>HELLO_TRE</code> environment variable. If set, then the container will print the variable's value as a greeting. If undefined, then the greeting is <code>Hello</code>.</p> <ul> <li> <p>Enter:</p> <pre><code>HELLO_TRE=Hello there\n</code></pre> </li> </ul> </li> <li> <p>Arguments to pass to container are container-specific arguments to be passed directly to the container when it runs. The <code>hello-tre</code> container supports two container-specific arguments:</p> <ul> <li>A <code>-d|--duration INTEGER</code> argument which causes the container to sleep (pause) for that number of seconds. If undefined, then the container does not sleep.</li> <li>A <code>-n|--name STRING</code> argument which causes the container to print a greeting with that name. If undefined, then the name is <code>user</code>.</li> <li> <p>Enter the following to request a sleep of 10 seconds and a greeting with your name:</p> <pre><code>-d 10\n-n YOUR_FIRST_NAME\n</code></pre> </li> </ul> </li> </ul>"},{"location":"safe-haven-services/open-ondemand/getting-started/#launch-the-run-container-app-job","title":"Launch the Run Container app job","text":"<p>Click Launch.</p> <p>Open OnDemand will create job files for the app's job in a job-specific job context directory in an app-specific directory under your <code>ondemand</code> directory.</p> <p>Open OnDemand submits the job for the app to a job scheduler which schedules the job onto the back-end based upon the resources - the number of CPUs/cores and amount of memory - requested for your job in the app form. Your job is then queued until sufficient resources are available on the selected back-end to run your job. This will depend upon:</p> <ul> <li>Resources available on your selected back-end.</li> <li>Extent to which jobs currently running on the back-end are using the back-end's resources.</li> <li>Resources requested by your job.</li> <li>Jobs from yourself and others already in the queue for the back-end.</li> </ul> <p>When a job is submitted, a runtime is also requested. If a job takes longer than this runtime, then it is cancelled. The default runtime is 6 hours.</p> <p>Open OnDemand will show an app job card with information about the app's job including:</p> <ul> <li>Job status (on the top right of the job card): initially 'Queued'.</li> <li>'Created at': The time the job was submitted.</li> <li>'Time Requested': The runtime requested for the job.</li> <li>'Session ID': An auto-generated value which is used as the name of the job-specific job context directory. This is a link to open a File Manager pointing at the job context directory.</li> <li>App-specific information, which includes values from the app form:<ul> <li>'Container/image URL in container registry'</li> <li>'Container runner'</li> <li>'Container name'</li> <li>'Cores'</li> <li>'Memory in GiB'</li> </ul> </li> </ul> <p> Run Container app job card showing job status as 'Queued'</p> <p>When the job starts, the Job status on the job card will update to 'Starting' and 'Time Requested' will switch to 'Time Remaining', the time your job has left to run before it is cancelled by the job scheduler.</p> <p>When the Job status updates to 'Running', a Host link will appear on the job card. This is the back-end on which the job, and so the <code>hello-tre</code> container, is now running. A message of form 'Container hello-tre-SESSION_ID is now running. Please wait until the container completes.' will also appear on the job card.</p> <p> Run Container app job card showing job status as 'Running'</p> <p>All going well, the container, and its job, should complete quickly.</p> <p>The Job status on the job card will update to 'Completed'.</p> <p> Run Container app job card showing job status as 'Completed'</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#how-containers-exchange-files-with-back-ends","title":"How containers exchange files with back-ends","text":"<p>Open OnDemand uses TRE Container Execution Service tools to run containers and containers run via Open OnDemand must conform to the requirements of the TRE Container Execution Service, and <code>hello-tre</code> does. For this walkthrough, the key points are that containers need to support three directories, so that when the container is run, three directories on the back-end can be mounted into the container:</p> Back-end directory Container directory Description Project-specific <code>/safe_data</code> subdirectory <code>$HOME/safe_data</code> OR <code>/safe_data/PROJECT_SUBDIRECTORY</code> If <code>$HOME/safe_data</code> exists in your home directory on the back-end, then that is mounted. Otherwise, a subdirectory of <code>/safe_data</code> corresponding to your project (and inferred from your user group) is mounted, if such a subdirectory can be found. <code>$HOME/safe_outputs/APP_SHORT_NAME/SESSION_ID</code> <code>/safe_outputs</code> <code>APP_SHORT_NAME</code> is a short-name for an app (e.g., <code>jupyter</code> for Run JupyterLab). <code>SESSION_ID</code> a unique session identifier created when an app is run. This directory is created in your home directory on the back-end when your container runs. The directory persists after the job which created the container ends. <code>$HOME/scratch/APP_SHORT_NAME/SESSION_ID</code> <code>/scratch</code> <code>APP_SHORT_NAME</code> and <code>SESSION_ID</code> are as above. This directory is also created in your home directory on the back-end when your container runs. This directory exists for the duration of the job which created the container. The <code>SESSION_ID</code> sub-directory is deleted when the job which created the container ends. <p>Together, these mounts provides a means for data, configuration files, scripts and code to be shared between the back-end on which the container is running and the environment within the container itself. Creating or editing a file within any of these directories on the back-end means that the changes will be available within the container, and vice-versa.</p> <p>When the <code>hello-tre</code> container is run, it writes two files into <code>/safe_outputs</code> within the container, and so into a <code>$HOME/outputs-NUMBER</code> on your home directory on the back-end:</p> <ul> <li><code>safe_data.txt</code>, which lists a selection of directories and files in the <code>/safe_data/PROJECT_SUBDIRECTORY</code> directory that was mounted into the container at <code>/safe_data</code>.</li> <li><code>safe_outputs.txt</code> which has a <code>This text is in safe_outputs.txt</code> message.</li> </ul>"},{"location":"safe-haven-services/open-ondemand/getting-started/#view-the-containers-output-files","title":"View the container's output files","text":"<p>As mentioned earlier, for most back-ends, your home directory is common to both the Open OnDemand VM and the back-ends so any files created within your home directory on a back-end will be available on the Open OnDemand VM, and vice-versa. This includes the contents of the <code>safe_outputs/APP_SHORT_NAME/SESSION_ID</code> and <code>scratch/APP_SHORT_NAME/SESSION_ID</code> directories. However, your project data files, in a project-specific directory under <code>/safe_data</code> are not available on the Open OnDemand VM.</p> <p>For DataLoch users, your home directory is not common to both the Open OnDemand VM and the back-end, so you cannot use the File Manager to browse files created by the container. However, another way of viewing these files will be described shortly.</p> <p>View the <code>safe_outputs/container/SESSION_ID</code> directory via the Open OnDemand File Manager:</p> <ol> <li>Select the Files menu, Home Directory option to open the File Manager.</li> <li>Click Home Directory, to go to your home directory.</li> <li>Click <code>safe_outputs/container/SESSION_ID</code> view the directory</li> <li>Click on <code>safe_data.txt</code> and <code>safe_outputs.txt</code> to view their contents.</li> </ol> <p> File Manager showing home directory after Run Container app completes</p> <p> File Manager showing outputs directory contents after Run Container app completes</p> <p>An alternative to the File Manager is to log in to the back-end and view the files there, which can be done for any back-end.</p> <p>View the <code>safe_outputs/container/SESSION_ID</code> directory within the back-end:</p> <ol> <li>Select Clusters menu, back-end Shell Access option, to log into the back-end.</li> <li> <p>Change into your home directory and view the directory and its files and their contents.</p> <pre><code>cd\nls safe_outputs/container/SESSION_ID\ncat safe_outputs/container/SESSION_ID/safe_data.txt\ncat safe_outputs/container/SESSION_ID/safe_outputs.txt\n</code></pre> </li> </ol> <p>As you have accessed Open OnDemand from your 'desktop' VM, you could also access the files directly on your 'desktop' VM, but we used the back-end Shell Access option to introduce this feature of Open OnDemand.</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#view-the-app-log-file-within-the-job-context-directory","title":"View the app log file within the job context directory","text":"<p>When an app job runs, a log file is created within the job-specific job context directory in an app-specific directory under your <code>ondemand</code> directory. This log file includes information from the app itself plus logs captured from the container as it runs. It can be useful to check the log file when debugging.</p> <p>For the <code>hello-tre</code> container, the logs includes information about the mounts and also a greeting and sleep (pause) information based on the environment variable and container arguments you defined in the app's form.</p> <p>As for the output files, you can use either the File Manager (non-DataLoch safe haven users only) or log into the back-end (all users) to view the log file.</p> <p>View the log file via the Open OnDemand File Manager:</p> <ol> <li>Click the Session ID link in the job card to open the File Manager, pointing at the job context directory for the job on the Open OnDemand VM.</li> <li>Click on the log file, <code>output.log</code>.</li> </ol> <p> File Manager showing log file within Run Container app's job context directory</p> <p>View the log file within the back-end:</p> <ol> <li>Select Clusters menu, back-end Shell Access option to log into the back-end.</li> <li> <p>Change into the job context directory for the job on the back-end and show the log file where <code>SESSION_ID</code> can be seen on the Session ID link on the job card:</p> <pre><code>cd ondemand/data/sys/dashboard/batch_connect/sys/container_app/output/SESSION_ID/\n</code></pre> </li> <li> <p>View the log file:</p> <pre><code>cat output.log\n</code></pre> </li> </ol> <p>For the <code>hello-tre</code> container, the log file includes four types of log information. There is information from the app itself and it sets itself up to run the container:</p> <pre><code>Wed Jul 30 11:32:41 UTC 2025 before.sh: JOB_FOLDER: /home/eidf147/eidf147/mikej147/ondemand/data/sys/dashboard/batch_connect/sys/container_app/output/4e0efea9-c556-4800-bcfd-414dbd92ed3c\nScript starting...\n...\nWed Jul 30 11:32:41 UTC 2025 script.sh: Running ces-pull podman ...\n...\n</code></pre> <p>This is followed by information from the container itself about your user name within the container and the directories mounted into the container:</p> <pre><code>Hello TRE!\n\nYour container is now running.\n\nYour user 'id' within the container is: uid=0(root) gid=0(root) groups=0(root).\n\nCheck mounted directories, ownership, permissions, file system type:\n/safe_data: nobody (65534) root(0) drwxrwx--- nfs\n/scratch: root (0) root(0) drwxr-xr-x ext2/ext3\n/safe_outputs: root (0) root(0) drwxr-xr-x ext2/ext3\n\nCheck read/write access to mounted directories\n\nList /safe_data contents and write to /safe_outputs/safe_data_files.out\nCheck write to /safe_outputs\nContents of /safe_outputs/safe_outputs.txt:\nThis text is in safe_outputs.txt\nCheck write to /scratch\nContents of /scratch/scratch.txt:\nThis text is in scratch.txt\n\nLook for optional 'HELLO_TRE' environment variable\nFound optional 'HELLO_TRE' environment variable with value: Hello there\n\nParse command-line arguments\nNumber of arguments: 4\nArguments (one per line):\n    -d\n    10\n    -n\n    Mike\n</code></pre> <p>For some containers run via Podman, including <code>hello-tre</code>, you are the 'root' user within the container but only within the container. This is why the files in the mounts belong to a 'root' or 'nobody' user and 'root' group when accessed from within the container. Any files you create in the mounted directories will be owned by your own user, and user group, on the back-end. You can check this yourself by inspecting the file ownership of the files within <code>safe_outputs/container/SESSION_ID</code>.</p> <p>Returning to the log file, there is information from the container itself about your user name within the container and the directories mounted into the container, including a message created using the value of the <code>HELLO_TRE</code> environment variable and the <code>-n</code> container argument, messages indicating that the container is sleeping for the duration specified by the <code>-d</code> container argument, and a farewell message, again using the <code>-n</code> container argument.</p> <pre><code>Hello there Mike!\n\nSleeping for 10 seconds...\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...and awake!\n\nFor more container examples and ideas, visit:\n  https://github.com/EPCCed/tre-container-samples\nGoodbye Mike!\n</code></pre> <p>Finally, the log file includes information from the app itself as it completes:</p> <pre><code>Cleaning up...\n</code></pre>"},{"location":"safe-haven-services/open-ondemand/getting-started/#run-the-active-jobs-app","title":"Run the Active Jobs app","text":"<p>Active Jobs allows you to see which of your jobs have been submitted, are running, or have completed.</p> <p>Click the 'Active Jobs' app on the Open OnDemand home page.</p> <p>The Active Jobs app will open to show a table of running and recently completed jobs.</p> <p>You will see a 'container_app' entry for your app's job.</p> <p>Your job will have a status of 'Completed'.</p> <p>Each job has a unique job ID created by the job scheduler when you submitted the job. Unfortunately, the job ID is not the same as the session ID for an app created by Open OnDemand. Rather, the job ID is created by the job scheduler. Each job created by an app has both an Open OnDemand session ID and a job scheduler job ID.</p> <p>To see more details about the job, click the &gt; button, by the job.</p> <p> Active Jobs app showing details of completed Run Container app job</p> <p>If any app does not run promptly, but is in a 'Queued' state, then the Active Jobs app can provide you with information on other jobs that are running and for which you may have to wait until one or more have completed before your app's job runs.</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#run-the-run-jupyterlab-app","title":"Run the Run JupyterLab app","text":"<p>In contrast to Run Container, which runs a container on a back-end without human interaction when it is running, Run JupyterLab allows you to run a JupyterLab container on a back-end, which creates an interactive JupyterLab service you can use. Please be reassured that no Python knowledge is assumed or required!</p> <p>Click the 'Run JupyterLab' app on the Open OnDemand home page.</p> <p>The Run JupyterLab app form will open.</p> <p> Excerpt of Run JupyterLab app form</p> <p>This app's form has far less settings since it is designed to run, using Podman, a JupyterLab container created for use with the TRE Container Execution Service.</p> <p>For Cluster, select the 'desktop' VM on which you are running the browser in which you are using Open OnDemand.</p> <p>Leave the other settings as-is.</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#launch-the-run-jupyterlab-app-job","title":"Launch the Run JupyterLab app job","text":"<p>Click Launch.</p> <p>Again, Open OnDemand will create job files for the app in a job-specific job context directory in an app-specific directory under your <code>ondemand</code> directory and then submits the job for the app to the job scheduler.</p> <p>Again, Open OnDemand will show an app job card with information about the app's job including:</p> <ul> <li>Job status (on the top right of the job card): initially 'Queued'.</li> <li>'Created at': The time the job was submitted.</li> <li>'Time Requested': The runtime requested for the job which defaults to 6 hours.</li> <li>'Session ID': An auto-generated value which is used as the name of the job-specific job context directory. This is a link to open a File Manager pointing at the job context directory.</li> <li>App-specific information, which includes values from the app form:<ul> <li>'Container name'</li> <li>'Connection timeout': when the app's job starts running, the app will then wait for JupyterLab to become available within the container. If this does not occur within this app-specific period (180 seconds i.e., 3 minutes), then the app's job will cancel itself.</li> <li>'Cores'</li> <li>'Memory in GiB'</li> </ul> </li> </ul> <p> Run JupyterLab app job card showing job status as 'Queued'</p> <p>When the job starts, the Job status on the job card will update to 'Starting' and 'Time Requested' will switch to 'Time Remaining', the time your job has left to run before it is cancelled by the job scheduler.</p> <p>When the Job status updates to 'Running', a Host link will appear on the job card, which allows you to log in to the back-end on which the job, and so the JupyterLab container, is now running. A 'JupyterLab running in container epcc-ces-jupyter-SESSION_ID' message will appear along with a Connect to JupyterLab button. The JupyterLab container is now ready for use.</p> <p> Run JupyterLab app job card showing job status as 'Running'</p> <p>Click Connect to JupyterLab. A new browser tab will open with JupyterLab.</p> <p>You may wonder why you were not prompted for a username and password. JupyterLab running in the container runs as a 'root' user. The 'root' user is within the context of the container only. JupyterLab is protected with an auto-generated password. The Connect to JupyterLab button is configured to log you into the container using this password automatically.</p> <p> JupyterLab running within a container</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#troubleshooting-proxy-error","title":"Troubleshooting: Proxy Error","text":"<p>If you click Connect to JupyterLab and get:</p> <p>Proxy Error</p> <p>The proxy server received an invalid response from an upstream server. The proxy server could not handle the request</p> <p>Reason: Error reading from remote server</p> <p>Apache/2.4.52 (Ubuntu) Server at host Port 443</p> <p>then, this can arise as sometimes there is a lag between the container having started and JupyterLab within the container being ready for connections.</p> <p>Wait 30 seconds, then refresh the web page, or click the Connect to JupyterLab button again.</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#use-jupyterlab-to-explore-how-directories-on-a-back-end-are-mounted-into-a-container","title":"Use JupyterLab to explore how directories on a back-end are mounted into a container","text":"<p>We can use JupyterLab to further explore how directories on a back-end are mounted into a container.</p> <p>Click the Host link to log into the back-end on which the job, and JupyterLab container, is running.</p> <p>Now, within JupyterLab, click the Terminal icon within the 'Launcher' tab. This opens up a command-line session within the container.</p> <p>Now run the following:</p> <pre><code>ls /safe_data/\n</code></pre> <p>You will see the contents of your <code>/safe_data/PROJECT_SUBDIRECTORY</code> on the back-end.</p> <p> Viewing mounted directories within JupyterLab</p> <p>Check this by running, in your Open OnDemand command-line session with the back-end:</p> <pre><code>ls /safe_data/PROJECT_SUBDIRECTORY/\n</code></pre> <p>The same files and subdirectories should be listed.</p> <p>Now, within the JupyterLab Terminal, create a file in each directory (<code>touch</code> creates an empty file):</p> <pre><code>touch /scratch/hello-from-jupyterlab-to-scratch.txt\ntouch /safe_outputs/hello-from-jupyterlab-to-outputs.txt\n</code></pre> <p>List the contents of the <code>scratch</code> and <code>safe_outputs</code> and directories:</p> <pre><code>ls -1 scratch/jupyter/SESSION_ID\nls -1 safe_outputs/jupyter/SESSION_ID\n</code></pre> <p>You should see the above files:</p> <pre><code>hello-from-jupyterlab-to-scratch.txt\nhello-from-jupyterlab-to-outputs.txt\n</code></pre> <p>Now, within your Open OnDemand command-line session with the back-end, create files in these directories:</p> <pre><code>touch scratch/jupyter/SESSION_ID/hello-from-scratch-to-jupyterlab.txt\ntouch safe_outputs/jupyter/SESSION_ID/hello-from-outputs-to-jupyterlab.txt\n</code></pre> <p>Then, within the JupyterLab Terminal, list the contents of the corresponding <code>/scratch</code> directory and you should see the files you created on the back-end plus those you created within JupyterLab:</p> <pre><code>ls -1 /scratch/\n</code></pre> <pre><code>hello-from-jupyterlab-to-scratch.txt\nhello-from-scratch-to-jupyterlab.txt\n</code></pre> <p>And similarly for <code>/safe_outputs</code>:</p> <pre><code>ls -1 /safe_outputs/\n</code></pre> <pre><code>hello-from-jupyterlab-to-outputs.txt\nhello-from-outputs-to-jupyterlab.txt\n</code></pre> <p>Hopefully, this demonstrates how the mounted directories provides a means for data, configuration files, scripts and code to be shared between the back-end on which the container is running and the environment within the container itself.</p> <p>As a reminder, <code>safe_outputs/jupyter/SESSION_ID</code> will persist after the job which created the container ends but the <code>SESSION_ID</code> subfolder in <code>scratch/jupyter</code> will be deleted.</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#revisit-the-active-jobs-app","title":"Revisit the Active Jobs app","text":"<p>Click the 'Active Jobs' app on the Open OnDemand home page.</p> <p>You will see a 'ood_jupyter_app' entry for your app's job.</p> <p>Your job will have a status of 'Running'.</p> <p> Active Jobs app showing details of running Run JupyterLab app job</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#finish-your-run-jupyterlab-app-job","title":"Finish your Run JupyterLab app job","text":"<p>You can end your job by as follows:</p> <ul> <li>Either, shut down JupyterLab via the File menu, Shut Down option.</li> <li>Or, click Cancel on the app's job card.</li> </ul> <p>The Job status on the job card will update to 'Completed'.</p> <p> Run JupyterLab app job card showing job status as 'Completed'</p> <p>Click the 'Active Jobs' app on the Open OnDemand home page.</p> <p>Your job will now have a status of 'Completed'.</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#use-homesafe_data","title":"Use <code>$HOME/safe_data</code>","text":"<p>As mentioned, if <code>$HOME/safe_data</code> exists in your home directory on the back-end, then that is mounted into a container. Otherwise, a subdirectory of <code>/safe_data</code> corresponding to your project (and inferred from your user group) is mounted, if such a subdirectory can be found.</p> <p>Using the File Manager, or via a session on the back-end accessed from within Open OnDemand, or on the 'desktop' VM from which you accessed Open OnDemand, create a <code>$HOME/safe_data</code> directory and then create some files in it. For example:</p> <pre><code>mkdir $HOME/safe_data/\ntouch $HOME/safe_data/a.txt\ntouch $HOME/safe_data/b.txt\ntouch $HOME/safe_data/c.txt\n</code></pre> <p>Rerun the Run JupyterLab app and, again, once JupyterLab has started, click the Terminal icon within the 'Launcher' tab.</p> <p>List the contents of <code>/safe_data</code>:</p> <pre><code>ls -1 /safe_data/\n</code></pre> <p>and you should see the files you created:</p> <pre><code>a.txt\nb.txt\nc.txt\n</code></pre> <p>Remember to delete <code>$HOME/safe_data</code> when you are done.</p>"},{"location":"safe-haven-services/open-ondemand/getting-started/#more-information","title":"More information","text":"<p>The following pages provide detailed information about all aspects of Open OnDemand introduced in this walkthrough:</p> <ul> <li>Run jobs</li> <li>Run containers</li> <li>View and run apps</li> <li>Browse and manage files</li> <li>Log into back-ends</li> </ul>"},{"location":"safe-haven-services/open-ondemand/home-page/","title":"Open OnDemand home page","text":""},{"location":"safe-haven-services/open-ondemand/home-page/#introduction","title":"Introduction","text":"<p>Open OnDemand's home page provides menus and icons to allow you to access Open OnDemand's features.</p> <p> Open OnDemand home page</p> <p>Open OnDemand's features are described on the following pages:</p> <ul> <li>Run jobs</li> <li>Run containers</li> <li>View and access apps</li> <li>Browse and manage files</li> <li>Log into back-ends</li> </ul> <p>General purpose features are described below.</p>"},{"location":"safe-haven-services/open-ondemand/home-page/#restart-your-open-ondemand-session","title":"Restart your Open OnDemand session","text":"<p>Select the Help (?) menu, Restart Web Server option to restart your Open OnDemand session.</p> <p> Help (?) menu button</p> <p>Info</p> <p>Despite its name, this option does not restart the Open OnDemand web server! It restarts your session only. It does not affect other users!</p> <p>Tip</p> <p>If the Open OnDemand service or apps have been updated during your session, then Restart Web Server allows you to pick up such changes without having to log out and log back into Open OnDemand,</p>"},{"location":"safe-haven-services/open-ondemand/home-page/#display-your-log-in-name","title":"Display your log in name","text":"<p>Click the Avatar (head and shoulders icon) on the menu bar to display your log in name e.g., 'Logged in as some-user'.</p> <p> Avatar menu button</p>"},{"location":"safe-haven-services/open-ondemand/home-page/#log-out","title":"Log out","text":"<p>Click Log Out (right arrow icon) on the menu bar to log out of Open OnDemand.</p> <p> Log Out menu button</p>"},{"location":"safe-haven-services/open-ondemand/jobs/","title":"Run jobs","text":""},{"location":"safe-haven-services/open-ondemand/jobs/#introduction","title":"Introduction","text":"<p>Open OnDemand allows you to run compute and data-related tasks on compute resources available to your safe haven.</p> <p>Certain users of certain safe havens may also have access to TRE-level compute resources, for example, the Superdome Flex high-performance computing cluster.</p> <p>This page introduces how Open OnDemand runs tasks, and information you need to know about when running tasks. Run containers focuses on aspects of running containers within this job execution environment.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#back-ends-clusters-jobs-and-apps","title":"Back-ends, clusters, jobs and apps","text":"<p>A compute resource upon which tasks can be run is called a back-end, or, in some parts of Open OnDemand, a cluster.</p> <p>Each run of a task on a back-end is called a job</p> <p>An Open OnDemand component that allows you to run jobs, or other useful functions, is called an app.</p> <p>Many apps allow you to run jobs on back-ends. However, other apps perform other useful functions, for example, the Active Jobs app which allows you to see which of your jobs have been submitted, are running, or have completed.</p> <p>A subset of apps that run jobs on back-ends are called interactive apps. All Container Execution Service apps that run containers are classed, in Open OnDemand terms, as 'interactive' even those apps that run non-interactive containers!</p> <p>Info</p> <p>In standard deployments of Open OnDemand, interactive apps refer only to apps that run web- or GUI-based services or software. However, within the TRE Open OnDemand service, Open OnDemand's application programming interface for interactive apps is used to implement both apps that run containers that run such services and those that run any other containers, because that interface is easier to implement apps with than that for non-interactive (in the Open OnDemand sense) apps!</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#back-end-cluster-names","title":"Back-end (cluster) names","text":"<p>Within Open OnDemand, back-ends are typically referred to via human-readable names. A selection of back-end names includes:</p> <ul> <li>DataLoch 1234 5678 GPU server</li> <li>eDRIS National Safe Haven GPU desktop 01</li> <li>ODAP GPU desktop 01</li> <li>Smart Data Foundry GPU desktop 01</li> <li>Superdome Flex</li> </ul> <p>A convention is adopted whereby safe haven-specific back-ends always cite the safe haven name.</p> <p>Within some interactive apps, you will see back-ends referred to via 'short-names'. Typically, these short-names are derived from the back-ends' VM names. However, a convention is adopted whereby safe haven-specific back-ends include the text 'tenant', to distinguish them from any TRE-level back-ends to which you might have access. So, for example, the short-names corresponding to the above back-ends are:</p> <ul> <li>dap_tenant_1234_5678</li> <li>nsh_tenant_gpu_desktop01</li> <li>odp_tenant_gpu_desktop01</li> <li>smartdf_tenant_gpu_desktop01</li> <li>shs_sdf01 - as the Superdome Flex is a TRE-level, not safe haven-specific, back-end its short-name does not include 'tenant'.</li> </ul> <p>Within job cards on the My Interactive Sessions page, described below, you will see the VM names upon which the jobs are running.</p> <p>Info</p> <p>The use of 'tenant' in short-names is adopted as a means to exploit Open OnDemand's use of filters to constrain certain apps to only be applicable to certain back-ends.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#job-scheduling-and-execution","title":"Job scheduling and execution","text":"<p>To run a job, including those created by apps, you need to select the resources - the number of CPUs/cores and amount of memory - you think your job will need.</p> <p>Open OnDemand then submits the job to a job scheduler which schedules the job onto the selected back-end based upon the resources requested. Your job is then queued until sufficient resources are available on the selected back-end to run your job. This will depend upon:</p> <ul> <li>Resources available on your selected back-end.</li> <li>Extent to which jobs currently running on the back-end are using the back-end's resources.</li> <li>Resources requested by your job.</li> <li>Jobs from yourself and others already in the queue.</li> </ul> <p>Note</p> <p>Open OnDemand and the back-ends use the Slurm open source job scheduler and workload manager to schedule and run jobs on back-ends.</p> <p>Unless you are using the Job Composer app, you should not have to worry about the details of how Slurm works. Open OnDemand's user interface and apps are designed to hide its details from users.</p> <p>Tip</p> <p>As back-ends are used by multiple users, be considerate to other users, and take care to request only the resources you think your job will need. This helps to ensure fair use for everyone.</p> <p>Tip</p> <p>See Open OnDemand tips for tips and troubleshooting relating to relating to requesting resources and job queues.</p> <p>When a job is submitted, a runtime is also requested. If a job takes longer than this runtime, then it is cancelled.</p> <p>Warning</p> <p>Container Execution Service apps will run for a maximum of 6 hours.</p> <p>Warning</p> <p>Any running jobs, and containers, will be cancelled during the monthly TRE maintenance period.</p> <p>For interactive apps, Open OnDemand uses the job scheduler to determine when the job has started. Apps that run interactive services (e.g., JupyterLab or RStudio Server) will then wait for the service to become available. If this does not occur within an app-specific period, the connection timeout, then the app's job will cancel itself.</p> <p>Note</p> <p>In standard deployments of Open OnDemand, the notification sent by the app includes information required by Open OnDemand to display how to connect to web- or GUI-based services started by the app. However, as mentioned above, all Container Execution Service apps that run containers use Open OnDemand's application programming interface for interactive apps, so you may see the connection timeout for the apps that run non-interactive containers too.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#your-ondemand-directory","title":"Your <code>ondemand</code> directory","text":"<p>Within your home directory on the Open OnDemand VM, Open OnDemand creates an <code>ondemand</code> directory. This is where Open OnDemand stores information about your current session and previous sessions.</p> <p>Every time a job is created by an app, Open OnDemand creates the job files for the app in a job-specific job context directory in an app-specific directory.</p> <p>Job Composer app job files are created in a directory:</p> <pre><code>$HOME/ondemand/data/sys/myjobs/projects/default/JOB_COMPOSER_ID/\n</code></pre> <p>where <code>JOB_COMPOSER_ID</code> is a unique job ID created by the app. For example:</p> <pre><code>$HOME/ondemand/data/sys/myjobs/projects/default/1/\n</code></pre> <p>Interactive app job files are created in a directory:</p> <pre><code>$HOME/ondemand/data/sys/dashboard/batch_connect/sys/APP_NAME/output/SESSION_ID/\n</code></pre> <p>where <code>APP_NAME</code> is the app name and <code>SESSION_ID</code> a unique session identifier. For example,</p> <pre><code>$HOME/ondemand/data/sys/dashboard/batch_connect/sys/container_app/output/e0b9deeb-4b9c-43f8-ad3f-1c85074a1485/\n</code></pre> <p>Open OnDemand caches information within this <code>ondemand</code> directory including information on previous jobs and information you last entered within app forms.</p> <p>Warning</p> <p>You can delete the <code>ondemand</code> directory - Open OnDemand will recreate it the next time you log in - but your history of previous jobs, and any cached values, will be lost.</p> <p>Tip</p> <p>You may find it useful to delete specific job context subdirectories from the <code>ondemand</code> directory if the <code>ondemand</code> directory grows to large.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#unsynched-home-directories","title":"Unsynched home directories","text":"<p>For most back-ends, your home directory is common to both the Open OnDemand VM and the back-ends so your directories and files on the Open OnDemand VM, and changes to these, are reflected on the back-ends and vice-versa.</p> <p>However, you may have access to back-ends where your home directory is not common to both the Open OnDemand VM and the back-end i.e., you have unsynched, separate, home directories on each VM.</p> <p>Currently, the back-ends where home directories are not common to both the Open OnDemand VM and the back-ends are as follows:</p> <ul> <li>Superdome Flex, shs-sdf01.nsh.loc.</li> <li>All DataLoch VMs.</li> </ul> <p>To use such back-ends, you need to do some set up to allow Open OnDemand to automatically copy your <code>ondemand</code> directory, and so your job files, to the back-end when you submit a job. How to enable this is described in the following section on Enable copy of <code>ondemand</code> directory to a back-end.</p> <p>You will also have to log into these back-end to view files created on these back-ends when you run jobs - see Log into back-ends</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#enable-automated-copy-of-ondemand-directory-to-a-back-end","title":"Enable automated copy of <code>ondemand</code> directory to a back-end","text":"<p>To enable Open OnDemand to automatically copy your <code>ondemand</code> directory to a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, you need to set up a passphrase-less SSH key between the Open OnDemand VM and the back-end.</p> <p>Note</p> <p>Setting up SSH keys does not need to be done for back-ends where your home directory is common to both on the Open OnDemand VM and the back-ends.</p> <p>Set up a passphrase-less SSH key between the Open OnDemand VM and the back-end:</p> <ol> <li>Select Clusters menu, Open OnDemand host Shell Access option.</li> <li>A new browser tab with an SSH session to the back-end will appear.</li> <li>When prompted, enter your project username and password.</li> <li> <p>Create a passphrase-less SSH key:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"open-ondemand\" -N \"\"\n</code></pre> </li> <li> <p>Copy public key to back-end:</p> <pre><code>ssh-copy-id BACK-END-HOSTNAME.nsh.loc\n</code></pre> <p>Information on the copy will be output:</p> <pre><code>/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/home/user/.ssh/id_rsa.pub\"\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\n(user@BACK-END-HOSTNAME.nsh.loc) Password:\n</code></pre> </li> <li> <p>When prompted, enter your project username and password. The key will then be added to the back-end:</p> <pre><code>Number of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'BACK-END-HOSTNAME.nsh.loc'\"\n\nand check to make sure that only the key(s) you wanted were added.\n</code></pre> </li> <li> <p>Check passphrase-less access to back-end:</p> <pre><code>ssh BACK-END-HOSTNAME.nsh.loc hostname\n</code></pre> <p>For example:</p> <pre><code>BACK-END-HOSTNAME.nsh.loc hostname\n</code></pre> </li> <li> <p>You should not be prompted for a passphrase or password.</p> </li> </ol>"},{"location":"safe-haven-services/open-ondemand/jobs/#what-happens-when-a-job-is-submitted","title":"What happens when a job is submitted","text":"<p>Briefly, when a job is submitted, the following occurs:</p> <ol> <li> <p>Open OnDemand creates a job context directory under your <code>ondemand</code> directory.</p> <ul> <li>For the Job Composer app:</li> </ul> <pre><code>ondemand/data/sys/myjobs/projects/default/JOB_COMPOSER_ID/\n</code></pre> <ul> <li>For interactive apps:</li> </ul> <pre><code>ondemand/data/sys/dashboard/batch_connect/sys/APP_NAME/output/SESSION_ID/\n</code></pre> </li> <li> <p>Open OnDemand submits the job to the job scheduler to run the job on your chosen back-end.</p> <ul> <li>A job scheduler preprocessing step is used to create a log file in an <code>ondemand/logs/slurm</code> directory.</li> <li>For back-ends where your home directory is not common to both both the Open OnDemand VM and the back-end, a job scheduler preprocessing step automatically copies your <code>ondemand</code> directory to the back-end.</li> </ul> </li> <li>The job scheduler queues your job, pending processing and memory resources on the back-end becoming available. The job status will be 'Queued'.</li> <li>When resources become available on the back-end, your job runs:<ul> <li>For jobs created via the Job Composer app, the job status will be 'Running'.</li> <li>For jobs created via apps, the job status will be 'Starting' and, when a notification is received from the running app by Open OnDemand, the job status will switch to 'Running'.</li> </ul> </li> <li>Your job will complete. The job status will be 'Completed'.</li> </ol> <p>Note</p> <p>The job status does not display whether a job that is 'Completed' did so with success or failure. Whether a job succeeded or failed can be seen in the job details for the job which can be seen via the Active Jobs app.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#browse-and-manage-jobs","title":"Browse and manage jobs","text":"<p>You can browse and manage jobs via the Active Jobs app.</p> <p>For interactive app jobs (not those created by the Job Composer app), you can also use the My Interactive Sessions page, which provides more app-specific information.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#my-interactive-sessions-page","title":"My Interactive Sessions page","text":"<p>Click My Interactive Sessions (overlaid squares icon) on the menu bar to open the My Interactive Sessions page.</p> <p> My Interactive Sessions menu button</p> <p>The My Interactive Sessions page shows app-specific jobs that have been submitted, are running, or have completed. Each job has a job card.</p> <p>Note</p> <p>Only information for jobs arising from what Open OnDemand terms 'interactive apps' is shown. All Container Execution Service apps are classed as 'interactive apps'. Information on jobs submitted by Open OnDemand's Job Composer app are shown on that app's own page.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#job-cards","title":"Job cards","text":"<p>When an interactive app's job is submitted, a job card is created and shown with information about the app's job including:</p> <ul> <li>Job status (on the top right of the job card): One of 'Queued', 'Starting', 'Running', 'Held', 'Suspended', 'Completed', 'Undetermined'.</li> <li>'Host': For 'Running' jobs, the back-end on which the job is running.</li> <li>'Created at': For 'Queued' jobs, the time the job was submitted.</li> <li>'Time Requested': For 'Queued' jobs, the runtime requested for the job.</li> <li>'Time Remaining': For 'Starting' and 'Running' jobs, the runtime remaining.</li> <li>App-specific information, which includes values from the app form.<ul> <li>For some apps, this will include the 'Connection timeout'.</li> </ul> </li> <li>App-specific status information, and, for apps that run containers with interactive web- or GUI-based services, a button to connect to the service.</li> </ul> <p> Example job card for the Run Container app</p> <p> Example job card for the Run JupyterLab app</p> <p>Note</p> <p>The job status does not display whether a job that is 'Completed' did so with success or failure. Whether a job succeeded or failed can be seen in the job details for the job which can be seen via the Active Jobs app.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#log-in-to-back-end-on-which-job-is-running","title":"Log in to back-end on which job is running","text":"<p>For 'Running' jobs, click the Host link to log into the back-end on which the job is running.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#open-file-manager-to-job-context-directory","title":"Open File Manager to job context directory","text":"<p>Click the Session ID link to open the File Manager, pointing at the job context directory for the job on the Open OnDemand VM.</p> <p>Note</p> <p>When using a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, then, if your job creates files on the back-end, you will have to log into the back-end to view your files.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#cancel-a-job","title":"Cancel a job","text":"<p>Click Cancel on a job card to cancel a running job.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#relaunch-a-job","title":"Relaunch a job","text":"<p>Click Relaunch Job (circling arrows icon) on a job card to relaunch a previously cancelled or completed job. A new session ID, and new set of job files, using the same configuration as for the previous run of the app, will be created.</p> <p> Relaunch Job button</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#delete-a-job-card","title":"Delete a job card","text":"<p>Click Delete on a job card to delete the job card.</p> <p>Note</p> <p>Deleting a job card does not delete the associated job context directory from the <code>ondemand</code> directory.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#log-files","title":"Log files","text":""},{"location":"safe-haven-services/open-ondemand/jobs/#job-scheduler-log-files","title":"Job scheduler log files","text":"<p>When a job is submitted to a back-end, a log file is created within an <code>ondemand/logs/slurm</code> directory within your home directory on the Open OnDemand VM.</p> <p>Log files have name <code>sbatch-YYYYMMDD-HHMMSS_OPEN_ONDEMAND_CLUSTER_NAME</code>. For example, <code>sbatch-20240807-082901-nsh_tenant_gpu_desktop01</code>.</p> <p>An example of the contents of a log file is as follows:</p> <pre><code># Open OnDemand back-end: OPEN_ONDEMAND_CLUSTER_NAME\n# Time: YYYY-MM-DD HH:MM:SS\n# Process: PROCESS-ID\n# Open OnDemand server environment\n...values environment variables in current Open OnDemand environment...\n# sbatch arguments from Open OnDemand\n...arguments passed from Open OnDemand to 'sbatch' command which runs job...\n</code></pre> <p>Note</p> <p>You should not have to concern yourself with the contents of these log files but they might prove useful if you need help with troubleshooting issues with running jobs via Open OnDemand.</p> <p>Tip</p> <p>You can safely delete these log files, if they're taking up too much space.</p>"},{"location":"safe-haven-services/open-ondemand/jobs/#app-log-files","title":"App log files","text":"<p>When an app job runs, a log file is created within the job-specific job context directory in an app-specific directory under your <code>ondemand</code> directory. This log file includes information from the app itself plus logs captured from anything spawned by the app, for example, containers, as these runs.</p> <p>It can be useful to check the log file when debugging.</p> <p>Depending on the app implementation, the log file may include a job ID, a unique job ID created by the job scheduler, when you submitted the job.</p> <p>Note</p> <p>The job ID is not the same as the session ID used for interactive apps or the job composer ID used by the Job Composer. Rather, the job ID is created by the job scheduler.</p> <p>Each job created by an app has both an app ID and a job scheduler job ID.</p>"},{"location":"safe-haven-services/open-ondemand/ssh/","title":"Log into back-ends","text":""},{"location":"safe-haven-services/open-ondemand/ssh/#introduction","title":"Introduction","text":"<p>Open OnDemand allows you to log into a back-end or the Open OnDemand VM (if applicable) in a number of ways.</p> <p>In each case, a new browser tab will open for the selected VM.</p> <p>When prompted, enter your project username and password.</p> <p>Info</p> <p>If you see 'Open OnDemand host Shell Access' listed as an option anywhere, then you can log into the Open OnDemand VM. This is supported for users who have access to back-ends where user home directories are not common to both the Open OnDemand VM and those back-ends.</p>"},{"location":"safe-haven-services/open-ondemand/ssh/#clusters-menu","title":"Clusters menu","text":"<p>The Clusters menu lists every back-end to which you have access, and the Open OnDemand VM (if applicable).</p> <p>Select a VM-specific menu option to log into the selected VM.</p> <p>When prompted, enter your project username and password.</p>"},{"location":"safe-haven-services/open-ondemand/ssh/#all-apps-page-cluster-apps","title":"All Apps page cluster apps","text":"<p>Select the Apps menu, All Apps option to go to the All Apps page.</p> <p>The All Apps page lists every back-end to which you have access, and the Open OnDemand VM (if applicable).</p> <p>Click a VM-specific Shell Access link to log into the selected VM.</p>"},{"location":"safe-haven-services/open-ondemand/ssh/#file-manager-open-in-terminal-button","title":"File Manager Open in Terminal button","text":"<p>Select the Files menu, Home Directory option to open the File Manager, then click Open in Terminal.</p>"},{"location":"safe-haven-services/open-ondemand/ssh/#my-interactive-sessions-page-job-cards-and-back-ends-on-which-jobs-are-running","title":"My Interactive Sessions page job cards and back-ends on which jobs are running","text":"<p>Click My Interactive Sessions (overlaid squares icon) on the menu bar to open the My Interactive Sessions page which shows running apps and their job cards.</p> <p> My Interactive Sessions menu button</p> <p>On a 'Running' job's job card, click the Host link to log into the back-end on which the job is running.</p>"},{"location":"safe-haven-services/open-ondemand/ssh/#active-jobs-app-and-back-ends-on-which-jobs-are-running-or-ran","title":"Active Jobs app and back-ends on which jobs are running or ran","text":"<p>Open the Active Jobs app.</p> <p>Click the &gt; button, by the job of interest, to open the job details.</p> <p>Click Open in Terminal to log into the back-end on which the job is running. Once logged in, your current directory will be changed to match the job context directory.</p>"},{"location":"safe-haven-services/open-ondemand/ssh/#job-composer-app-and-back-ends-on-which-jobs-will-be-run-are-running-or-ran","title":"Job Composer app and back-ends on which jobs will be run, are running, or ran","text":"<p>Open the Job Composer app.</p> <p>Select a job.</p> <p>Click Open Terminal (either button) to log into the back-end on which the currently selected job will be run, is running or was run. Once logged in, your current directory will be changed to match the job context directory.</p>"},{"location":"safe-haven-services/open-ondemand/tips/","title":"Open OnDemand tips","text":""},{"location":"safe-haven-services/open-ondemand/tips/#find-number-of-cpus-and-cores-available-on-a-back-end","title":"Find number of CPUs and cores available on a back-end","text":"<p>You can find the number of CPUs available on a back-end by logging into the back-end and running one of the following commands which count the number of occurrences of the terms in the <code>/proc/cpuinfo</code> file which gives the number of CPUs and cores available.</p> <pre><code>cat /proc/cpuinfo | grep processor | wc -l\ncat /proc/cpuinfo | grep 'core id' | wc -l\n</code></pre> <p>For example:</p> <pre><code>8\n8\n</code></pre> <p>This back-end has 8 CPUs and 8 cores.</p>"},{"location":"safe-haven-services/open-ondemand/tips/#find-memory-available-on-a-back-end","title":"Find memory available on a back-end","text":"<p>You can find the total memory available on a back end by logging into the back-end and running the following command:</p> <pre><code>free -h\n</code></pre> <p>For example:</p> <pre><code>total        used        free      shared  buff/cache   available\nMem:           125Gi       5.2Gi       6.0Gi       140Mi       114Gi       119Gi\nSwap:             0B          0B          0B\n</code></pre> <p>This back-end has 125 Gib memory in total.</p>"},{"location":"safe-haven-services/open-ondemand/tips/#troubleshooting-sbatch-error-memory-specification-can-not-be-satisfied","title":"Troubleshooting 'sbatch: error: Memory specification can not be satisfied'","text":"<p>If, on submitting a job you get an error:</p> <p>Failed to submit session with the following error:</p> <pre><code>sbatch: error: Memory specification can not be satisfied\nsbatch: error: Batch job submission failed: Requested node configuration is not available\n</code></pre> <p>then this can arise if you have requested more memory for your job than is available on the back-end.</p>"},{"location":"safe-haven-services/open-ondemand/tips/#troubleshooting-job-stays-in-queued-state","title":"Troubleshooting: Job stays in 'Queued' state","text":"<p>A job may stay in a 'Queued' state for the following reasons:</p> <ul> <li>You requested more CPUs/cores than are available on the back-end. If this is the case, then you need to cancel the job and resubmit it, requesting a number of CPUs/cores that the back-end can provide.</li> <li>You requested a number of CPUs/cores and/or memory that is available on the back-end, but those resources are not available at the moment due to other users' jobs. Your job will remain 'Queued' until one or more of the other users' jobs complete and resources are available for your job. You can see if this is the case via the Job details in the Active Jobs app. If so then your job state will be one of:<ul> <li>'State: PENDING', 'Reason: Resources': your job is waiting for resources, and when these are available your job will run.</li> <li>'State: PENDING', 'Reason: Priority': your job is waiting for resources, but there are are other jobs ahead of yours.</li> </ul> </li> </ul>"},{"location":"safe-haven-services/open-ondemand/tips/#troubleshooting-cannot-remove-ondemanddatasysappnfs","title":"Troubleshooting: 'Cannot remove 'ondemand/data/sys/APP/.nfs'","text":"<p>If you delete your <code>ondemand</code> directory on the Open OnDemand VM, you may see an error like the following:</p> <pre><code>rm: cannot remove 'ondemand/data/sys/myjobs/.nfs0000000601ac7ca000000002': Device or resource busy\n</code></pre> <p>The file is a system file held by a lingering process created by Open OnDemand as part of your session.</p> <p>Within Open OnDemand, select the Help (?) menu, Restart Web Server option to restart your Open OnDemand session, which ends the process and allows you to remove the file, and directory.</p>"},{"location":"safe-haven-services/open-ondemand/apps/active-jobs/","title":"Active Jobs","text":"<p>Active Jobs is an Open OnDemand app that allows you to see which of your jobs have been submitted, are running, or have completed.</p> <p>The Active Jobs app shows a table of running and recently completed jobs.</p>"},{"location":"safe-haven-services/open-ondemand/apps/active-jobs/#jobs-table","title":"Jobs table","text":"<p>The job ID is a unique job ID created by the job scheduler, when you submitted the job.</p> <p> The Active Jobs app job table</p> <p>Note</p> <p>The job ID is not the same as the session ID used for interactive apps or the job composer ID used by the Job Composer. Rather, the job ID is created by the job scheduler.</p> <p>Each job created by an app has both an app ID and a job scheduler job ID</p> <p>The job status can be one of: 'Queued', 'Running', 'Hold', 'Suspend', 'Completed', 'Undetermined'.</p> <p>Note</p> <p>The job status does not display whether a job that is 'Completed' did so with success or failure. Whether a job succeeded or failed can be seen in the job details for the job.</p>"},{"location":"safe-haven-services/open-ondemand/apps/active-jobs/#job-details","title":"Job details","text":"<p>To see details about a job, click the &gt; button, by the job of interest.</p> <p>The 'Output Location' is the location of the job context directory for the job on the Open OnDemand VM.</p> <p> Job details within the Active Jobs app</p>"},{"location":"safe-haven-services/open-ondemand/apps/active-jobs/#open-file-manager-to-job-context-directory","title":"Open File Manager to job context directory","text":"<p>Click Open in File Manager to open the File Manager pointing at the job context directory for the job on the Open OnDemand VM.</p>"},{"location":"safe-haven-services/open-ondemand/apps/active-jobs/#log-into-to-back-end-on-which-job-is-running","title":"Log into to back-end on which job is running","text":"<p>Click Open in Terminal to log into the back-end on which the currently selected job will be run. Once logged in, your current directory will be changed to match the job context directory.</p>"},{"location":"safe-haven-services/open-ondemand/apps/active-jobs/#cancel-a-job","title":"Cancel a job","text":"<p>Click the Delete Job (red trashcan icon) by the job in the job table or click Delete in the job details to cancel (delete) a running job.</p> <p> Delete Job button</p>"},{"location":"safe-haven-services/open-ondemand/apps/container-app/","title":"Run Container","text":"<p>Run Container is a Container Execution Service app that allows you to run a container on a back-end. The app is designed to run batch containers, those that perform some computational or data-related task without human interaction when they are running. The app is not designed for containers that spawn interactive services (for example, JupyterLab).</p> <p>Containers run must conform to the Container requirements of the TRE Container Execution Service.</p>"},{"location":"safe-haven-services/open-ondemand/apps/container-app/#run-a-container","title":"Run a container","text":"<p>Complete the following information the app form:</p> <ul> <li> <p>Cluster: A back-end (cluster) within your safe haven on which to run the container. Back-end-specific  short-names are used in the drop-down list, and safe haven-specific back-ends include the text 'tenant', to distinguish them from any TRE-level back-ends to which you might have access (see Back-end (cluster) names for more information).</p> <p>Note</p> <p>National Safe Haven users: If using a 'desktop' back-end, then you must select the 'desktop' you have been granted access to.</p> </li> <li> <p>Container/image URL in container registry: URL specifying both the container to run and the container registry from which it is to be pulled. For example, <code>ghcr.io/mikej888/hello-tre:1.0</code>. See Container registries for supported container registries.</p> </li> <li>Container registry username: Username to access the container registry.</li> <li>Container registry access token: Access token to access to the container registry. An access token granting read-only access to the container registry is strongly recommended.</li> <li>Container runner: Container runner - 'podman' or 'apptainer' - with which to run container on the back-end. The selected runner must be available on the selected back-end.</li> <li>Container name (Podman only): Name to be given to the container when it is run. Your job will fail if there is already a running container with that name. If omitted, then the default is <code>CONTAINER_NAME-SESSION_ID</code>, where <code>CONTAINER_NAME</code> is derived from the image name (if the image name is <code>my-container:1.0</code> then <code>CONTAINER_NAME</code> is <code>my-container</code>) and <code>SESSION_ID</code> is a unique session identifier for the app's job.</li> <li>Cores: Number of cores/CPUs requested for this job. Your selected back-end must have at least that number of cores/CPUs request.</li> <li>Memory in GiB: Memory requested for this job. Your selected back-end must have at least that amount of memory available.</li> <li>Use GPU?: Request that the container use a GPU. If selected, then your selected back-end must have a GPU.</li> <li>Command-line options to pass to container runner are container runner-specific options to control the container runner's behaviour.</li> <li> <p>Environment variables to pass to container: Environment variables to be passed on by the container runner and set within the container when it runs. Each line should define one environment variable and value, each in the form, <code>ENVIRONMENT_VARIABLE=value</code>. For example:</p> <pre><code>HELLO_TRE=Greetings\n</code></pre> <ul> <li>If a value has spaces then, if using Apptainer, enclose the value in double-quotes. If using Podman, do not enclose the value in double-quotes.</li> </ul> </li> <li> <p>Arguments to pass to container: Container-specific arguments to be passed directly to the container when it runs. For example:</p> <pre><code>-d 5\n-n container-app-user\n</code></pre> </li> </ul> <p>Click Launch.</p> <p>Open OnDemand will create job files for the app in a job-specific job context directory in an app-specific directory under your <code>ondemand</code> directory and then submits the job for the app to the job scheduler.</p> <p>Open OnDemand will show an app job card with information about the app's job including:</p> <ul> <li>Job status (on the top right of the job card): initially 'Queued'.</li> <li>'Created at': The time the job was submitted.</li> <li>'Time Requested': The runtime requested for the job.</li> <li>'Session ID': An auto-generated value which is used as the name of the job-specific job context directory.</li> <li>App-specific information, which includes values from the app form:<ul> <li>'Container/image URL in container registry'</li> <li>'Container runner'</li> <li>'Cores'</li> <li>'Memory in GiB'</li> </ul> </li> </ul> <p>When the job starts, the Job status on the job card will update to 'Starting' and 'Time Requested' will switch to 'Time Remaining', the time your job has left to run before it is cancelled by the job scheduler.</p> <p>When the Job status updates to 'Running', a Host link will appear on the job card. This is the back-end on which the job, and so the container, is now running. A message of form 'Container CONTAINER_NAME is now running. Please wait until the container completes.' will also appear on the job card.</p> <p>Warning</p> <p>Your job, and so your container. will run for a maximum of 6 hours, after which it will be cancelled.</p> <p>Warning</p> <p>Any running jobs, and containers, will be cancelled during the monthly TRE maintenance period.</p>"},{"location":"safe-haven-services/open-ondemand/apps/container-app/#sharing-files-between-the-back-end-and-the-container","title":"Sharing files between the back-end and the container","text":"<p>See Sharing files between a back-end and a container</p>"},{"location":"safe-haven-services/open-ondemand/apps/container-app/#view-job-files","title":"View job files","text":"<p>On a job's job card, click the Session ID link to open the File Manager, pointing at the job context directory for the job on the Open OnDemand VM.</p> <p>Note</p> <p>When using a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, then, if your job creates files on the back-end, you will have to log into the back-end to view your files - see Log into back-ends.</p>"},{"location":"safe-haven-services/open-ondemand/apps/container-app/#log-into-back-end","title":"Log into back-end","text":"<p>While the job is running, click the Host link to log into to back-end on which the job is running.</p> <p>If the job has completed, see Log into back-ends for ways to log into the back-end.</p>"},{"location":"safe-haven-services/open-ondemand/apps/container-app/#take-a-break","title":"Take a break","text":"<p>Your container job will continue to run even if you do the following:</p> <ul> <li>Close the browser tab.</li> <li>Log out of Open OnDemand.</li> <li>Log out of the VM from which you accessed Open OnDemand.</li> </ul>"},{"location":"safe-haven-services/open-ondemand/apps/container-app/#end-your-job","title":"End your job","text":"<p>You can end your job by as follows:</p> <ul> <li>Cancel or delete the job via Open OnDemand. See Browse and manage jobs.</li> </ul>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/","title":"Job Composer","text":"<p>Job Composer is an Open OnDemand app that allows you to submit a Slurm batch job to a back-end.</p> <p>Note</p> <p>To use the Job Composer app requires you to have some familiarity with the Slurm open source job scheduler and workload manager. In particular, how to write Slurm job submission files.</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#create-slurm-job","title":"Create Slurm job","text":"<p>To create a Slurm job, select the New Job menu, then select:</p> <ul> <li>From Default Template option to create a 'hello world' job from a template, with a default job name to be submitted to the default back-end, which is the first back-end you have access to in alphabetical order. If you select this option, the job files are created.</li> <li> <p>From Template option to creates a 'hello world' job from a template, to be submitted to a back-end that you select. If you select this option, you can:</p> <ol> <li>Enter a Job Name.</li> <li> <p>Select a back-end, Cluster.</p> <p>Note</p> <p>National Safe Haven users: If using a 'desktop' back-end, then you must select the 'desktop' you have been granted access to.</p> </li> <li> <p>Click Open Dir to view the template job files.</p> </li> <li>Click Create New Job to create the job files.</li> </ol> </li> <li> <p>From Specified Path option creates a new job from the contents of an existing directory. This directory can have the job submission script and any other files within it. If not, you can create/edit these before submission.</p> </li> <li>From Selected job option to create a new job from the contents of the job context directory for the currently selected job on the page. If you select this option, the job files are created.</li> </ul> <p>When a new job is created, the job files are created in a directory:</p> <pre><code>$HOME/ondemand/data/sys/myjobs/projects/default/JOB_COMPOSER_ID/\n</code></pre> <p>where <code>JOB_COMPOSER_ID</code> is a unique job ID created by the app. For example:</p> <pre><code>$HOME/ondemand/data/sys/myjobs/projects/default/1/\n</code></pre>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#edit-job-files","title":"Edit job files","text":"<p>Click Open Editor to open an editor to edit the Slurm job submission script (default name <code>main_job.sh</code>).</p> <p>Click on a file name under Folder contents to open an editor to edit that file in the job context directory.</p> <p>Click Open Dir or click Edit Files to open the File Manager pointing at the job context directory.</p> <p>Click Open Terminal (either button) to log into the back-end on which the currently selected job will be run. Once logged in, your current directory will be changed to match the job context directory.</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#troubleshooting-edit-job-files-cd-no-such-file-or-directory","title":"Troubleshooting: Edit job files 'cd ... No such file or directory'","text":"<p>If you see 'cd ... No such file or directory' error after you have logged into the back-end, then this means that the job context is not in your home directory in the back-end. This can happen if you selected a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, and you have not yet submitted your job.</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#submit-slurm-job","title":"Submit Slurm job","text":"<p>Click Submit job (green play icon) to submit your job.</p> <p> Submit Job button</p> <p>Warning</p> <p>Any running jobs will be cancelled during the monthly TRE maintenance period.</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#browse-and-manage-jobs","title":"Browse and manage jobs","text":""},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#jobs-table","title":"Jobs table","text":"<p>The job ID is a unique job ID created by the Slurm job scheduler, when you submitted the job.</p> <p>The job status can be one of: 'Not Submitted', 'Queued', 'Running', 'Hold', 'Suspend', 'Completed', 'Undetermined'.</p> <p>Note</p> <p>The job status does not display whether a job that is 'Completed' did so with success or failure. Whether a job succeeded or failed can be seen in the job details for the job which can be seen via the Active Jobs app.</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#open-file-manager-to-job-context-directory","title":"Open File Manager to job context directory","text":"<p>Click Open Dir or click Edit Files to open the File Manager pointing at the job context directory for the currently selected job.</p> <p>Note</p> <p>When using a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, then, if your job creates files on the back-end, you will have to log into the back-end to view your files - see Log into back-ends.</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#log-into-to-back-end","title":"Log into to back-end","text":"<p>Click Open Terminal (either button) to log into the back-end on which the currently selected job will be run, is running or was run. Once logged in, your current directory will be changed to match the job context directory.</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#troubleshooting-log-into-back-end-cd-no-such-file-or-directory","title":"Troubleshooting: Log into back-end 'cd ... No such file or directory'","text":"<p>If you see 'cd ... No such file or directory' error after you have logged into the back-end, then this means that the job context is not in your home directory in the back-end. This can happen if you selected a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, and you have not yet submitted your job.</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#stop-a-job","title":"Stop a job","text":"<p>Click Stop Job (amber stop icon) to stop the currently selected job.</p> <p> Stop Job button</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#delete-a-job","title":"Delete a job","text":"<p>Click Delete Job (red trashcan icon) to stop and delete the currently selected job.</p> <p> Delete Job button</p> <p>The job context directory for the job will also be deleted.</p>"},{"location":"safe-haven-services/open-ondemand/apps/job-composer/#run-a-hello-world-example","title":"Run a 'hello world' example","text":"<p>Create and submit job:</p> <ol> <li>Select New Job menu, From Template option.</li> <li>Enter Job Name: Hello World</li> <li>Select a Cluster, back-end.</li> <li>Click Create New Job.</li> <li>View the <code>main_job.sh</code> script contents.</li> <li>Under 'main_job.sh', click Open Editor.</li> <li>A new browser tab will appear with an editor.</li> <li> <p>Change the line:</p> <pre><code>echo \"Hello World\" &gt; output_file\n</code></pre> <p>to:</p> <pre><code>echo \"Hello World to $USER from $(hostname)\" &gt; output_file\n</code></pre> </li> <li> <p>Click Save.</p> </li> <li>Switch to the Job Composer browser tab.</li> <li> <p>Click Submit job (green play icon).</p> <p> Submit Job button</p> </li> <li> <p>The job 'Status' should go from 'Queued' to 'Completed'.</p> </li> </ol> <p>If you selected a back-end where your home directory is common to both the Open OnDemand VM and the back-end, then:</p> <ol> <li>Click <code>res.txt</code> under Folder Contents.</li> <li> <p>A new browser tab will appear with the contents of the file:</p> <pre><code>Created output file with 'Hello World'\n</code></pre> </li> <li> <p>Close the tab.</p> </li> <li>Switch to the Job Composer browser tab.</li> <li>Click <code>output_file</code> under Folder Contents.</li> <li> <p>A new browser tab will appear with the contents of the file. For example:</p> <pre><code>Hello World to someuser from some-vm.nsh.loc\n</code></pre> </li> <li> <p>Close the tab.</p> </li> </ol> <p>If you selected a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, then:</p> <ol> <li>Click Open Terminal to log into the back-end on which the job was run. Once logged in, your current directory will be changed to match the job context directory.</li> <li> <p>View the job context directory and its contents:</p> <pre><code>pwd\nls -1\n</code></pre> <pre><code>/home/someuser/ondemand/data/sys/myjobs/projects/default/1\nmain_job.sh\noutput_file\nres.txt\n</code></pre> </li> <li> <p>View <code>res.txt</code>:</p> <pre><code>cat res.txt\n</code></pre> <pre><code>Created output file with 'Hello World'\n</code></pre> </li> <li> <p>View <code>output_file</code>:</p> <pre><code>cat output_file\n</code></pre> <pre><code>Hello World to someuser from some-vm.nsh.loc\n</code></pre> </li> </ol>"},{"location":"safe-haven-services/open-ondemand/apps/jupyter-app/","title":"Run JupyterLab","text":"<p>Run JupyterLab is a Container Execution Service app that allows you to run a JupyterLab container on a back-end within your safe haven.</p> <p>The container is run using Podman.</p>"},{"location":"safe-haven-services/open-ondemand/apps/jupyter-app/#run-jupyterlab-container","title":"Run JupyterLab container","text":"<p>Complete the following information the app form:</p> <ul> <li> <p>Cluster: A back-end (cluster) within your safe haven on which to run the container. Back-end-specific  short-names are used in the drop-down list, and safe haven-specific back-ends include the text 'tenant', to distinguish them from any TRE-level back-ends to which you might have access (see Back-end (cluster) names for more information).</p> <p>Note</p> <p>National Safe Haven users: If using a 'desktop' back-end, then you must select the 'desktop' you have been granted access to.</p> </li> <li> <p>Container name: Name to be given to the container when it is run. Your job will fail if there is already a running container with that name. If omitted, then the default is <code>epcc-ces-jupyter-SESSION_ID</code>, where <code>SESSION_ID</code> is a unique session identifier for the app's job.</p> </li> <li>Cores: Number of cores/CPUs requested for this job. Your selected back-end must have at least that number of cores/CPUs request.</li> <li>Memory in GiB: Memory requested for this job. Your selected back-end must have at least that amount of memory available.</li> </ul> <p>Click Launch.</p> <p>Open OnDemand will create job files for the app in a job-specific job context directory in an app-specific directory under your <code>ondemand</code> directory and then submits the job for the app to the job scheduler.</p> <p>Open OnDemand will show an app job card with information about the app's job including:</p> <ul> <li>Job status (on the top right of the job card): initially 'Queued'.</li> <li>'Created at': The time the job was submitted.</li> <li>'Time Requested': The runtime requested for the job.</li> <li>'Session ID': An auto-generated value which is used as the name of the job-specific job context directory.</li> <li>App-specific information, which includes values from the app form:<ul> <li>'Connection timeout': when the app's job starts running, the app will then wait for JupyterLab to become available within the container. If this does not occur within this app-specific period, then the app's job will cancel itself.</li> <li>'Cores'</li> <li>'Memory in GiB'</li> </ul> </li> </ul> <p>When the job starts, the Job status on the job card will update to 'Starting' and 'Time Requested' will switch to 'Time Remaining', the time your job has left to run before it is cancelled by the job scheduler.</p> <p>When the Job status updates to 'Running', a Host link will appear on the job card, which allows you to log in to the back-end on which the job, and so the JupyterLab container, is now running. A 'JupyterLab running in container CONTAINER_NAME' message will appear along with a Connect to JupyterLab button. The JupyterLab container is now ready for use.</p> <p>Click Connect to JupyterLab. A new browser tab will open with JupyterLab.</p> <p>Warning</p> <p>Open OnDemand will wait 180 seconds (3 minutes) for your container to start. If it does not start within this time the job will be cancelled.</p> <p>Warning</p> <p>Your job, and so your container. will run for a maximum of 6 hours.</p> <p>Warning</p> <p>Any running jobs, and containers, will be cancelled during the monthly TRE maintenance period.</p>"},{"location":"safe-haven-services/open-ondemand/apps/jupyter-app/#troubleshooting-proxy-error","title":"Troubleshooting: Proxy Error","text":"<p>If you click Connect to JupyterLab and get:</p> <p>Proxy Error</p> <p>The proxy server received an invalid response from an upstream server. The proxy server could not handle the request</p> <p>Reason: Error reading from remote server</p> <p>Apache/2.4.52 (Ubuntu) Server at host Port 443</p> <p>then, this can arise as sometimes there is a lag between the container having started and JupyterLab within the container being ready for connections.</p> <p>Wait 30 seconds, then refresh the web page, or click the Connect to JupyterLab button again.</p>"},{"location":"safe-haven-services/open-ondemand/apps/jupyter-app/#log-in-to-jupyterlab","title":"Log in to JupyterLab","text":"<p>You will not be prompted for a username and password. JupyterLab running in the container runs as a 'root' user. JupyterLab is protected with an auto-generated password. The Connect to JupyterLab button is configured to log you into the container using this password automatically.</p> <p>Note</p> <p>You are the 'root' user only within the context of the container. You will not have 'root' access to the back-end on which the container is running! Any files you create in the directories mounted into the container will be owned by your own user, and user group, on the back-end.</p>"},{"location":"safe-haven-services/open-ondemand/apps/jupyter-app/#sharing-files-between-the-back-end-and-the-container","title":"Sharing files between the back-end and the container","text":"<p>See Sharing files between a back-end and a container</p>"},{"location":"safe-haven-services/open-ondemand/apps/jupyter-app/#view-job-files","title":"View job files","text":"<p>On a job's job card, click the Session ID link to open the File Manager, pointing at the job context directory for the job on the Open OnDemand VM.</p> <p>Note</p> <p>When using a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, then, if your job creates files on the back-end, you will have to log into the back-end to view your files - see Log into back-ends.</p>"},{"location":"safe-haven-services/open-ondemand/apps/jupyter-app/#log-into-back-end","title":"Log into back-end","text":"<p>While the job is running, click the Host link to log into to back-end on which the job is running.</p> <p>If the job has completed, see Log into back-ends for ways to log into the back-end.</p>"},{"location":"safe-haven-services/open-ondemand/apps/jupyter-app/#take-a-break","title":"Take a break","text":"<p>Your container job will continue to run even if you do the following:</p> <ul> <li>Log out of JupyterLab via the File menu, Log Out option.</li> <li>Close the browser tab.</li> <li>Log out of Open OnDemand.</li> <li>Log out of the VM from which you accessed Open OnDemand.</li> </ul> <p>You can re-access your running container via the Connect to JupyterLab on your session's job card on the My Interactive Sessions page accessed via My Interactive Sessions (overlaid squares icon) on the menu bar.</p> <p> My Interactive Sessions menu button</p>"},{"location":"safe-haven-services/open-ondemand/apps/jupyter-app/#end-your-job","title":"End your job","text":"<p>You can end your job by as follows:</p> <ul> <li>Either, shut down JupyterLab via the File menu, Shut Down option.</li> <li>Or, cancel or delete the job via Open OnDemand. See Browse and manage jobs.</li> </ul>"},{"location":"safe-haven-services/open-ondemand/apps/rstudio-app/","title":"Run RStudio Server","text":"<p>Run RStudio Server is a Container Execution Service app that allows you to run an RStudio Server container on a back-end within your safe haven.</p> <p>The container is run using Podman.</p>"},{"location":"safe-haven-services/open-ondemand/apps/rstudio-app/#run-rstudio-server-container","title":"Run RStudio Server container","text":"<p>Complete the following information the app form:</p> <ul> <li> <p>Cluster: A back-end (cluster) within your safe haven on which to run the container. Back-end-specific  short-names are used in the drop-down list, and safe haven-specific back-ends include the text 'tenant', to distinguish them from any TRE-level back-ends to which you might have access (see Back-end (cluster) names for more information).</p> <p>Note</p> <p>National Safe Haven users: If using a 'desktop' back-end, then you must select the 'desktop' you have been granted access to.</p> </li> <li> <p>Container name: Name to be given to the container when it is run. Your job will fail if there is already a running container with that name. If omitted, then the default is <code>epcc-ces-rstudio-SESSION_ID</code>, where <code>SESSION_ID</code> is a unique session identifier for the app's job.</p> </li> <li>RStudio Server password: RStudio Server running in the container needs to be password-protected. Specify the password to use.</li> <li>Cores: Number of cores/CPUs requested for this job. Your selected back-end must have at least that number of cores/CPUs request.</li> <li>Memory in GiB: Memory requested for this job. Your selected back-end must have at least that amount of memory available.</li> </ul> <p>Click Launch.</p> <p>Open OnDemand will create job files for the app in a job-specific job context directory in an app-specific directory under your <code>ondemand</code> directory and then submits the job for the app to the job scheduler.</p> <p>Open OnDemand will show an app job card with information about the app's job including:</p> <ul> <li>Job status (on the top right of the job card): initially 'Queued'.</li> <li>'Created at': The time the job was submitted.</li> <li>'Time Requested': The runtime requested for the job.</li> <li>'Session ID': An auto-generated value which is used as the name of the job-specific job context directory.</li> <li>App-specific information, which includes values from the app form:<ul> <li>'Connection timeout': when the app's job starts running, the app will then wait for RStudio Server to become available within the container. If this does not occur within this app-specific period, then the app's job will cancel itself.</li> <li>'Cores'</li> <li>'Memory in GiB'</li> </ul> </li> </ul> <p>When the job starts, the Job status on the job card will update to 'Starting' and 'Time Requested' will switch to 'Time Remaining', the time your job has left to run before it is cancelled by the job scheduler.</p> <p>When the Job status updates to 'Running', a Host link will appear on the job card, which allows you to log in to the back-end on which the job, and so the RStudio Server container, is now running. A 'RStudio Server running in container CONTAINER_NAME' message will appear along with a Connect to RStudio Server button. The RStudio Server container is now ready for use.</p> <p>Click Connect to RStudio Server. A new browser tab will open with RStudio Server.</p> <p>Warning</p> <p>Open OnDemand will wait 180 seconds (3 minutes) for your container to start. If it does not start within this time the job will be cancelled.</p> <p>Warning</p> <p>Your job, and so your container. will run for a maximum of 6 hours.</p> <p>Warning</p> <p>Any running jobs, and containers, will be cancelled during the monthly TRE maintenance period.</p>"},{"location":"safe-haven-services/open-ondemand/apps/rstudio-app/#troubleshooting-proxy-error","title":"Troubleshooting: Proxy Error","text":"<p>If you click Connect to RStudio Server and get:</p> <p>Proxy Error</p> <p>The proxy server received an invalid response from an upstream server. The proxy server could not handle the request</p> <p>Reason: Error reading from remote server</p> <p>Apache/2.4.52 (Ubuntu) Server at host Port 443</p> <p>then, this can arise as sometimes there is a lag between the container having started and RStudio Server within the container being ready for connections.</p> <p>Wait 30 seconds, then refresh the web page, or click the Connect to RStudio Server button again.</p>"},{"location":"safe-haven-services/open-ondemand/apps/rstudio-app/#log-in-to-rstudio-server","title":"Log in to RStudio Server","text":"<p>A Sign in to RStudio page will appear. Enter:</p> <ul> <li>Username: root</li> <li>Password: password you selected when completing the app form.</li> </ul> <p>Click Sign in.</p> <p>Note</p> <p>You are the 'root' user only within the context of the container. You will not have 'root' access to the back-end on which the container is running! Any files you create in the directories mounted into the container will be owned by your own user, and user group, on the back-end.</p>"},{"location":"safe-haven-services/open-ondemand/apps/rstudio-app/#sharing-files-between-a-back-end-and-the-container","title":"Sharing files between a back-end and the container","text":"<p>See Sharing files between a back-end and a container</p>"},{"location":"safe-haven-services/open-ondemand/apps/rstudio-app/#view-job-files","title":"View job files","text":"<p>On a job's job card, click the Session ID link to open the File Manager, pointing at the job context directory for the job on the Open OnDemand VM.</p> <p>Note</p> <p>When using a back-end where your home directory is not common to both the Open OnDemand VM and the back-end, then, if your job creates files on the back-end, you will have to log into the back-end to view your files - see Log into back-ends.</p>"},{"location":"safe-haven-services/open-ondemand/apps/rstudio-app/#log-into-back-end","title":"Log into back-end","text":"<p>While the job is running, click the Host link to log into to back-end on which the job is running.</p> <p>If the job has completed, see Log into back-ends for ways to log into the back-end.</p>"},{"location":"safe-haven-services/open-ondemand/apps/rstudio-app/#take-a-break","title":"Take a break","text":"<p>Your container job will continue to run even if you do the following:</p> <ul> <li>Log out of RStudio Server via the File menu, Sign Out option.</li> <li>Log out of RStudio Server via the File menu, Quit Session... option.</li> <li>Close the browser tab.</li> <li>Log out of Open OnDemand.</li> <li>Log out of the VM from which you accessed Open OnDemand.</li> </ul> <p>You can re-access your running container via the Connect to RStudio Server on your session's job card on the My Interactive Sessions page accessed via My Interactive Sessions (overlaid squares icon) on the menu bar.</p> <p> My Interactive Sessions menu button</p>"},{"location":"safe-haven-services/open-ondemand/apps/rstudio-app/#end-your-job","title":"End your job","text":"<p>You can end your job as follows:</p> <ul> <li>Cancel or delete the job via Open OnDemand. See Browse and manage jobs.</li> </ul>"},{"location":"safe-haven-services/superdome-flex-tutorial/L1_Accessing_the_SDF_Inside_the_EPCC_TRE/","title":"Accessing the Superdome Flex inside the EPCC Trusted Research Environment","text":""},{"location":"safe-haven-services/superdome-flex-tutorial/L1_Accessing_the_SDF_Inside_the_EPCC_TRE/#what-is-the-superdome-flex","title":"What is the Superdome Flex?","text":"<p>The Superdome Flex (SDF) is a high-performance computing cluster manufactured by Hewlett Packard Enterprise. It has been designed to handle multi-core, high-memory tasks in environments where security is paramount. The hardware specifications of the SDF within the Trusted Research Environment (TRE) are as follows:</p> <ul> <li>576 physical cores (1152 hyper-threaded cores)</li> <li>18TB of dynamic memory (17 TB available to users)</li> <li>768TB or more of permanent memory</li> </ul> <p>The software specification of the SDF are:</p> <ul> <li>Red Hat Enterprise Linux (v8.7 as of 27/03/23)</li> <li>Slurm job manager</li> <li>Access to local copies of R (CRAN) and python (conda) repositories</li> <li>Singularity container platform</li> </ul>"},{"location":"safe-haven-services/superdome-flex-tutorial/L1_Accessing_the_SDF_Inside_the_EPCC_TRE/#key-point","title":"Key Point","text":"<p><code>The SDF is within the TRE. Therefore, the same restrictions apply, i.e. the SDF is isolated from the internet (no downloading code from public GitHub repos) and copying/recording/extracting anything on the SDF outside of the TRE is strictly prohibited unless through approved processes.</code></p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L1_Accessing_the_SDF_Inside_the_EPCC_TRE/#accessing-the-sdf","title":"Accessing the SDF","text":"<p>Users can only access the SDF by ssh-ing into it via their VM desktop.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L1_Accessing_the_SDF_Inside_the_EPCC_TRE/#hello-world","title":"Hello world","text":"<pre><code>**** On the VM desktop terminal ****\n\nssh shs-sdf01\n&lt;Enter VM password&gt;\n\necho \"Hello World\"\n\nexit\n</code></pre>"},{"location":"safe-haven-services/superdome-flex-tutorial/L1_Accessing_the_SDF_Inside_the_EPCC_TRE/#sdf-vs-vm-file-systems","title":"SDF vs VM file systems","text":"<p>The SDF file system is separate from the VM file system, which is again separate from the project data space. Files need to be transferred between the three systems for any analysis to be completed within the SDF.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L1_Accessing_the_SDF_Inside_the_EPCC_TRE/#example-showing-separate-sdf-and-vm-file-systems","title":"Example showing separate SDF and VM file systems","text":"<pre><code>**** On the VM desktop terminal ****\n\ncd ~\ntouch test.txt\nls\n\nssh shs-sdf01\n&lt;Enter VM password&gt;\n\nls # test.txt is not here\nexit\n\nscp test.txt shs-sdf01:/home/&lt;USERNAME&gt;/\n\nssh shs-sdf01\n&lt;Enter VM password&gt;\n\nls # test.txt is here\n</code></pre>"},{"location":"safe-haven-services/superdome-flex-tutorial/L1_Accessing_the_SDF_Inside_the_EPCC_TRE/#example-copying-data-between-project-data-space-and-sdf","title":"Example copying data between project data space and SDF","text":"<p>Transferring and synchronising data sets between the project data space and the SDF is easier with the rsync command (rather than manually checking and copying files/folders with scp). rsync only transfers files that are different between the two targets, more details in its manual.</p> <pre><code>**** On the VM desktop terminal ****\n\nman rsync # check instructions for using rsync\n\nrsync -avPz -e ssh /safe_data/my_project/ shs-sdf01:/home/&lt;USERNAME&gt;/my_project/ # sync project folder and SDF home folder\n\nssh shs-sdf01\n&lt;Enter VM password&gt;\n\n*** Conduct analysis on SDF ***\n\nexit\n\nrsync -avPz -e ssh /safe_data/my_project/current_wip shs-sdf01:/home/&lt;USERNAME&gt;/my_project/ # sync project file and ssh home page # re-syncronise project folder and SDF home folder\n\n*** Optionally remove the project folder on SDF ***\n</code></pre>"},{"location":"safe-haven-services/superdome-flex-tutorial/L2_running_R_Python_analysis_scripts/","title":"Running R/Python Scripts","text":"<p>Running analysis scripts on the SDF is slightly different to running scripts on the Desktop VMs. The Linux distribution differs between the two with the SDF using Red Hat Enterprise Linux (RHEL) and the Desktop VMs using Ubuntu. Therefore, it is highly advisable to use virtual environments (e.g. conda environments) to complete any analysis and aid the transition between the two distributions. Conda should run out of the box on the Desktop VMs, but some configuration is required on the SDF.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L2_running_R_Python_analysis_scripts/#setting-up-conda-environments-on-you-first-connection-to-the-sdf","title":"Setting up conda environments on you first connection to the SDF","text":"<pre><code>*** SDF Terminal ***\n\nconda activate base # Test conda environment\n\n# Conda command will not be found. There is no need to install!\n\neval \"$(/opt/anaconda3/bin/conda shell.bash hook)\" # Tells your terminal where conda is\n\nconda init # changes your .bashrc file so conda is automatically available in the future\n\nconda config --set auto_activate_base false # stop conda base from being activated on startup\n\npython # note python version\n\nexit()\n</code></pre> <p>The base conda environment is now available but note that the python and gcc compilers are not the latest (Python 3.9.7 and gcc 7.5.0).</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L2_running_R_Python_analysis_scripts/#getting-an-up-to-date-python-version","title":"Getting an up-to-date python version","text":"<p>In order to get an up-to-date python version we first need to use an updated gcc version. Fortunately, conda has an updated gcc toolset that can be installed.</p> <pre><code>*** SDF Terminal ***\n\nconda activate base # If conda isn't already active\n\nconda create -n python-v3.11 gcc_linux-64=11.2.0 python=3.11.3\n\nconda activate python-v3.11\n\npython\n\nexit()\n</code></pre>"},{"location":"safe-haven-services/superdome-flex-tutorial/L2_running_R_Python_analysis_scripts/#running-r-scripts-on-the-sdf","title":"Running R scripts on the SDF","text":"<p>The default version of R available on the SDF is v4.1.2. Alternative R versions can be installed using conda similar to the python conda environment above.</p> <pre><code>conda create -n r-v4.3 gcc_linux-64=11.2.0 r-base=4.3\n\nconda activate r-v4.3\n\nR\n\nq()\n</code></pre>"},{"location":"safe-haven-services/superdome-flex-tutorial/L2_running_R_Python_analysis_scripts/#final-points","title":"Final points","text":"<ul> <li> <p>The SDF, like the rest of the SHS, is separated from the internet. The installation of python/R libraries to your environment is from a local copy of the respective conda/CRAN library repositories. Therefore, not all packages may be available and not all package versions may be available.</p> </li> <li> <p>It is discouraged to run extensive python/R analyses without submitting them as job requests using Slurm.</p> </li> </ul>"},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/","title":"Submitting Scripts to Slurm","text":""},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/#what-is-slurm","title":"What is Slurm?","text":"<p>Slurm is a workload manager that schedules jobs submitted to a shared resource. Slurm is a well-developed tool that can manage large computing clusters, such as ARCHER2, with thousands of users each with different priorities and allocated computing hours. Inside the TRE, Slurm is used to help ensure all users of the SDF get equitable access. Therefore, users who are submitting jobs with high resource requirements (&gt;80 cores, &gt;1TB of memory) may have to wait longer for resource allocation to enable users with lower resource demands to continue their work.</p> <p>Slurm is currently set up so all users have equal priority and there is no limit to the total number of CPU hours allocated to a user per month. However, there are limits to the maximum amount of resources that can be allocated to an individual job. Jobs that require more than 200 cores, more than 4TB of memory, or an elapsed runtime of more than 96 hours will be rejected. If users need to submit jobs with large resource demand, they need to submit a resource reservation request by emailing their project's service desk.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/#why-do-you-need-to-use-slurm","title":"Why do you need to use Slurm?","text":"<p>The SDF is a resource shared across all projects within the TRE and all users should have equal opportunity to use the SDF to complete resource-intense tasks appropriate to their projects. Users of the SDF are required to consider the needs of the wider community by:</p> <ul> <li> <p>requesting resources appropriate to their intended task and timeline.</p> </li> <li> <p>submitting resource requests via Slurm to enable automatic scheduling and fair allocation alongside other user requests.</p> </li> </ul> <p>Users can develop code, complete test runs, and debug from the SDF command line without using Slurm. However, only 32 of the 512 cores are accessible without submitting a job request to Slurm. These cores are accessible to all users simultaneously.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/#slurm-basics","title":"Slurm basics","text":"<p>Slurm revolves around four main entities: nodes, partitions, jobs and job steps. Nodes and partitions are relevant for more complex distributed computing clusters so Slurm can allocate appropriate resources to jobs across multiple pieces of hardware. Jobs are requests for resources and job steps are what need to be completed once the resources have been allocated (completed in sequence or parallel). Job steps can be further broken down into tasks.</p> <p>There are four key commands for Slurm users:</p> <ul> <li> <p>squeue: get details on a job or job step, i.e. has a job been allocated resources or is it still pending?</p> </li> <li> <p>srun: initiate a job step or execute a job. A versatile function that can initiate job steps as part of a larger batch job or submit a job itself to get resources and run a job step. This is useful for testing job steps, experimenting with different resource allocations or running job steps that require large resources but are relatively easy to define in a line or two (i.e. running a sequence alignment).</p> </li> <li> <p>scancel: stop a job from continuing.</p> </li> <li> <p>sbatch: submit a job script containing multiple steps (i.e. srun) to be completed with the defined resources. This is the typical function for submitting jobs to Slurm.</p> </li> </ul> <p>More details on these functions (and several not mentioned here) can be seen on the Slurm website.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/#submitting-a-simple-job","title":"Submitting a simple job","text":"<pre><code>*** SDF Terminal ***\n\nsqueue -u $USER # Check if there are jobs already queued or running for you\n\nsrun --job-name=my_first_slurm_job --nodes 1 --ntasks 10 --cpus-per-task 2 echo 'Hello World'\n\nsqueue -u $USER --state=CD # List all completed jobs\n</code></pre> <p>In this instance, the srun command completes two steps: job submission and job step execution. First, it submits a job request to be allocated 10 CPUs (1 CPU for each of the 10 tasks). Once the resources are available, it executes the job step consisting of 10 tasks each running the 'echo \"Hello World\"' function.</p> <p>srun accepts a wide variety of options to specify the resources required to complete its job step. Within the SDF, you must always request 1 node (as there is only one node) and never use the --exclusive option (as no one will have exclusive access to this shared resource). Notice that running srun blocks your terminal from accepting any more commands and the output from each task in the job step, i.e. Hello World in the above example, outputs to your terminal. We will compare this to running a sbatch command.\u0011</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/#submitting-a-batch-job","title":"Submitting a batch job","text":"<p>Batch jobs are incredibly useful because they run in the background without blocking your terminal. Batch jobs also output the results to a log file rather than straight to your terminal. This allows you to check a job was completed successfully at a later time so you can move on to other things whilst waiting for a job to complete.</p> <p>A batch job can be submitted to Slurm by passing a job script to the sbatch command. The first few lines of a job script outline the resources to be requested as part of the job. The remainder of a job script consists of one or more srun commands outlining the job steps that need to be completed (in sequence or parallel) once the resources are available. There are numerous options for defining the resource requirements of a job including:</p> <ul> <li>The number of CPUs available for each active task: --ncpus-per-task</li> <li>The amount of memory available per CPU (in MB by default but can be in GB if G is appended to the number): --mem-per-cpu</li> <li>The total amount of memory (in MB by default but can be in GB if G is appended to the number): --mem</li> <li>The maximum number of tasks invoked at one time: --ntasks</li> <li>The number of nodes (Always 1 when using SDF): --nodes</li> <li>If the job requires exclusive access to all the resources of a node (never part of job request, but required for parallel job steps within batch scripts): --exclusive</li> </ul> <p>More information on the various options are in the sbatch documentation.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/#example-job-script","title":"Example Job Script","text":"<pre><code>#!/usr/bin/env bash\n#SBATCH -J HelloWorld\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=10\n#SBATCH --cpus-per-task=2\n\n% Run echo task in sequence\n\nsrun --ntasks 5 --cpus-per-task 2 echo \"Series Task A. Time: \" $(date +\u201d%H:%M:%S\u201d)\n\nsrun --ntasks 5 --cpus-per-task 2 echo \"Series Task B. Time: \" $(date +\u201d%H:%M:%S\u201d)\n\n% Run echo task in parallel with the ampersand character\n\nsrun --exclusive --ntasks 5 --cpus-per-task 2 echo \"Parallel Task A. Time: \" $(date +\u201d%H:%M:%S\u201d) &amp;\n\nsrun --exclusive --ntasks 5 --cpus-per-task 2 echo \"Parallel Task B. Time: \" $(date +\u201d%H:%M:%S\u201d)\n</code></pre>"},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/#example-job-script-submission","title":"Example job script submission","text":"<pre><code>*** SDF Terminal ***\n\nnano example_job_script.sh\n\n*** Copy example job script above ***\n\nsbatch example_job_script.sh\n\nsqueue -u $USER -r 5\n\n*** Wait for the batch job to be completed ***\n\ncat example_job_script.log # The series tasks should be grouped together and the parallel tasks interspersed.\n</code></pre> <p>The example batch job is intended to show two things: 1) the usefulness of the sbatch command and 2) the versatility of a job script. As the sbatch command allows you to submit scripts and check their outcome at your own discretion, it is the most common way of interacting with Slurm. Meanwhile, the job script command allows you to specify one global resource request and break it up into multiple job steps with different resource demands that can be completed in parallel or in sequence.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/#submitting-pythonr-code-to-slurm","title":"Submitting python/R code to Slurm","text":"<p>Although submitting job steps containing python/R analysis scripts can be done with srun directly, as below, it is more common to submit bash scripts that call the analysis scripts after setting up the environment (i.e. after calling conda activate).</p> <pre><code>**** Python code job submission ****\n\nsrun --job-name=my_first_python_job --nodes 1 --ntasks 10 --cpus-per-task 2 --mem 10G python3 example_script.py\n\n**** R code job submission ****\n\nsrun --job-name=my_first_r_job --nodes 1 --ntasks 10 --cpus-per-task 2 --mem 10G Rscript example_script.R\n</code></pre>"},{"location":"safe-haven-services/superdome-flex-tutorial/L3_submitting_scripts_to_slurm/#signposting","title":"Signposting","text":"<p>Useful websites for learning more about Slurm:</p> <ul> <li> <p>The Slurm documentation website</p> </li> <li> <p>The Introduction to HPC carpentries lesson on Slurm</p> </li> </ul>"},{"location":"safe-haven-services/superdome-flex-tutorial/L4_parallelised_python_analysis/","title":"Parallelised Python analysis with Dask","text":"<p>This lesson is adapted from a workshop introducing users to running python scripts on ARCHER2 as developed by Adrian Jackson.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L4_parallelised_python_analysis/#introduction","title":"Introduction","text":"<p>Python does not have native support for parallelisation. Python contains a Global Interpreter Lock (GIL) which means the python interpreter only allows one thread to execute at a time. The advantage of the GIL is that C libraries can be easily integrated into Python scripts without checking if they are thread-safe. However, this means that most common python modules cannot be easily parallelised. Fortunately, there are now several re-implementations of common python modules that work around the GIL and are therefore parallelisable. Dask is a python module that contains a parallelised version of the pandas data frame as well as a general format for parallelising any python code.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L4_parallelised_python_analysis/#dask","title":"Dask","text":"<p>Dask enables thread-safe parallelised python execution by creating task graphs (a graph of the dependencies of the inputs and outputs of each function) and then deducing which ones can be run separately. This lesson introduces some general concepts required for programming using Dask. There are also some exercises with example answers to help you write your first parallelised python scripts.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L4_parallelised_python_analysis/#arrays-data-frames-and-bags","title":"Arrays, data frames and bags","text":"<p>Dask contains three data objects to enable parallelised analysis of large data sets in a way familiar to most python programmers. If the same operations are being applied to a large data set then Dask can split up the data set and apply the operations in parallel. The three data objects that Dask can easily split up are:</p> <ul> <li> <p>Arrays: Contains large numbers of elements in multiple dimensions, but each element must be of the same type. Each element has a unique index that allows users to specify changes to individual elements.</p> </li> <li> <p>Data frames: Contains large numbers of elements which are typically highly structured with multiple object types allowed together. Each element has a unique index that allows users to specify changes to individual elements.</p> </li> <li> <p>Bags: Contains large numbers of elements which are semi/un-structured. Elements are immutable once inside the bag. Bags are useful for conducting initial analysis/wrangling of raw data before more complex analysis is performed.</p> </li> </ul>"},{"location":"safe-haven-services/superdome-flex-tutorial/L4_parallelised_python_analysis/#example-dask-array","title":"Example Dask array","text":"<p>You may need to install dask or create a new conda environment with it in.</p> <pre><code>conda create -n dask-env gcc_linux-64=11.2.0 python=3.11.3 dask\n\nconda activate dask-env\n</code></pre> <p>Try running the following Python using dask:</p> <pre><code>import dask.array as da\n\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\n\nprint(x)\n\nprint(x.compute())\n\nprint(x.sum())\n\nprint(x.sum().compute())\n</code></pre> <p>This should demonstrate that dask is both straightforward to implement simple parallelism, but also lazy in that it does not compute anything until you force it to with the .compute() function.</p> <p>You can also try out dask DataFrames, using the following code:</p> <pre><code>import dask.dataframe as dd\n\ndf = dd.read_csv('surveys.csv')\n\ndf.head()\ndf.tail()\n\ndf.weight.max().compute()\n</code></pre> <p>You can try using different blocksizes when reading in the csv file, and then undertaking an operation on the data, as follows: Experiment with varying blocksizes, although you should be aware that making your block size too small is likely to cause poor performance (the blocksize affects the number of bytes read in at each operation).</p> <pre><code>df = dd.read_csv('surveys.csv', blocksize=\"10000\")\ndf.weight.max().compute()\n</code></pre> <p>You can also experiment with Dask Bags to see how that functionality works:</p> <pre><code>import dask.bag as db\nfrom operator import add\nb = db.from_sequence([1, 2, 3, 4, 5], npartitions=2)\nprint(b.compute())\n</code></pre>"},{"location":"safe-haven-services/superdome-flex-tutorial/L4_parallelised_python_analysis/#dask-delayed","title":"Dask Delayed","text":"<p>Dask delayed lets you construct your own task graphs/parallelism from Python functions. You can find out more about dask delayed from the dask documentation Try parallelising the code below using the .delayed function or the @delayed decorator, an example answer can be found here.</p> <pre><code>def inc(x):\n    return x + 1\n\ndef double(x):\n    return x * 2\n\ndef add(x, y):\n    return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n    a = inc(x)\n    b = double(x)\n    c = add(a, b)\n    output.append(c)\n\ntotal = sum(output)\n\nprint(total)\n</code></pre>"},{"location":"safe-haven-services/superdome-flex-tutorial/L4_parallelised_python_analysis/#mandelbrot-exercise","title":"Mandelbrot Exercise","text":"<p>The code below calculates the members of a Mandelbrot set using Python functions:</p> <pre><code>import sys\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef mandelbrot(h, w, maxit=20, r=2):\n    \"\"\"Returns an image of the Mandelbrot fractal of size (h,w).\"\"\"\n    start = time.time()\n\n    x = np.linspace(-2.5, 1.5, 4*h+1)\n\n    y = np.linspace(-1.5, 1.5, 3*w+1)\n\n    A, B = np.meshgrid(x, y)\n\n    C = A + B*1j\n\n    z = np.zeros_like(C)\n\n    divtime = maxit + np.zeros(z.shape, dtype=int)\n\n    for i in range(maxit):\n        z = z**2 + C\n        diverge = abs(z) &gt; r # who is diverging\n        div_now = diverge &amp; (divtime == maxit) # who is diverging now\n        divtime[div_now] = i # note when\n        z[diverge] = r # avoid diverging too much\n\n    end = time.time()\n\n    return divtime, end-start\n\nh = 2000\nw = 2000\n\nmandelbrot_space, time = mandelbrot(h, w)\n\nplt.imshow(mandelbrot_space)\n\nprint(time)\n</code></pre> <p>Your task is to parallelise this code using Dask Array functionality. Using the base python code above, extend it with Dask Array for the main arrays in the computation. Remember you need to specify a chunk size with Dask Arrays, and you will also need to call compute at some point to force Dask to actually undertake the computation. Note, depending on where you run this you may not see any actual speed up of the computation. You need access to extra resources (compute cores) for the calculation to go faster. If in doubt, submit a python script of your solution to the SDF compute nodes to see if you see speed up there. If you are struggling with this parallelisation exercise, there is a solution available for you here.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L4_parallelised_python_analysis/#pi-exercise","title":"Pi Exercise","text":"<p>The code below calculates Pi using a function that can split it up into chunks and calculate each chunk separately. Currently it uses a single chunk to produce the final value of Pi, but that can be changed by calling pi_chunk multiple times with different inputs. This is not necessarily the most efficient method for calculating Pi in serial, but it does enable parallelisation of the calculation of Pi using multiple copies of pi_chunk called simultaneously.</p> <pre><code>import time\nimport sys\n\n# Calculate pi in chunks\n\n# n     - total number of steps to be undertaken across all chunks\n# lower - the lowest number of this chunk\n# upper - the upper limit of this chunk such that i &lt; upper\n\ndef pi_chunk(n, lower, upper):\n    step = 1.0 / n\n    p = step * sum(4.0/(1.0 + ((i + 0.5) * (i + 0.5) * step * step)) for i in range(lower, upper))\n    return p\n\n# Number of slices\n\nnum_steps = 10000000\n\nprint(\"Calculating PI using:\\n \" + str(num_steps) + \" slices\")\n\nstart = time.time()\n\n# Calculate using a single chunk containing all steps\n\np = pi_chunk(num_steps, 1, num_steps)\n\nstop = time.time()\n\nprint(\"Obtained value of Pi: \" + str(p))\n\nprint(\"Time taken: \" + str(stop - start) + \" seconds\")\n</code></pre> <p>For this exercise, your task is to implemented the above code on the SDF, and then parallelise using Dask. There are a number of different ways you could parallelise this using Dask, but we suggest using the Futures map functionality to run the pi_chunk function on a range of different inputs. Futures map has the following definition:</p> <pre><code>Client.map(func, *iterables[, key, workers, ...])\n</code></pre> <p>Where func is the function you want to run, and then the subsequent arguments are inputs to that function. To utilise this for the Pi calculation, you will first need to setup and configure a Dask Client to use, and also create and populate lists or vectors of inputs to be passed to the pi_chunk function for each function run that Dask launches.</p> <p>If you run Dask with processes then it is possible that you will get errors about forking processes, such as these:</p> <pre><code>    An attempt has been made to start a new process before the current process has finished its bootstrapping phase.\n    This probably means that you are not using fork to start your child processes and you have forgotten to use the proper idiom in the main module:\n</code></pre> <p>In that case you need to encapsulate your code within a main function, using something like this:</p> <pre><code>if __name__ == \"__main__\":\n</code></pre> <p>If you are struggling with this exercise then there is a solution available for you here.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L4_parallelised_python_analysis/#signposting","title":"Signposting","text":"<ul> <li> <p>More information on parallelised python code can be found in the carpentries lesson</p> </li> <li> <p>Dask itself has several detailed tutorials</p> </li> </ul>"},{"location":"safe-haven-services/superdome-flex-tutorial/L5_parallelised_r_analysis/","title":"Parallelised R Analysis","text":"<p>This lesson is adapted from a workshop introducing users to running R scripts on ARCHER2 as developed by Adrian Jackson.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L5_parallelised_r_analysis/#introduction","title":"Introduction","text":"<p>In this exercise we are going to try different methods of parallelising R on the SDF. This will include single node parallelisation functionality (e.g. using threads or processes to use cores within a single node), and distributed memory functionality that enables the parallelisation of R programs across multiple nodes.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L5_parallelised_r_analysis/#example-parallelised-r-code","title":"Example parallelised R code","text":"<p>You may need to activate an R conda environment.</p> <pre><code>conda activate r-v4.2\n</code></pre> <p>Try running the following R script using R on the SDF login node:</p> <pre><code>n &lt;- 8*2048\nA &lt;- matrix( rnorm(n*n), ncol=n, nrow=n )\nB &lt;- matrix( rnorm(n*n), ncol=n, nrow=n )\nC &lt;- A %*% B\n</code></pre> <p>You can run this as follows on the SDF (assuming you have saved the above code into a file named matrix.R):</p> <pre><code>Rscript ./matrix.R\n</code></pre> <p>You can check the resources used by R when running on the login node using this command:</p> <pre><code>top -u $USER\n</code></pre> <p>If you run the R script in the background using &amp;, as follows, you can then monitor your run using the top command. You may notice when you run your program that at points R uses many more resources than a single core can provide, as demonstrated below:</p> <pre><code>    PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND\n    178357 adrianj 20 0 15.542 0.014t 13064 R 10862 2.773 9:01.66 R\n</code></pre> <p>In the example above it can be seen that &gt;10862% of a single core is being used by R. This is an example of R using automatic parallelisation. You can experiment with controlling the automatic parallelisation using the OMP_NUM_THREADS variable to restrict the number of cores available to R. Try using the following values:</p> <pre><code>export OMP_NUM_THREADS=8\n\nexport OMP_NUM_THREADS=4\n\nexport OMP_NUM_THREADS=2\n</code></pre> <p>You may also notice that not all the R script is parallelised. Only the actual matrix multiplication is undertaken in parallel, the initialisation/creation of the matrices is done in serial.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L5_parallelised_r_analysis/#parallelisation-with-datatables","title":"Parallelisation with data.tables","text":"<p>We can also experiment with the implicit parallelism in other libraries, such as data.table. You will first need to install this library on the SDF. To do this you can simply run the following command:</p> <pre><code>install.packages(data.table)\n</code></pre> <p>Once you have installed data.table you can experiment with the following code:</p> <pre><code>library(data.table)\nvenue_data &lt;- data.table( ID = 1:50000000,\nCapacity = sample(100:1000, size = 50000000, replace = T), Code = sample(LETTERS, 50000000, replace = T),\nCountry = rep(c(\"England\",\"Scotland\",\"Wales\",\"NorthernIreland\"), 50000000))\nsystem.time(venue_data[, mean(Capacity), by = Country])\n</code></pre> <p>This creates some random data in a large data table and then performs a calculation on it. Try running R with varying numbers of threads to see what impact that has on performance. Remember, you can vary the number of threads R uses by setting OMP_NUM_THREADS= before you run R. If you want to try easily varying the number of threads you can save the above code into a script and run it using Rscript, changing OMP_NUM_THREADS each time you run it, e.g.:</p> <pre><code>export OMP_NUM_THREADS=1\n\nRscript ./data_table_test.R\n\nexport OMP_NUM_THREADS=2\n\nRscript ./data_table_test.R\n</code></pre> <p>The elapsed time that is printed out when the calculation is run represents how long the script/program took to run. It\u2019s important to bear in mind that, as with the matrix multiplication exercise, not everything will be parallelised. Creating the data table is done in serial so does not benefit from the addition of more threads.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L5_parallelised_r_analysis/#loop-and-function-parallelism","title":"Loop and function parallelism","text":"<p>R provides a number of different functions to run loops or functions in parallel. One of the most common functions is to use are the <code>{X}apply</code> functions:</p> <ul> <li> <p><code>apply</code> Apply a function over a matrix or data frame</p> </li> <li> <p><code>lapply</code> Apply a function over a list, vector, or data frame</p> </li> <li> <p><code>sapply</code> Same as <code>lapply</code> but returns a vector</p> </li> <li> <p><code>vapply</code> Same as <code>sapply</code> but with a specified return type that improves safety and can improve speed</p> </li> </ul> <p>For example:</p> <pre><code>res &lt;- lapply(1:3, function(i) {\n    sqrt(i)*sqrt(i*2)\n    })\n</code></pre> <p>The <code>{X}apply</code> functionality supports iteration over a dataset without requiring a loop to be constructed. However, the functions outlined above do not exploit parallelism, even if there is potential for parallelisation many operations that utilise them.</p> <p>There are a number of mechanisms that can be used to implement parallelism using the <code>{X}apply</code> functions. One of the simplest is using the <code>parallel</code> library, and the <code>mclapply</code> function:</p> <pre><code>library(parallel)\nres &lt;- mclapply(1:3, function(i) {\n    sqrt(i)\n})\n</code></pre> <p>Try experimenting with the above functions on large numbers of iterations, both with lapply and mclapply. Can you achieve better performance using the MC_CORES environment variable to specify how many parallel processes R uses to complete these calculations? The default on the SDF is 2 cores, but you can increase this in the same way we did for OMP_NUM_THREADS, e.g.:</p> <pre><code>export MC_CORES=16\n</code></pre> <p>Try different numbers of iterations of the functions (e.g. change 1:3 in the code to something much larger), and different numbers of parallel processes, e.g.:</p> <pre><code>export MC_CORES=2\n\nexport MC_CORES=8\n\nexport MC_CORES=16\n</code></pre> <p>If you have separate functions then the above approach will provide a simple method for parallelising using the resources within a single node. However, if your functionality is more loop-based, then you may not wish to have to package this up into separate functions to parallelise.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L5_parallelised_r_analysis/#parallelisation-with-foreach","title":"Parallelisation with foreach","text":"<p>The <code>foreach</code> package can be used to parallelise loops as well as functions. Consider a loop of the following form:</p> <pre><code>main_list &lt;- c()\nfor (i in 1:3) {\n    main_list &lt;- c(main_list, sqrt(i))\n}\n</code></pre> <p>This can be converted to <code>foreach</code> functionality as follows:</p> <pre><code>main_list &lt;- c()\nlibrary(foreach)\nforeach(i=1:3) %do% {\n    main_list &lt;- c(main_list, sqrt(i))\n}\n</code></pre> <p>Whilst this approach does not significantly change the performance or functionality of the code, it does let us then exploit parallel functionality in foreach. The <code>%do%</code> can be replaced with a <code>%dopar%</code> which will execute the code in parallel.</p> <p>To test this out we\u2019re going to try an example using the <code>randomForest</code> library. We can now run the following code in R:</p> <pre><code>library(foreach)\nlibrary(randomForest)\nx &lt;- matrix(runif(50000), 1000)\ny &lt;- gl(2, 500)\nrf &lt;- foreach(ntree=rep(250, 4), .combine=combine) %do%\nrandomForest(x, y, ntree=ntree)\nprint(rf)\n</code></pre> <p>Implement the above code and run with a <code>system.time</code> to see how long it takes. Once you have done this you can change the <code>%do%</code> to a <code>%dopar%</code> and re-run. Does this provide any performance benefits?</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L5_parallelised_r_analysis/#parallelisation-with-doparallel","title":"Parallelisation with doParallel","text":"<p>To exploit the parallelism with <code>dopar</code> we need to provide parallel execution functionality and configure it to use extra cores on the system. One method to do this is using the <code>doParallel</code> package.</p> <pre><code>library(doParallel)\nregisterDoParallel(8)\n</code></pre> <p>Does this now improve performance when running the <code>randomForest</code> example? Experiment with different numbers of workers by changing the number set in <code>registerDoParallel(8)</code> to see what kind of performance you can get. Note, you may also need to change the number of clusters used in the foreach, e.g. what is specified in the <code>rep(250, 4)</code> part of the code, to enable more than 4 different sets to be run at once if using more than 4 workers. The amount of parallel workers you can use is dependent on the hardware you have access to, the number of workers you specify when you setup your parallel backend, and the amount of chunks of work you have to distribute with your foreach configuration.</p>"},{"location":"safe-haven-services/superdome-flex-tutorial/L5_parallelised_r_analysis/#cluster-parallelism","title":"Cluster parallelism","text":"<p>It is possible to use different parallel backends for <code>foreach</code>. The one we have used in the example above creates new worker processes to provide the parallelism, but you can also use larger numbers of workers through a parallel cluster, e.g.:</p> <pre><code>my.cluster &lt;- parallel::makeCluster(8)\nregisterDoParallel(cl = my.cluster)\n</code></pre> <p>By default <code>makeCluster</code> creates a socket cluster, where each worker is a new independent process. This can enable running the same R program across a range of systems, as it works on Linux and Windows (and other clients). However, you can also fork the existing R process to create your new workers, e.g.:</p> <pre><code>cl &lt;-makeCluster(5, type=\"FORK\")\n</code></pre> <p>This saves you from having to create the variables or objects that were setup in the R program/script prior to the creation of the cluster, as they are automatically copied to the workers when using this forking mode. However, it is limited to Linux style systems and cannot scale beyond a single node.</p> <p>Once you have finished using a parallel cluster you should shut it down to free up computational resources, using <code>stopCluster</code>, e.g.:</p> <pre><code>stopCluster(cl)\n</code></pre> <p>When using clusters without the forking approach, you need to distribute objects and variables from the main process to the workers using the <code>clusterExport</code> function, e.g.:</p> <pre><code>library(parallel)\nvariableA &lt;- 10\nvariableB &lt;- 20\nmySum &lt;- function(x) variableA + variableB + x\ncl &lt;- makeCluster(4)\nres &lt;- try(parSapply(cl=cl, 1:40, mySum))\n</code></pre> <p>The program above will fail because <code>variableA</code> and <code>variableB</code> are not present on the cluster workers. Try the above on the SDF and see what result you get.</p> <p>To fix this issue you can modify the program using <code>clusterExport</code> to send <code>variableA</code> and <code>variableB</code> to the workers, prior to running the <code>parSapply</code> e.g.:</p> <pre><code>clusterExport(cl=cl, c('variableA', 'variableB'))\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/container-examples/","title":"Container Examples","text":"<p>To help with writing your own Dockerfiles to run within the TRE via the CES, a set of examples for commonly used software stacks can be found in our TRE Container Samples repository.</p> <p>Please contact your Service Manager if you need further support with the use of containers.</p> Application Comments Julia Octave PostgreSQL Python Quarto Separate containers for R and Jupyter Rocker Interactive <p>Additional examples can be made available on request. To do so, please contact the EIDF Helpdesk referencing this page, otherwise send your query by email to eidf@epcc.ed.ac.uk with the subject line \"TRE Container Support\".</p> Available on demand Comments Freesurfer Jamovi MinIO S3 Nextflow PSPP Stata <p>These containers are not fully fledged applications or workflow examples, but provide a template for setting up the technical parts of the containerisation process such as user mapping, and mapping to any required <code>safe_data</code> directories or similar. Please refer to the <code>README</code> in each example for guidance on how to use the container.</p>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/","title":"Development Workflow","text":"<p>This document describes in detail the four steps recommended for the development of containers for TRE use.</p>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#step-1-writing-a-dockerfile","title":"Step 1. Writing a <code>Dockerfile</code>","text":""},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#11-tre-specific-advice","title":"1.1 TRE-specific advice","text":"<ul> <li> <p>Declare TRE specific directories using the line <code>RUN mkdir /safe_data /safe_outputs /scratch</code> in your Dockerfile. While the CES tools automatically generate these directories within the container, explicitly creating them    enhances transparency and helps others more easily understand the container\u2019s structure and operation.    Note that files should not be added to these directories through the Dockerfile prior to mounting, as they would be overwritten during the mounting process. The directories will be fully accessible within the container during run time.</p> </li> <li> <p>Start with a well-known application base container on a public registry.    Projects should add a minimum of additional project software and packages    so that the container is clearly built for a specific purpose. Avoid    patching and preserve the original container setup wherever possible.    Complex containers, particularly interactive ones, have been carefully    designed by competent developers to meet high security and usability    standards. Modifying these without the required knowledge might introduce    unwanted risks and side effects.</p> </li> <li> <p>Do not copy data files into the image. As a general rule, images should only contain software and configuration files. Any data files required will be presented to the container at runtime (e.g. via the <code>/safe_data</code> mount) and should not be copied into the container during the build.</p> </li> <li> <p>Add all the additional content (code files, libraries, packages, data, and licences) needed for your analysis work to your Dockerfile. Since the TRE VMs do not have internet access, all necessary code, dependencies, and resources must be pre-packaged within the container to ensure it runs successfully.</p> </li> <li> <p>Apply the principle of least privilege, that is select a non-privileged user inside the container whenever possible.</p> </li> </ul>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#12-general-recommendations","title":"1.2 General recommendations","text":"<p>It is highly recommended that users follow these Dockerfile best practice guidelines:</p> <ul> <li>Use fully-qualified and pinned images in all <code>FROM</code> statements. Images can be hosted on multiple repositories, and image tags are mutable. The only way to ensure reproducible builds is by pinning images by their full signature. The signature of an image can be viewed with <code>docker inspect --format='{{index .RepoDigests 0}}' &lt;image&gt;</code>. The image repository is usually <code>docker.io</code> or <code>ghcr.io</code>.</li> </ul> <p>Example:</p> <pre><code># Incorrect\nFROM nvidia/cuda:latest\nFROM docker.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04\nFROM docker.io/nvidia/cuda:latest\n\n# Correct\nFROM docker.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04@sha256:622e78a1d02c0f90ed900e3985d6c975d8e2dc9ee5e61643aed587dcf9129f42\n</code></pre> <ul> <li>Group consecutive and related commands into a single <code>RUN</code> statement. Each <code>RUN</code> statement causes a new layer to be created in the image. By grouping <code>RUN</code> statements together, and deleting temporary files, the final image image size can be greatly reduced.</li> </ul> <p>Example:</p> <pre><code># Incorrect\nRUN apt-get -y update\nRUN apt-get -y install curl\nRUN apt-get -y install git\n\n# Correct\n# - Single RUN statement with commands broken over multiple lines\n# - Temporary apt files are deleted and not stored in the final image\nRUN : \\\n   &amp;&amp; apt-get update -qq \\\n   &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install \\\n         -qq -y --no-install-recommends \\\n         curl \\\n         git \\\n   &amp;&amp; apt-get clean \\\n   &amp;&amp; rm -rf /var/lib/apt/lists/* \\\n   &amp;&amp; :\n</code></pre> <ul> <li>Use <code>COPY</code> instead of <code>ADD</code>. Compared to the <code>COPY</code> command, <code>ADD</code> supports much more functionality such as unpacking archives and downloading from URLs. While this may seem convenient, using <code>ADD</code> may result in much larger images with layers which are unnecessary. In the following example, using COPY after unpacking locally makes the image more efficient:</li> </ul> <p>Example:</p> <pre><code># Incorrect\nFROM python:3.9-slim\n# Unpack a tar archive\nADD my_project.tar.gz /target-dir/\nCMD [\"python\", \"/target-dir/main.py\"]\n\n# Correct\nFROM python:3.9-slim\n# Unpack my_project.tar.gz locally first, then copy\nCOPY my_project/ /target-dir/\nCMD [\"python\", \"/target-dir/main.py\"]\n</code></pre> <ul> <li>Use multi-stage builds where appropriate. Separating build/compilation steps into a separate stage helps to minimise the content of the final image and reduce the overall size.</li> </ul> <p>Example:</p> <pre><code>FROM some-image AS builder\nRUN apt update &amp;&amp; apt -y install build-dependencies\nCOPY . .\nRUN ./configure --prefix=/opt/app &amp;&amp; make &amp;&amp; make install\n\nFROM some-minimal-image\nRUN apt update &amp;&amp; apt -y install runtime-dependencies\nCOPY --from=builder /opt/app /opt/app\n</code></pre> <ul> <li>Use a minimal image in the last stage to reduce the final image size. When using multi-stage builds, the final <code>FROM</code> image should use a minimal image such as from Distroless or Chainguard to minimise image content and size.</li> </ul> <p>Example:</p> <pre><code>FROM some-image AS builder\n# ...\nFROM gcr.io/distroless/base-debian12\n# ...\n</code></pre> <ul> <li>Use a linter such as Hadolint to verify the content of the <code>Dockerfile</code>.  Automated code linting tools can be very useful in detecting common mistakes and pitfalls when developing software. Some configuration tweaks may be required however, as shown in the example below.</li> </ul> <p>Example:</p> <pre><code># Ignore DL3008 (Pin versions in apt get install)\ndocker run --pull always --rm -i docker.io/hadolint/hadolint:latest hadolint --ignore DL3008 - &lt; Dockerfile\n</code></pre> <p>See the following resources for additional recommendations:</p> <ul> <li>https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316</li> <li>https://docs.docker.com/develop/develop-images/dockerfile_best-practices/</li> <li>https://sysdig.com/blog/dockerfile-best-practices/</li> </ul>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#step-2-build-test-locally-and-push-to-registry","title":"Step 2. Build, test locally and push to registry","text":""},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#21-build-and-test-the-container-locally","title":"2.1 Build and test the container locally","text":"<p>Docker, Podman, Kubernetes, and Apptainer container images can be created from a single Dockerfile, as all of them support the OCI container image format either natively or indirectly.</p> <p>Containers can be built using the following command:</p> <pre><code>docker build -t &lt;image&gt;:&lt;tag&gt; -f &lt;path-to-Dockerfile&gt;\n</code></pre> <p>Run your image locally, so that minor errors can be immediately fixed before proceeding with the next steps:</p> <pre><code>docker run &lt;options&gt; &lt;image&gt;:&lt;tag&gt;\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#example","title":"Example","text":"<p>A container with the following Dockerfile:</p> <pre><code>   FROM python:3.13.3@sha256:a4b2b11a9faf847c52ad07f5e0d4f34da59bad9d8589b8f2c476165d94c6b377\n\n   # Create TRE directories\n   RUN mkdir /safe_data /safe_outputs /scratch\n\n   # Add app files\n   COPY . /app\n   WORKDIR /app\n\n   # Install Python dependencies\n   RUN pip install --no-cache-dir -r requirements.txt\n\n   # Default command\n   CMD [\"python\", \"app.py\"]\n</code></pre> <p>where this is the app structure:</p> <pre><code>.\n\u251c\u2500\u2500 app.py\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 Dockerfile\n</code></pre> <p>can be built with the command:</p> <pre><code>docker build -t myapp:v1.1 . --platform linux/amd64\n</code></pre> <p>where <code>--platform linux/amd64</code> is added to ensure image compatibility with the TRE environment in case the image is being built on a different platform.</p> <p>The container can then be tested with:</p> <pre><code>docker run --rm myapp:v1.1\n</code></pre> <p>Podman can be used equivalently to Docker in the commands above.</p>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#22-automating-dockerfile-validation","title":"2.2 Automating Dockerfile validation","text":"<p>During development, we recommend that tools like GitHub or GitLab are used for version control and recording of the image content. Using the <code>pre-commit</code> tool, it is possible to configure your local repository so that Hadolint (and similar tools) are run automatically each time <code>git commit</code> is run. This is recommended to ensure linting and auto-formatting tools are always run before code is pushed to GitHub.</p> <p>To run Hadolint, include the hook in your <code>.pre-commit-config.yaml</code> file:</p> <pre><code>repos:\n  - repo: https://github.com/hadolint/hadolint\n    rev: v2.12.0\n    hooks:\n      - id: hadolint-docker\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#23-push-to-ghcr","title":"2.3 Push to GHCR","text":"<p>To prepare the image before pushing to GHCR, we recommend to save the image with a unique, descriptive tag. While it is useful to define a <code>latest</code> tag, each production image should also be tagged with a label such as the version or build date. For non-local images, the registry and repository should also be included. Images can also be tagged multiple times.</p> <p>Example:</p> <pre><code>docker build \\\n   --tag ghcr.io/my/image:v1.2.3 \\\n   --tag ghcr.io/my/image:latest \\\n   ...\n</code></pre> <p>The following commands can be used to upload the image to a private GHCR repository.</p> <pre><code>echo \"${GHCR_TOKEN}\" | docker login ghcr.io -u $GHCR_NAMESPACE --password-stdin\ndocker build . --tag \"ghcr.io/$GHCR_NAMESPACE/$IMAGE:$TAG\" --tag \"ghcr.io/$GHCR_NAMESPACE/$IMAGE:latest\"\ndocker push \"ghcr.io/$GHCR_NAMESPACE/$IMAGE:latest\"\ndocker push \"ghcr.io/$GHCR_NAMESPACE/$IMAGE:$TAG\"\ndocker logout\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#25-optional-build-and-upload-automation-using-github-actions","title":"2.5 Optional - Build and upload automation using GitHub Actions","text":""},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#building-in-ci","title":"Building in CI","text":"<p>Below is a sample GHA configuration which runs Hadolint, builds a container named <code>ghcr.io/my/repo</code>, then runs the Trivy container scanning tool. The Trivy SBOM report is then uploaded as a job artifact.</p> <p>This assumes:</p> <ul> <li>The repo contains a <code>Dockerfile</code> in the top-level directory,</li> <li>The <code>Dockerfile</code> contains an <code>ARG</code> or <code>ENV</code> variable which defines the version of the packaged software.</li> </ul> <pre><code># File .github/workflows/main.yaml\nname: main\non:\n  push:\ndefaults:\n  run:\n    shell: bash\njobs:\n  build:\n    runs-on: ubuntu-22.04\n    steps:\n      - name: checkout\n        uses: actions/checkout@v4\n      - name: run hadolint\n        run: docker run --rm -i ghcr.io/hadolint/hadolint hadolint --failure-threshold error - &lt; Dockerfile\n      - name: build image\n        run: |\n          set -euxo pipefail\n          repository=\"ghcr.io/myrepo\"\n          version=\"myversion\"\n          image=\"${repository}:${version}\"\n          docker build . --tag \"${image}\"\n          echo \"image=${image}\" &gt;&gt; \"$GITHUB_ENV\"\n          echo \"Built ${image}\"\n      - name: push image\n        run: |\n          set -euxo pipefail\n          echo \"${{ secrets.GITHUB_TOKEN }}\" | docker login ghcr.io -u $ --password-stdin\n          docker push \"${image}\"\n      - name: run trivy\n        uses: aquasecurity/trivy-action@master\n        with:\n          image-ref: \"${{ env.image }}\"\n          format: 'github'\n          output: 'dependency-results.sbom.json'\n          github-pat: \"${{ secrets.GITHUB_TOKEN }}\"\n          severity: 'MEDIUM,CRITICAL,HIGH'\n          scanners: \"vuln\"\n      - name: upload trivy report\n        uses: actions/upload-artifact@v4\n        with:\n          name: 'trivy-sbom-report'\n          path: 'dependency-results.sbom.json'\n</code></pre> <p>Note that manually running Hadolint via pre-commit can be skipped if you are using pre-commit and the pre-commit.ci service.</p>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#publishing-in-ci","title":"Publishing in CI","text":"<p>Note Images can also be built and pushed from your local environment as normal.</p> <p>Once the stage has been reached where your software package is ready for distribution, the GHA example above can be extended to automatically publish new image versions to the GHCR. An introduction to GHCR can be found in the GitHub docs here.</p> <pre><code># After the image has been built and scanned\n- name: push image\n  run: |\n    set -euxo pipefail\n    echo \"${{ secrets.GITHUB_TOKEN }}\" | docker login ghcr.io -u $ --password-stdin\n    docker push \"${image}\"\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#step-3-test-in-ces-test-environment","title":"Step 3. Test in CES test environment","text":"<p>EPCC provides a test environment that allows users to test their containers in a TRE-like environment without having to be logged into the TRE itself. Containers that run successfully in such environment can then be expected to perform in the same way in the TRE.</p>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#31-accessing-test-environment","title":"3.1 Accessing test environment","text":"<p>The test environment is located in the eidf147 project. Please ask your research coordinator to contact EPCC and request for you to be added to the test environment.</p>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#32-pull-and-run","title":"3.2 Pull and run","text":"<p>Do not use container CLIs directly</p> <p>The CES wrapper scripts must be used to run containers in the TRE. This is to ensure that the correct data directories are automatically made available.</p> <p>You must not use commands such as <code>podman run ...</code> or <code>docker run ...</code> directly.</p> <p>Containers can only be used on the TRE desktop hosts using shell commands. Containers can only be pulled from the GHCR into the TRE using a <code>ces-pull</code> script. Hence containers must be pushed to GHCR for them to be used in the TRE. Although alternative methods can be used in the test environment, we encourage users to follow the exact same procedure as they would in the TRE.</p> <p>Once access has been granted to the test environemnt in the eidf147 project, the user can pull a container from their private GHCR repository using the <code>ces-pull</code> command:</p> <pre><code>ces-pull [&lt;runtime&gt;] &lt;github_user&gt; &lt;ghcr_token&gt; ghcr.io/&lt;namespace&gt;/&lt;container_name&gt;:&lt;container_tag&gt;\n</code></pre> <p>If a runtime is not specified then podman is used as the default.</p> <p>The container can then be run with <code>ces-run</code>:</p> <pre><code>ces-run [&lt;runtime&gt;] ghcr.io/&lt;namespace&gt;/&lt;container_name&gt;:&lt;container_tag&gt;\n</code></pre> <p><code>ces-run</code> supports the following optional arguments under most runtimes:</p> <ul> <li>Use <code>--opt-file=&lt;file&gt;</code> to specify a file containing additional options to be passed to the runtime</li> <li>Use <code>--arg-file=&lt;file&gt;</code> to specify a file containing arguments to pass to the container command or entrypoint</li> <li>Use <code>--env-file=&lt;file&gt;</code> to specify a file containing environment variables which will be set inside the container</li> </ul> <p>Containers that require a GPU can be run by adding the <code>--gpu</code> option. See <code>ces-run [&lt;runtime&gt;] --help</code> for all available options.</p> <p>We recommend to test containers without network connection to best mimick their functionality inside the TRE, where the container will not be able to access the internet. With Podman, for example, this can be achieved by passing the option <code>--network=none</code> through the <code>opt-file</code>.</p> <p>Once the container runs successfully in the test environment, it is ready to be used inside the TRE.</p>"},{"location":"safe-haven-services/tre-container-user-guide/development-workflow/#step-4-pull-and-run-container-inside-the-tre","title":"Step 4. Pull and run container inside the TRE","text":"<p>To use the containers inside the TRE, log into the desired VM using the steps described in the EIDF User Documentation: https://docs.eidf.ac.uk/safe-haven-services/safe-haven-access/</p> <p>The same steps described in Section 3.2 of this document can then be used to pull and run the container.</p>"},{"location":"safe-haven-services/tre-container-user-guide/introduction/","title":"Introduction","text":""},{"location":"safe-haven-services/tre-container-user-guide/introduction/#overview","title":"Overview","text":"<p>The Container Execution Service (CES) has been introduced to allow project code developed and tested by researchers outside the Trusted Research Environment (TRE) in personal development environments to be imported and run on the project data inside the TRE using a well-documented, transparent, secure workflow. The primary role of the TRE is to store and share data securely; it is not intended to be a software development and testing environment. The CES helps researchers perform software development tasks in their chosen environment, rather than the restricted one offered in the TRE. This guide describes the process of building and testing TRE-ready containers outside the TRE, specifically within a test enviroment created in the EIDF, so that they are ready to be pulled and used inside the TRE.</p> <p>We assume that users are already familiar with container concepts and have some experience in building their own images. If a user is new to container tools, the following resources would be a good starting point:</p> <ul> <li>https://carpentries-incubator.github.io/docker-introduction/</li> <li>https://docs.docker.com/get-started/overview/</li> <li>https://docker-curriculum.com/</li> </ul> <p>It is also assumed that software development best practices are followed, such as version control using Git(Hub) and Continuous Integration (CI). This can help create container build audit trails to satisfy any IG (Information Governance) concerns about the provenance of the project code.</p> <p>The development process includes the following steps:</p> <ol> <li>Create a Dockerfile following general recommendations and TRE-specific advice.</li> <li>Build the image and upload to the GitHub Container Registry (GHCR), either locally or using a CI/CD pipeline.</li> <li>Test the container in the CES test environment to ensure it functions correctly and has no external runtime dependencies.</li> <li>Login to a TRE desktop enabled for container execution to pull and run the container.</li> </ol> <p>This guide contains a number of workflow examples designed to assist users in building TRE-ready containers. Other container examples are also available in our TRE Container Samples repository.</p>"},{"location":"safe-haven-services/tre-container-user-guide/introduction/#tre-directories","title":"TRE directories","text":"<p>Before continuing with this guide, it is important that the users are aware of the TRE file system directories so that they can be used correctly within their containers. Project data inside the TRE can be found in the <code>/safe_data/&lt;project-id&gt;</code> directory. The CES tools automatically map this to a directory called <code>/safe_data</code> inside the container. Additionally, two directories are created and mapped to the user's home directory. The first one is <code>/scratch</code>, a temporary directory that is removed after container termination on the host system. The second one is a unique container job output directory mapped to the <code>/safe_outputs</code> directory in the container, where output files that the users wishes to preserve should be placed. Note that containers that have been pulled into the TRE are destroyed after they have been run. Only the files written to the container outputs directory are guaranteed to be retained.</p> Directory on host system Directory in container Intended use <code>/safe_data/&lt;your_project_name&gt;/</code> /<code>safe_data</code> Read-only access if required by IG, or read-write access, to data and other project files. <code>~/outputs_&lt;unique_id&gt;</code> <code>/safe_outputs</code> Will be created at container startup as an empty directory. Intended for any outputs: logs, data, models. Content saved in this directory will be retained after the container exits. <code>~/scratch_&lt;unique_id&gt;</code> <code>/scratch</code> Temporary directory that is removed after container termination on the host system. Any temporary files should be placed here. <p>Temporary files can also be written into any directory in the container\u2019s internal file system. However, the use of <code>/scratch</code> can be more efficient if the service is able to mount it on high-performing storage devices.</p>"},{"location":"safe-haven-services/tre-container-user-guide/introduction/#advising-information-governance-of-required-software-stack","title":"Advising Information Governance of required software stack","text":"<p>Projects must establish that the software stack they intend to import in the container is acceptable for the project\u2019s IG approvals. Projects should only seek to use container-based software if the standard TRE desktop environment is not sufficient for the research scope. However, it is broadly understood that the standard desktop software, whilst useful in most cases, is inadequate for many purposes and specifically ML, and software import using containers is intended to address this.</p>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/","title":"Workflow Examples","text":"<p>The following sections will guide the user in the process of creating different types of containers.</p> <p>For a complete list of examples, please see our TRE Container Samples repository, at EPCCed/tre-container-samples.</p>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#example-1-pytorch","title":"Example 1 - PyTorch","text":"<p>This section will explain how to create a container that runs a script using PyTorch inside the TRE. The expected directory structure is:</p> <pre><code>  \u251c\u2500\u2500 Dockerfile\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 torch_gpu_test.py\n</code></pre> <p>where <code>requirements.txt</code> contains:</p> <pre><code>numpy\ntorch\n</code></pre> <p>and <code>torch_gpy_test.py</code> is any script that performs a test task using PyTorch.</p>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e1-step-1-writing-the-dockerfile","title":"E1 - Step 1. Writing the Dockerfile","text":"<p>In our Dockerfile, we want to start by using an officially supported image that already contains as much of the software required to run the script as possible.</p> <p>In our case, we want a ready-made python 3 image to start with, which we can find in DockerHub. The latest stable version at the time of writing is 3.13.3, so we will use this and include the digest in our Dockerfile.</p> <p>Next, we need to set up the TRE directories, copy our files into the container, install the necessary packages and finally execute the script. This process can be accomplished with the following Dockerfile:</p> <pre><code>FROM python:3.13.3@sha256:a4b2b11a9faf847c52ad07f5e0d4f34da59bad9d8589b8f2c476165d94c6b377\n\n# Create TRE directories\nRUN mkdir /safe_data /safe_outputs /scratch\n\nWORKDIR /usr/app\nCOPY requirements.txt ./\n\n# Use hadolint ignore=DL3013 when linting this Dockerfile.\nRUN pip install --no-cache-dir --upgrade pip  &amp;&amp; \\\n    pip install --no-cache-dir -r requirements.txt\n\nCOPY torch_gpu_test.py .\n\nCMD [\"python\", \"./torch_gpu_test.py\"]\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e1-step-2-build-and-push-to-ghcr","title":"E1 - Step 2. Build and push to GHCR","text":"<p>As mentioned in our guidelines, it is good practice to check the Dockerfile with a linting tool to detect common mistakes before building our container:</p> <pre><code># Ignore DL3013 to allow --upgrade\ndocker run --pull always --rm -i docker.io/hadolint/hadolint:latest hadolint --ignore DL3013 - &lt; Dockerfile\n</code></pre> <p>We can then define our GHCR variables:</p> <pre><code>export GHCR_NAMESPACE=mynamespace\nexport GHCR_TOKEN=mytoken\n</code></pre> <p>build our container:</p> <pre><code>docker build . --tag ghcr.io/$GHCR_NAMESPACE/pytorch-test:v1.1 --tag ghcr.io/$GHCR_NAMESPACE/pytorch-test:latest --platform linux/amd64\n</code></pre> <p>and run it locally:</p> <pre><code>docker run --rm ghcr.io/$GHCR_NAMESPACE/pytorch-test:v1.1\n</code></pre> <p>If the container runs without errors, we can push our image to GHCR using our namespace and token:</p> <pre><code>echo \"${GHCR_TOKEN}\" | docker login ghcr.io -u $GHCR_NAMESPACE --password-stdin\ndocker push \"ghcr.io/$GHCR_NAMESPACE/pytorch-test:v1.1\"\ndocker push \"ghcr.io/$GHCR_NAMESPACE/pytorch-test:latest\"\ndocker logout\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e1-step-3-test-in-ces-test-environment","title":"E1 - Step 3. Test in CES test environment","text":"<p>Log into the 'ces-dev02' VM of the project EIDF147, which is the designated test environment for the CES.</p> <p>Then, pull and run the container using the commands:</p> <pre><code>ces-pull podman $GHCR_NAMESPACE $GHCR_TOKEN ghcr.io/$GHCR_NAMESPACE/pytorch-test:v1.1\nces-run podman --gpu ghcr.io/$GHCR_NAMESPACE/pytorch-test:v1.1\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e1-step-4-pull-and-run-in-the-tre","title":"E1 - Step 4. Pull and run in the TRE","text":"<p>The container can be imported and run inside the TRE using the same commands as the previous step.</p>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#example-2-python-ml","title":"Example 2 - Python ML","text":"<p>This example demonstrates how to build a container which requires a ML model that would normally be downloaded from the internet when first run. Such models are typically cached in a hidden directory so it can be difficult to understand how to manually download the model, where to put it, and how to load it.</p> <p>The approach we take is to run a sample piece of code during the container build phase, which downloads the model to the hidden cache directory. This then becomes part of the container.</p> <p>The expected directory structure is:</p> <pre><code>  \u251c\u2500\u2500 Dockerfile\n  \u251c\u2500\u2500 cache-easyocr.py\n  \u251c\u2500\u2500 doc1.png\n  \u251c\u2500\u2500 test_easyocr.py\n</code></pre> <p>where <code>cache-easyocr</code> contains:</p> <pre><code>#!/usr/bin/env python3\n\nimport easyocr\nreader = easyocr.Reader(['en'])\n</code></pre> <p><code>test_easyocr.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\nimport easyocr\nimport json\nimport sys\nimport shutil\n\nsrc=\"/src/\"\ndst=\"/safe_outputs/\"\nreader = easyocr.Reader(['en'])\n\nfor filename in sys.argv[1:]:\n    print(\"Processing file: \", filename)\n    text_list = reader.readtext(filename)\n\n    entities = []\n    for (bbox_list, text, confidence) in text_list:\n        print('Found text \"%s\" with confidence %f at %s' % (text, confidence, bbox_list))\n        entities.append({'text': text,\n            'confidence': confidence,\n            })\n    out_file = filename + '.json'\n    with open(out_file, 'w') as fd:\n        json.dump(entities, fd)\n        print(entities)\n    shutil.copyfile(out_file, dst + out_file.split(\"/\")[2])\n</code></pre> <p>and <code>doc1.png</code> is an image that contains text.</p>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e2-step-1-writing-the-dockerfile","title":"E2 - Step 1. Writing the Dockerfile","text":"<p>Similarly to the PyTorch example, we will start our Dockerfile using the <code>python:3</code> image:</p> <pre><code>FROM python:3.13.3@sha256:a4b2b11a9faf847c52ad07f5e0d4f34da59bad9d8589b8f2c476165d94c6b377\n</code></pre> <p>We then install easyocr and download the HuggingFace ML models:</p> <pre><code>RUN pip install easyocr\nCOPY cache_easyocr.py .\nRUN python ./cache_easyocr.py\n</code></pre> <p>As there is no internet in the TRE, we need to make sure that HuggingFace does not attempt hub access:</p> <pre><code>ENV HF_HUB_OFFLINE=1\n</code></pre> <p>We can then create the TRE directories, copy the files inside the container and execute the script:</p> <pre><code>RUN mkdir /safe_data /safe_outputs /scratch /src\nCOPY test_easyocr.py doc1.png /src/\nENTRYPOINT [ \"python3\", \"/src/test_easyocr.py\", \"/src/doc1.png\"]\n</code></pre> <p>The final Dockerfile is:</p> <pre><code>FROM python:3.13.3@sha256:a4b2b11a9faf847c52ad07f5e0d4f34da59bad9d8589b8f2c476165d94c6b377\n\n# Install easyocr globally\nRUN pip install easyocr\n\n# Download ML models during the build phase\nCOPY cache_easyocr.py .\nRUN python ./cache_easyocr.py\n\n# Set environment variable to prevent HuggingFace hub access\nENV HF_HUB_OFFLINE=1\n\n# Create TRE directories\nRUN mkdir /safe_data /safe_outputs /scratch /src\n# Copy files inside the container\nCOPY test_easyocr.py doc1.png /src/\n# Run the script with the desired parameters\nENTRYPOINT [ \"python3\", \"/src/test_easyocr.py\", \"/src/doc1.png\"]\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e2-step-2-build-and-push-to-ghcr","title":"E2 - Step 2. Build and push to GHCR","text":"<p>As mentioned in our guidelines, before building our container we first want to check the Dockerfile with a linting tool to detect common mistakes:</p> <pre><code># Ignore DL3013, DL3042 and DL3045\ndocker run --pull always --rm -i docker.io/hadolint/hadolint:latest hadolint --ignore DL3013 --ignore DL3042 --ignore DL3045 - &lt; Dockerfile\n</code></pre> <p>Note that we can deliberately ignore hadolint flags if the selected features are required by the container to run successfully.</p> <p>We then define our GHCR variables:</p> <pre><code>export GHCR_NAMESPACE=mynamespace\nexport GHCR_TOKEN=mytoken\n</code></pre> <p>build our container:</p> <pre><code>docker build . --tag ghcr.io/$GHCR_NAMESPACE/python-ml-test:v1.1 --tag ghcr.io/$GHCR_NAMESPACE/python-ml-test:latest --platform linux/amd64\n</code></pre> <p>and run it locally:</p> <pre><code>docker run --rm --network=none test_easyocr ghcr.io/$GHCR_NAMESPACE/python-ml-test:v1.1\n</code></pre> <p>If successful, we should see an output of this type:</p> <pre><code>Processing file:  /src/doc1.png\nFound text \"Apple is looking at buying U.K.\" with confidence 0.771268 at [[np.int32(1), np.int32(4)], [np.int32(221), np.int32(4)], [np.int32(221), np.int32(25)], [np.int32(1), np.int32(25)]]\nFound text \"startup\" with confidence 0.994821 at [[np.int32(225), np.int32(9)], [np.int32(279), np.int32(9)], [np.int32(279), np.int32(23)], [np.int32(225), np.int32(23)]]\nFound text \"for\" with confidence 0.999294 at [[np.int32(283), np.int32(7)], [np.int32(307), np.int32(7)], [np.int32(307), np.int32(21)], [np.int32(283), np.int32(21)]]\nFound text \"billion\" with confidence 0.999928 at [[np.int32(331), np.int32(7)], [382, np.int32(7)], [382, np.int32(21)], [np.int32(331), np.int32(21)]]\n[{'text': 'Apple is looking at buying U.K.', 'confidence': np.float64(0.7712680738082065)}, {'text': 'startup', 'confidence': np.float64(0.9948211556483356)}, {'text': 'for', 'confidence': np.float64(0.9992943653537125)}, {'text': 'billion', 'confidence': np.float64(0.9999277279271659)}]\n</code></pre> <p>If the container runs without errors, we can push our image to GHCR using our namespace and token:</p> <pre><code>echo \"${GHCR_TOKEN}\" | docker login ghcr.io -u $GHCR_NAMESPACE --password-stdin\ndocker push \"ghcr.io/$GHCR_NAMESPACE/python-ml-test:v1.1\"\ndocker push \"ghcr.io/$GHCR_NAMESPACE/python-ml-test:latest\"\ndocker logout\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e2-step-3-test-in-ces-test-environment","title":"E2 - Step 3. Test in CES test environment","text":"<p>As with the previous example, we now want to test our container in the designated test environment for the CES. Log into the 'ces-dev02' VM of the project EIDF147, then pull and run the container using the commands:</p> <pre><code>ces-pull podman $GHCR_NAMESPACE $GHCR_TOKEN ghcr.io/$GHCR_NAMESPACE/python-ml-test:v1.1\nces-run podman --gpu ghcr.io/$GHCR_NAMESPACE/python-ml-test:v1.1\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e2-step-4-pull-and-run-in-the-tre","title":"E2 - Step 4. Pull and run in the TRE","text":"<p>We can import and run the container in the TRE using the commands from the earlier step.</p>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#example-3-interactive-rstudio-rocker-container","title":"Example 3 - Interactive RStudio Rocker container","text":"<p>This section guides users through the process of creating a Rocker RStudio container that can be imported into the TRE and used for data analysis. In the example, a script is copied into the container, and the necessary packages are installed to ensure it runs correctly. Finally, RStudio is accessed from the host, allowing users to interact with the application as if it were running natively.</p> <p>We will assume that this is the directory structure of our files:</p> <pre><code>  .\n  \u251c\u2500\u2500 src\n  \u2502   \u251c\u2500\u2500 install_packages.R\n  \u2502   \u251c\u2500\u2500 plot_example.R\n  \u251c\u2500\u2500 Dockerfile\n</code></pre> <p>Where <code>install_packages.R</code> contains the following:</p> <pre><code>install.packages(\"ggplot2\")\n</code></pre> <p>and <code>plot_example.R</code> is a simple script that generates a graph:</p> <pre><code># library\nlibrary(ggplot2)\n\n# basic graph\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n# a data frame with all the annotation info\nannotation &lt;- data.frame(\n   x = c(2,4.5),\n   y = c(20,25),\n   label = c(\"label 1\", \"label 2\")\n)\n\n# Add text\np + geom_text(data=annotation, aes( x=x, y=y, label=label),                 ,\n           color=\"orange\",\n           size=7 , angle=45, fontface=\"bold\" )\n\nggsave(\"test_figure.png\")\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e3-step-1-writing-the-dockerfile","title":"E3 - Step 1. Writing the Dockerfile","text":"<p>We start the process by selecting the parent image. Searching for Rocker in DockerHub, we are presented with a number of options. Only some of these originate from official sources. We want to select the most appropriate image published from a reputable source, in our case the Rocker Project.</p> <p>For this example, we choose the latest version of Rocker RStudio, rocker/rstudio from Docker Hub.</p> <p>Clicking on \"tags\", we can then select \"latest\" and see the full signature of the image. We can then include the pinned image in our Dockerfile:</p> <pre><code>FROM docker.io/rocker/rstudio:latest@sha256:ee7c4efa46f0b5d46e051393ef05f262aceb959463b15fc3648955965290d231\n</code></pre> <p>The next step is to include the TRE directories:</p> <pre><code>RUN mkdir /safe_data /safe_outputs /scratch\n</code></pre> <p>We then want to copy our script inside the container. As we do not mean to preserve the script, only its output, we will copy it in a new <code>/src</code> directory. Note that we cannot save the files to <code>/scratch</code> as they would otherwise be overwritten during the mounting process to the TRE directories. To avoid repeating the <code>RUN</code> command, we will simply add our new directory to it:</p> <pre><code>RUN mkdir /safe_data /safe_outputs /scratch /src\n</code></pre> <p>and then copy the files:</p> <pre><code>COPY ./src/* /src\n</code></pre> <p>We then switch into our code directory:</p> <pre><code>WORKDIR /src\n</code></pre> <p>And finally install the required packages:</p> <pre><code>RUN r install_packages.R\n</code></pre> <p>This gives us the following Dockerfile:</p> <pre><code>FROM docker.io/rocker/rstudio:latest@sha256:ee7c4efa46f0b5d46e051393ef05f262aceb959463b15fc3648955965290d231\n\nRUN mkdir /safe_data /safe_outputs /scratch /src\n\nCOPY ./src/* /src\n\nWORKDIR /src\n\nRUN r install_packages.R\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e3-step-2-build-and-push-to-ghcr","title":"E3 - Step 2. Build and push to GHCR","text":"<p>Before building our container, we want to check the Dockerfile with a linting tool to detect common mistakes:</p> <pre><code># Ignore DL3008 (Pin versions in apt get install)\ndocker run --pull always --rm -i docker.io/hadolint/hadolint:latest hadolint --ignore DL3008 - &lt; Dockerfile\n</code></pre> <p>We can then define our GHCR variables:</p> <pre><code>export GHCR_NAMESPACE=mynamespace\nexport GHCR_TOKEN=mytoken\n</code></pre> <p>build our container:</p> <pre><code>docker build . --tag ghcr.io/$GHCR_NAMESPACE/rocker-test:v1.1 --tag ghcr.io/$GHCR_NAMESPACE/rocker-test:latest --platform linux/amd64\n</code></pre> <p>and run it locally:</p> <pre><code>docker run --rm -e \"PASSWORD=test\" -p 8787:8787 ghcr.io/$GHCR_NAMESPACE/rocker-test:v1.1\n</code></pre> <p>We can then access RStudio by navigating to 'localhost:8787' in a browser. At the login page, type 'root' and 'test' for username and password respectively. Note that you will only be 'root' within the context of the container and not outside of it. The same applies in the test environment and TRE.</p> <p>Once we made sure the container runs, we can push our image to GHCR using our namespace and token:</p> <pre><code>echo \"${GHCR_TOKEN}\" | docker login ghcr.io -u $GHCR_NAMESPACE --password-stdin\ndocker push \"ghcr.io/$GHCR_NAMESPACE/rocker-test:v1.1\"\ndocker push \"ghcr.io/$GHCR_NAMESPACE/rocker-test:latest\"\ndocker logout\n</code></pre>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e3-step-3-test-in-ces-test-environment","title":"E3 - Step 3. Test in CES test environment","text":"<p>Log into the 'ces-dev02' VM of the project EIDF147, which is the designated test environment for the CES.</p> <p>Rocker is one of those containers that requires to be started by the 'root' user. As such, it should be run inside the TRE - and our test environemnt - using podman. To pull the container using podman as our container engine, we use the command:</p> <pre><code>ces-pull podman $GHCR_NAMESPACE $GHCR_TOKEN ghcr.io/$GHCR_NAMESPACE/rocker-test:v1.1\n</code></pre> <p>The Rocker container was designed to be run using docker. In order for it to run successfully with podman, the container directories <code>/var/lib/rstudio-server</code> and <code>/run</code> need to be mounted to a tmpfs. As such, the following options are required:</p> <pre><code>'--mount type=tmpfs,destination=/var/lib/rstudio-server'\n'--mount type=tmpfs,destination=/run'\n</code></pre> <p>Our full <code>opt-file.txt</code> then looks like this:</p> <pre><code>-p 8787:8787\n-it\n--mount type=tmpfs,destination=/var/lib/rstudio-server\n--mount type=tmpfs,destination=/run\n</code></pre> <p>If we want to set our password, we can add it to the <code>env-file.txt</code> as follows:</p> <pre><code>PASSWORD=test\n</code></pre> <p>We can then run our container:</p> <pre><code>ces-run podman --opt-file opt-file.txt --env-file env-file.txt ghcr.io/$GHCR_NAMESPACE/rocker-test:v1.1\n</code></pre> <p>To streamline the process, we can place all the necessary commands into an executable script <code>run.sh</code> as follows:</p> <pre><code>opt_file=\"./opt-file.txt\"\n\nif [ -f \"$opt_file\" ] ; then\n        rm \"$opt_file\"\nfi\necho -e '-p 8787:8787' &gt;&gt; ${opt_file}\necho -e '-it' &gt;&gt; ${opt_file}\necho -e '--mount type=tmpfs,destination=/var/lib/rstudio-server' &gt;&gt; ${opt_file}\necho -e '--mount type=tmpfs,destination=/run' &gt;&gt; ${opt_file}\n\nenv_file=\"./env-file.txt\"\n\nif [ -f \"$env_file\" ] ; then\n        rm \"$env_file\"\nfi\necho -e \"PASSWORD=test\" &gt;&gt; ${env_file}\n\nces-run podman --opt-file $opt_file --env-file $env_file ghcr.io/$GHCR_NAMESPACE/rocker-test:v1.1\n</code></pre> <p>After executing the <code>run.sh</code> script, we can open a browser tab and access RStudio at <code>localhost:8787</code>. As done previously during the local test, we can log in using the credentials <code>root</code> and <code>test</code> for username and password respectively. From within RStudio, we can then run our script, which can be found in <code>/src</code>, and then copy our output to <code>/safe_output</code> so that it can be preserved after we exit the container.</p> <p>The container is running successfully if:</p> <ul> <li>The log-in is successful.</li> <li>The rstudio user has full access to TRE directories <code>/safe_data</code>, <code>/safe_outputs</code> and <code>/scratch</code>.</li> <li>The files saved in <code>/safe_outputs</code> and <code>/safe_data</code> (when writing permission is granted by IG) have correct permission on the host, that is they belong to the logged-in user.</li> </ul>"},{"location":"safe-haven-services/tre-container-user-guide/workflow-examples/#e3-step-4-pull-and-run-in-the-tre","title":"E3 - Step 4. Pull and run in the TRE","text":"<p>The container can then be imported inside the TRE using the same commands as Step 3.</p>"},{"location":"services/","title":"EIDF Services","text":""},{"location":"services/#computing-services","title":"Computing Services","text":"<p>Data Science Virtual Desktops</p> <p>Managed File Transfer</p> <p>Notebooks</p> <p>Ultra2</p> <p>Graphcore Bow Pod64</p> <p>GitLab Code Storage</p>"},{"location":"services/#data-services","title":"Data Services","text":"<p>S3</p> <p>Data Catalogue</p>"},{"location":"services/cerebras/","title":"Cerebras CS3 Wafer-Scale Cluster","text":"<p>Overview</p> <p>Connect</p> <p>Running jobs</p>"},{"location":"services/cerebras/access/","title":"Overview","text":"<p>The Cerebras CS3 Wafer-Scale cluster is a cluster of 4 CS3 Wafer-Scale Engines running as a cluster at EPCC.</p>"},{"location":"services/cerebras/access/#specifications","title":"Specifications","text":"<p>The specifications of the CS3 system are listed here: Cerebras CS3.</p>"},{"location":"services/cerebras/access/#getting-access","title":"Getting Access","text":"<p>Access to the Cerebras service is currently by arrangement with EIDF. Please apply for a project on the EIDF Portal.</p>"},{"location":"services/cerebras/connect/","title":"Login","text":"<p>The route for SSH access to the CS3 user-nodes is via the <code>eidf-gateway</code> bastion, as for the EIDF VirDS desktop VMs. This requires two credentials, your SSH key pair protected by a passphrase and a Time-based one-time password (TOTP).</p>"},{"location":"services/cerebras/connect/#access-credentials","title":"Access credentials","text":"<p>To access the CS3 system, follow the login example below. For the majority of authorised users, no further passwords or credentials are required other than for <code>eidf-gateway</code>.</p> <p>To configure your SSH keys and MFA (TOTP) token, see our general SSH documentation.</p>"},{"location":"services/cerebras/connect/#ssh-login-example","title":"SSH Login example","text":"<p>To login to the CS3 user-nodes, you will need to use the SSH Key and TOTP token as noted above. With the appropriate key loaded<code>ssh -J &lt;username&gt;@eidf-gateway.epcc.ed.ac.uk cerebras</code> will then prompt you, roughly once per day, for your TOTP code.</p>"},{"location":"services/cerebras/run/","title":"Running jobs","text":""},{"location":"services/cerebras/run/#software","title":"Software","text":"<p>The user-nodes are equipped with a normal developer packages. If you feel that something is missing, please submit a ticket via the Portal</p>"},{"location":"services/cerebras/run/#virtual-environment-setup","title":"Virtual Environment Setup","text":"<p>In general, our system is compatible with the documentation from Cerebras which should be followed. In this early phase, a few small tweaks are required:</p> <p>Use Cerebras ModelZoo 2.5.0 for compatibility to the Cerebras machine's installed software-sdk version For completness both the clone and checkout are included below:</p> <pre><code>git clone https://github.com/Cerebras/modelzoo.git ./modelzoo\ncd modelzoo\ngit checkout Release_2.5.0\n</code></pre>"},{"location":"services/cerebras/run/#running-codes","title":"Running codes","text":"<p>Run as per the normal Cerebras documentation. It is advisable to run codes inside a <code>tmux</code> session so you can return to them without having to leave SSH sessions active whilst jobs run.</p> <p>You may see some warnings about mount paths, e.g. <code>The following editable packages are not in a volume accessible to the cluster</code>, however these can be safely ignored.</p>"},{"location":"services/cerebras/run/#example-training-llama4b-on-a-single-cs3","title":"Example training Llama4b on a single CS3","text":"<p>With a suitably configured venv as above, and the modelzoo checked out:</p> <ul> <li>Navigate to <code>&lt;your modelzoo checkout&gt;/src/cerebras/modelzoo/models/nlp/llama/configs/</code> and copy the file <code>params_llama3p1_8b_msl_128k.yaml</code> to <code>params_example.yaml</code></li> <li>Adjust the copied <code>params_example.yaml</code> config to reduce the <code>max_steps</code> field to <code>50</code></li> <li>Adjust the copied <code>params_example.yaml</code> config to change the <code>data_dir</code> field to <code>/home/y26/shared/rpj_1t_100k_llama3p1_msl8k_train</code></li> <li>Navigate to <code>&lt;your modelzoo checkout&gt;/src/cerebras/modelzoo/models/nlp/llama/</code></li> <li>Run using <code>cszoo fit --num_csx=1 configs/params_example.yaml --mount_dirs /home/&lt;your_project&gt;/&lt;your_project&gt;/&lt;your_username&gt;/  /home/y26/shared/ --python_paths ~/modelzoo/src/ --model_dir llama4b_u3</code></li> </ul>"},{"location":"services/cerebras/run/#example-training-vision-transformer-on-imagenet-mini","title":"Example: Training Vision Transformer on ImageNet Mini","text":"<p>This tutorial will train a toy Visual transformer on a collection of captioned data, the produced model being able to input and image and output a caption.</p> <p>We make use of the imagenet-mini dataset,  a subset of 1000 samples from the ImageNet dataset.</p> <ol> <li>Set up your virtual environment as described above</li> <li> <p>Create a space in which the model will be stored</p> <pre><code>mkdir -p ~/imagenet_tutorial\ncd ~/imagenet_tutorial\n</code></pre> </li> <li> <p>Copy the training configuration:</p> <pre><code>cp /home/y26/shared/params_vit_imagenet.yaml ~/imagenet_tutorial\n</code></pre> </li> <li> <p>Run the training job:</p> <pre><code>cszoo fit params_vit_imagenet.yaml --num_csx=1 \\\n  --mount_dirs /home/y26/shared/ /home/&lt;your_project&gt;/&lt;your_project&gt;/&lt;your_username&gt;/ \\\n  --python_paths /home/&lt;your_project&gt;/&lt;your_project&gt;/&lt;your_username&gt;/modelzoo/src\n</code></pre> </li> </ol>"},{"location":"services/datacatalogue/","title":"EIDF Data Catalogue Information","text":"<p>Metadata information</p>"},{"location":"services/datacatalogue/metadata/","title":"EIDF Metadata Information","text":""},{"location":"services/datacatalogue/metadata/#what-is-fair","title":"What is FAIR?","text":"<p>FAIR stands for Findable, Accessible, Interoperable, and Reusable, and helps emphasise the best practices with publishing and sharing data (more details: FAIR Principles)</p>"},{"location":"services/datacatalogue/metadata/#what-is-metadata","title":"What is metadata?","text":"<p>Metadata is data about data, to help describe the dataset. Common metadata fields are things like the title of the dataset, who produced it, where it was generated (if relevant), when it was generated, and some key words describing it</p>"},{"location":"services/datacatalogue/metadata/#what-is-ckan","title":"What is CKAN?","text":"<p>CKAN is a metadata catalogue - i.e. it is a database for metadata rather than data. This will help with all aspects of FAIR:</p> <ul> <li>it will be a signposting portal for where the data actually resides</li> <li>it will ensure that at least metadata (even if not the data) is in a format which is easily retrievable via an identifier</li> <li>the metadata (and hopefully data) use terms from vocabularies that are widely recognised in the relevant field</li> <li>the metadata has lots of attributes to help others use it, and there are clear licence conditions where necessary</li> </ul>"},{"location":"services/datacatalogue/metadata/#what-metadata-will-we-need-to-provide","title":"What metadata will we need to provide?","text":"<ul> <li>the title of the dataset; if a short title is not particularly descriptive, then please add a longer, separate, description too.</li> <li>the name of the person who created the dataset</li> <li>if it has spatial relevance, the latitude and longitude of the location where the dataset was generated, if possible (e.g. if a sensor has collected data, then it should be straightforward to know the lat and long)</li> <li>the temporal period that the dataset covers</li> <li>it is important to standardise the licencing for all data and we will use Creative Commons 4.0 by default. If you want a different licence, please come and talk to us.</li> <li>If the dataset is from a third party, you must tell us the licence of that dataset</li> <li>As well as the theme you've picked for your WP directory, you can add other themes in the metadata file. For example, you might have decided your WP theme is geophysics, but a dataset is also related to geodesy. Again, please check that this term is in the FAST vocabulary.</li> <li>if there is likely to be more than 1 way that the data could be made available (e.g. netCDF and csv)</li> </ul>"},{"location":"services/datacatalogue/metadata/#why-do-i-need-to-use-a-controlled-vocabulary","title":"Why do I need to use a controlled vocabulary?","text":"<p>Using a standard vocabulary (such as the FAST Vocabulary)  has many benefits:</p> <ul> <li>the terms are managed by an external body</li> <li>the hierarchy has been agreed (e.g. you will see for that for geophysics, it has \"skos broader\" topics of \"physics\" and \"earth sciences\", which I hope you agree with! Don't worry what \"skos\" means)</li> <li>using controlled vocabularies means that everybody who uses it knows they are using the same definitions as everybody else using it</li> <li>the vocabulary is updated at given intervals</li> </ul> <p>All of these advantages mean that we, as a project, don't need to think about this - there is no need to reinvent the wheel when other institutes (e.g. National Libraries) have created. You might recognise WorldCat - it is an organisation which manages a global catalogue of ~18000 libraries world-wide, so they are in a good position to generate a comprehensive vocabulary of academic topics!</p>"},{"location":"services/datacatalogue/metadata/#what-about-licensing-what-does-cc-by-sa-40-mean","title":"What about licensing? (What does CC-BY-SA 4.0 mean?)","text":"<p>The R in FAIR stands for reusable - more specifically it includes this subphrase: \"(Meta)data are released with a clear and accessible data usage license\". This means that we have to tell anyone else who uses the data what they're allowed to do with it - and, under the FAIR philosophy, more freedom is better.</p> <p>CC-BY-SA 4.0 allows anyone to remix, adapt, and build upon your work (even for commercial purposes), as long as they credit you and license their new creations under the identical terms. It also explicitly includes Sui Generis Database Rights, giving rights to the curation of a database even if you don't have the rights to the items in a database (e.g. a Spotify playlist, even though you don't own the rights to each track).</p> <p>Human readable summary: Creative Commons 4.0 Human Readable Full legal code: Creative Commons 4.0 Legal Code</p>"},{"location":"services/datacatalogue/metadata/#im-stuck-how-do-i-get-help","title":"I'm stuck! How do I get help?","text":"<p>If you are an existing user of EIDF or other EPCC systems then please submit your queries via our EIDF Helpdesk otherwise send your query by email to eidf@epcc.ed.ac.uk.</p>"},{"location":"services/datapublishing/catalogue/","title":"Data Publishing","text":""},{"location":"services/datapublishing/catalogue/#customising-your-entry-in-the-eidf-data-catalogue","title":"Customising your entry in the EIDF Data Catalogue","text":"<p>When/if your project is approved and you are close to publishing your data, a CKAN organisation will be created in the EIDF Data Catalogue for you.</p> <p>You can login to the EIDF Data Catalogue using your SAFE credentials - there is a \"Log in\" link on the top right. Find your organisation and then you can customise it by clicking on the \"Manage\" button on the top-right, e.g. you can provide a more friendly name than the EIDF project number, you can provide a description for your organisation, provide a logo or image representing your organisation and associate metadata pairs to aid discovery. Customising your organisation will make it more attractive to those that may want to use your data and will also aid in discovery.</p>"},{"location":"services/datapublishing/catalogue/#creating-your-datasets","title":"Creating your dataset(s)","text":"<p>Do NOT use the CKAN interface to create Datasets</p> <p>The EIDF Portal creates these for you and associates S3 buckets with your data. You can provide additional metadata once the Dataset records are in CKAN. Please do not add datasets through the CKAN interface either. Contact us if would like anything removed.</p> <p>Once your project is approved go to your project in the EIDF portal at this link:</p> <ul> <li>https://projects.eidf.ac.uk/ingest/</li> </ul> <p>Select the project which you want to use to ingest data. The list of <code>Ingest Datasets</code> will be empty unless you have already created Datasets.</p> <p>Create a Dataset by pressing on the <code>New</code> button. You will need to provide the following minimal bits of information:</p> <ul> <li>Name: The name for your dataset.</li> <li>S3 Bucket name: this entry will automatically be populated from your dataset name to create your S3 bucket name. You can customise the name for yourself subject to the constraints specified below the text box by editing the link directly. Note though if you change the dataset name you will overwrite the S3 bucket name link if you have customised it. Your project id at the start you will not be able to change.</li> <li>Number of buckets: you may want to distribute your data over a number of S3 buckets if your dataset is big or structured into subsets.</li> <li>Description: a description of your dataset.</li> <li>Link: a link describing your group/contact information.</li> <li>Contact email: a contact email to answer queries about your data set (this is optional).</li> <li>License: the license under which you will distribute your data.</li> </ul> <p>An example of the form is given below.</p> <p></p> <p>Once you are happy with the content press on the <code>Create</code> button. This will be used to create the requested number of S3 buckets to which you will be able copy your data.</p> <p>You should now be able to click on a link to your dataset to see a copy of the information that you provided. You can supplement your Dataset entry in the EIDF catalogue with additional metadata once you have logged into the data catalogue using your SAFE credentials.</p>"},{"location":"services/datapublishing/catalogue/#data-upload","title":"Data upload","text":"<p>Having created an S3 bucket please consult the S3 tutorial section to get an overview of the commands you will require to upload your data to S3.</p> <p>Note that versioning is enabled for data publishing S3 buckets. Versioning preserves existing files that are overwritten or deleted and allows to retrieve and restore every version of every file in an S3 bucket.</p> <p>When you have added your data in S3, you can add the data S3 link to the catalogue dataset as a resource.</p>"},{"location":"services/datapublishing/catalogue/#metadata-format","title":"Metadata format","text":"<p>Metadata for resources in your dataset are added directly through the EIDF Data Catalogue.</p> <p>Make sure you're logged in to the EIDF Data Catalogue. Open the page of your dataset and click on \"Manage\" at the top right. Open the \"Resources\"  tab and press the button \"+ Add new resource\". Now you can fill in the form and describe your data as you wish. Some entries that are required and these are marked with a red \"*\" in the EIDF Data Catalogue:</p> <ul> <li>Name: a descriptive name for your dataset.</li> <li>Access URL: this is a link to a file in S3 or a set of files with a common prefix, that you uploaded as explained above.</li> <li>Description: a human readable description of your data set.</li> <li>Resource format: the type of data included in your resource.</li> <li>Unique Identifier</li> <li>Licence: the licence under which you are releasing your data.</li> </ul> <p>Note</p> <p>If it is not going to be immediately obvious to a third party as to how your data may be used then please do provide a link to some documentation showing people how to unpack/use your data. Not everyone who may want to use your data may be a domain expert in your field.</p>"},{"location":"services/datapublishing/service/","title":"Data Publishing","text":""},{"location":"services/datapublishing/service/#service-provision","title":"Service provision","text":"<p>The EIDF guarantees, to the best of its ability, to continue its services until at least the 31 July 2032 and aims to continue beyond 2032 subject to funding. However, should we have to terminate the service we will give you at least 3 months notice to retrieve your data. It is thus important to keep your contact details up to date. The publishing service is not an archiving service and we recommend where possible that you have a backup version of your data outside of the EIDF.</p> <p>We reserve the right to remove any data that is not legal or inappropriate as well as any data that remains at the end of service provision.</p> <p>Some basic assumptions -</p> <ul> <li>You already have a SAFE account and can access the   EIDF portal.   Otherwise consult the EIDF portal documentation before proceeding.</li> <li>To qualify for the free data publishing service your data must be open and freely available to all.   If you want to control access to your data you should use the S3 service instead. This is not a free service.</li> <li>There is no charge for this service if your data can be processed with free tools, distributed   and used freely without restriction, and fits within our fair usage policy.</li> </ul>"},{"location":"services/datapublishing/service/#applying-for-a-data-project","title":"Applying for a data project","text":"<p>To start the process you will need to apply for an EIDF data project which is slightly different from other EIDF project applications.</p> <p>In the EIDF portal:</p> <ul> <li>Press on the <code>Your project applications</code> link.</li> <li>Press on the <code>New Application</code> link and put in an application for us to host your data.<ul> <li>You will be asked to supply a title for your application.</li> <li>A start date (when you hope to start publishing your data).</li> <li>A proposed end date (at the moment you will not be able to go beyond 31-Jul-2032).</li> </ul> </li> </ul> <p>For the EIDF Services you require chose the \"EIDF Data Publishing\" choice. If you also require EIDF Compute Services you will have to put in a separate application.</p> <p>Be sure to describe the dataset(s) that you wish to ingest. Submit your application. Your application will be reviewed and you will be notified if your project has been approved or rejected - someone may be in touch to clarify points in your application.</p> <p>Once your data project has been approved we will create an organisation in our EIDF Data Catalogue. The Data Catalogue is a customised CKAN instance, an open source application for data management systems. We map EIDF projects to CKAN organisations. A CKAN organisation allows you to brand your organisation, allow you to provide metadata to aid the discovery of your data and to publish your datasets together with metadata specific to those data sets.</p>"},{"location":"services/experimental/","title":"Experimental System Access","text":""},{"location":"services/experimental/#introduction","title":"Introduction","text":"<p>A pair of redundant SSH Gateways were built to replace the aging Hydra-VPN Service. This service replicates the requirements of Hydra-VPn, while implementing much higher security standards, with greater ease of use.</p> <p>In order to access these gateways, you need to be a member of a project which uses them, for example a NextGenIO Project. If you're unsure if you need access, speak to your EPCC Project Contact.</p> <p>These gateways act as a jump host only, you can't SSH to them directly. See the Using the Gateway section for more information.</p>"},{"location":"services/experimental/#gaining-access","title":"Gaining Access","text":"<ol> <li>Access SAFE</li> <li>Select 'Projects' and 'Request Access'</li> <li>Search for your project code, e.g. nx04</li> <li>Select 'Request machine account' then 'Apply'</li> <li>Under the 'Select a machine for the login account' select 'gateways'</li> <li>Select 'Next'</li> <li>Specify the username you'd like. This can't be the same as an account name in another project</li> <li>Upload an SSH public key. This is required to access these gateways.</li> <li>Select 'Request'</li> <li>You'll get an email when your account has been accepted.</li> </ol>"},{"location":"services/experimental/#set-mfa-token","title":"Set MFA Token","text":"<p>Once your account has been added, you need to enable MFA. Like the SSH key, this is required to use these gateways.</p> <ol> <li>Access SAFE</li> <li>Select 'Login Accounts' and select the username you just made</li> <li>Select 'Set MFA-Token'</li> <li>Scan the QR Code into the authenticator of your choice, and verify the code displayed in your app in the 'Verification Code' box.</li> <li>MFA has now been enabled.</li> </ol>"},{"location":"services/experimental/#using-the-gateway","title":"Using the Gateway","text":"<p>See the EIDF-Gateway docs for instructions on how to use the gateways from Linux/MacOS or Windows.</p> <p>Substitute all mentions of <code>eidf-gateway.epcc.ed.ac.uk</code> to be <code>gateway.epcc.ed.ac.uk</code> in your config. These are two seperate services and you cannot access experimental services through <code>eidf-gateway.epcc.ed.ac.uk</code></p>"},{"location":"services/gitlab/","title":"EIDF GitLab Service","text":"<p>The EIDF GitLab Service is a GitLab deployment in the EIDF Data Science Cloud.</p> <p>The GitLab Service is open to all EIDF users and offers code storage, issue tracking and CI/CD pipelines.</p> <p>Follow Quickstart to start using the EIDF GitLab Service.</p>"},{"location":"services/gitlab/quickstart/","title":"Quickstart","text":""},{"location":"services/gitlab/quickstart/#accessing","title":"Accessing","text":"<p>Access the EIDF gitlab in your browser by opening https://gitlab.eidf.ac.uk/. You must be a member of an active EIDF project and have a user account to use the EIDF gitlab Service.</p> <p></p> <p>Click on \"Sign In with SAFE\". You will be redirected to the SAFE login page to if you're not logged in already.</p>"},{"location":"services/gitlab/quickstart/#create-new-project","title":"Create New Project","text":"<p>You will be presented with the list of GitLab Projects you currently have access to. Click on \"New Project\" to create a project.</p> <p></p> <p>Selecting \"Blank Project\" will take to you the project creation screen. You can choose to change the project url to make the new project either a personal one or one owned by a GitLab group.</p> <p></p> <p>GitLab offers a variety of options for project, permission and access configurations. Please consult the GitLab Documentation for specifics.</p>"},{"location":"services/gitlab/quickstart/#clone-repository-via-https","title":"Clone repository via HTTPS","text":"<p>To clone a repository via HTTPS, click on the \"Code\" button to get the https-based url.</p> <p></p> <p>From the command line, you can clone this repository by adding your username and token into the URL between the <code>https://</code> and <code>@gitlab.eidf</code> parts of the url as shown. Your EIDF GitLab username can easily be found by clicking your profile image near the top left of any GitLab page. The username is the part below your name, without the <code>@</code> symbol.</p> <p></p>"},{"location":"services/gitlab/quickstart/#clone-repository-via-ssh","title":"Clone repository via SSH","text":"<p>To clone a repository using git over SSH, click on the \"Code\" button to get the git+ssh URL. You will need to have added an SSH key to your account for this method to work.</p> <p>For a more complete set of documentation relating to adding and using SSH keys with Gitlab, see the upstream Gitlab Documentation</p>"},{"location":"services/gpuservice/","title":"Overview","text":"<p>The EIDF GPU Service (EIDF GPU Service) provides access to a range of Nvidia GPUs, in both full GPU and MIG variants. The EIDF GPU Service is built upon Kubernetes.</p> <p>MIG (Multi-instance GPU) allow a single GPU to be split into multiple isolated smaller GPUs. This means that multiple users can access a portion of the GPU without being able to access what others are running on their portion. The EIDF GPU Service hosts 3G.20GB and 1G.5GB MIG variants which are approximately 1/2 and 1/7 of a full Nvidia A100 40 GB GPU respectively.</p> <p>The current full specification of the EIDF GPU Service as of April 2025:</p> <ul> <li>9254 CPU Cores (AMD EPYC and Intel Xeon)</li> <li>59 TiB Memory</li> <li>Ceph Persistent Volumes CephFS - up to 100TiB</li> <li>Ceph Persistent Volumes RBD - up to 100TiB</li> <li>16 Nvidia H200 141GB</li> <li>136 Nvidia H100 80GB</li> <li>80 Nvidia A100 80GB</li> <li>56 Nvidia A100 40GB</li> <li>16 Nvidia MIG A100 3G.20GB</li> <li>56 Nvidia MIG A100 1G.5GB</li> </ul> <p>Quotas</p> <p>This is the full configuration of the cluster.</p> <p>Each project will have access to a quota across this shared configuration.</p> <p>Changes to the default quota must be discussed and agreed with the EIDF Services team.</p> <p>NOTE</p> <p>If you request a GPU on the EIDF GPU Service you will be assigned one at random unless you specify a GPU type. Please see Getting started with Kubernetes to learn about specifying GPU resources.</p>"},{"location":"services/gpuservice/#service-access","title":"Service Access","text":"<p>Users should have an EIDF Account as the EIDF GPU Service is only accessible through EIDF Virtual Machines.</p> <p>Existing projects can request access to the EIDF GPU Service through a service request to the EIDF helpdesk or emailing eidf@epcc.ed.ac.uk .</p> <p>New projects wanting to using the GPU Service should include this in their EIDF Project Application.</p> <p>Each project will be given a namespace within the EIDF GPU service to operate in.</p> <p>This namespace will normally be the EIDF Project code appended with \u2019ns\u2019, i.e. <code>eidf989ns</code> for a project with code 'eidf989'.</p> <p>Once access to the EIDF GPU service has been confirmed, Project Leads will be give the ability to add a kubeconfig file to any of the VMs in their EIDF project - information on access to VMs is available here.</p> <p>All EIDF VMs with the project kubeconfig file downloaded can access the EIDF GPU Service using the kubectl command line tool.</p> <p>The VM does not require to be GPU-enabled.</p> <p>A quick check to see if a VM has access to the EIDF GPU service can be completed by typing <code>kubectl -n &lt;project-namespace&gt; get jobs</code> in to the command line.</p> <p>If this is first time you have connected to the GPU service the response should be <code>No resources found in &lt;project-namespace&gt; namespace</code>.</p> <p>EIDF GPU Service vs EIDF GPU-Enabled VMs</p> <p>The EIDF GPU Service is a container based service which is accessed from EIDF Virtual Desktop VMs.</p> <p>This allows a project to access multiple GPUs of different types.</p> <p>An EIDF Virtual Desktop GPU-enabled VM is limited to a small number (1-2) of GPUs of a single type.</p> <p>Projects do not have to apply for a GPU-enabled VM to access the GPU Service.</p>"},{"location":"services/gpuservice/#project-quotas","title":"Project Quotas","text":"<p>A standard project namespace has the following initial quota (subject to ongoing review):</p> <ul> <li>CPU: 100 Cores</li> <li>Memory: 1TiB</li> <li>GPU: 12</li> </ul> <p>Quota is a maximum on a Shared Resource</p> <p>A project quota is the maximum proportion of the service available for use by that project.</p> <p>Any submitted job requests that would exceed the total project quota will be queued.</p>"},{"location":"services/gpuservice/#project-queues","title":"Project Queues","text":"<p>EIDF GPU Service has been using the Kueue system since February 2024. The use of this is detailed in the Kueue.</p> <p>Job Queuing</p> <p>During periods of high demand, jobs will be queued awaiting resource availability on the Service.</p> <p>As a general rule, the higher the GPU/CPU/Memory resource request of a single job the longer it will wait in the queue before enough resources are free on a single node for it be allocated.</p> <p>GPUs in high demand, such as Nvidia H100s, typically have longer wait times.</p> <p>Furthermore, a project may have a quota of up to 12 GPUs but due to demand may only be able to access a smaller number at any given time.</p>"},{"location":"services/gpuservice/#additional-service-policy-information","title":"Additional Service Policy Information","text":"<p>Additional information on service policies can be found here.</p>"},{"location":"services/gpuservice/#eidf-gpu-service-tutorial","title":"EIDF GPU Service Tutorial","text":"<p>This tutorial teaches users how to submit tasks to the EIDF GPU Service, but it is not a comprehensive overview of Kubernetes.</p> Lesson Objective Getting started with Kubernetes a. What is Kubernetes?b. How to send a task to a GPU node.c. How to define the GPU resources needed. Requesting persistent volumes with Kubernetes a. What is a persistent volume? b. How to request a PV resource. Running a PyTorch task a. Accessing a Pytorch container.b. Submitting a PyTorch task to the cluster.c. Inspecting the results. Template workflow a. Loading large data sets asynchronously.b. Manually or automatically building Docker images.c. Iteratively changing and testing code in a job."},{"location":"services/gpuservice/#further-reading-and-help","title":"Further Reading and Help","text":"<ul> <li> <p>The Nvidia developers blog provides several examples of how to run ML tasks on a Kubernetes GPU cluster.</p> </li> <li> <p>Kubernetes documentation has a useful kubectl cheat sheet.</p> </li> <li> <p>More detailed use cases for the <code>kubectl</code> can be found in the Kubernetes documentation.</p> </li> </ul>"},{"location":"services/gpuservice/faq/","title":"GPU Service FAQ","text":""},{"location":"services/gpuservice/faq/#gpu-service-frequently-asked-questions","title":"GPU Service Frequently Asked Questions","text":""},{"location":"services/gpuservice/faq/#how-do-i-access-the-gpu-service","title":"How do I access the GPU Service?","text":"<p>The default access route to the GPU Service is via an EIDF DSC VM. The DSC VM will have access to all EIDF resources for your project and can be accessed through the VDI (SSH or if enabled RDP) or via the EIDF SSH Gateway.</p>"},{"location":"services/gpuservice/faq/#how-do-i-obtain-my-project-kubeconfig-file","title":"How do I obtain my project kubeconfig file?","text":"<p>Project Leads and Managers can access the kubeconfig file from the Project page in the Portal. Project Leads and Managers can provide the file on any of the project VMs or give it to individuals within the project.</p>"},{"location":"services/gpuservice/faq/#access-to-gpu-service-resources-in-default-namespace-is-forbidden","title":"Access to GPU Service resources in default namespace is 'Forbidden'","text":"<pre><code>Error from server (Forbidden): error when creating \"myjobfile.yml\": jobs is forbidden: User &lt;user&gt; cannot create resource \"jobs\" in API group \"\" in the namespace \"default\"\n</code></pre> <p>Some version of the above error is common when submitting jobs/pods to the GPU cluster using the kubectl command. This arises when the project namespace is not included in the kubectl command for submitting job/pods and kubectl tries to use the \"default\" namespace which projects do not have permissions to use. Resubmitting the job/pod with <code>kubectl -n &lt;project-namespace&gt; create \"myjobfile.yml\"</code> should solve the issue.</p>"},{"location":"services/gpuservice/faq/#i-cant-mount-my-pvc-in-multiple-containers-or-pods-at-the-same-time","title":"I can't mount my PVC in multiple containers or pods at the same time","text":"<p>If a PVC can only be mounted by one pod at a time, the PVC provisioner used was Ceph RBD. The block devices provided by Ceph to the Kubernetes PV/PVC providers cannot be mounted in multiple pods at the same time. They can only be accessed by one pod at a time, once a pod has unmounted the PVC and terminated, the PVC can be reused by another pod.</p> <p>The service now provides a CephFS provisioner which allows for ReadWriteMany PVCs. To mount a PVC to multiple pods, please create a PVC using the <code>csi-cephfs-sc</code> storage class.</p>"},{"location":"services/gpuservice/faq/#how-many-gpus-can-i-use-in-a-pod","title":"How many GPUs can I use in a pod?","text":"<p>The current limit is 8 GPUs per pod. Each underlying host node has either 4 or 8 GPUs. If you request 8 GPUs, you will be placed in a queue until a node with 8 GPUs is free or other jobs to run. If you request 4 GPUs this could run on a node with 4 or 8 GPUs.</p>"},{"location":"services/gpuservice/faq/#why-did-a-validation-error-occur-when-submitting-a-pod-or-job-with-a-valid-specification-file","title":"Why did a validation error occur when submitting a pod or job with a valid specification file?","text":"<p>If an error like the below occurs:</p> <pre><code>error: error validating \"myjobfile.yml\": error validating data: the server does not allow access to the requested resource; if you choose to ignore these errors, turn validation off with --validate=false\n</code></pre> <p>There may be an issue with the kubectl version that is being run. This can occur if installing in virtual environments or from packages repositories.</p> <p>The current version verified to operate with the GPU Service is v1.24.10. kubectl and the Kubernetes API version can suffer from version skew if not with a defined number of releases. More information can be found on this under the Kubernetes Version Skew Policy.</p>"},{"location":"services/gpuservice/faq/#insufficient-shared-memory-size","title":"Insufficient Shared Memory Size","text":"<p>My SHM is very small, and it causes \"OSError: [Errno 28] No space left on device\" when I train a model using multi-GPU. How to increase SHM size?</p> <p>The default size of SHM is only 64M. You can mount an empty dir to /dev/shm to solve this problem:</p> <pre><code>   spec:\n     containers:\n       - name: [NAME]\n         image: [IMAGE]\n         volumeMounts:\n         - mountPath: /dev/shm\n           name: dshm\n     volumes:\n       - name: dshm\n         emptyDir:\n            medium: Memory\n</code></pre>"},{"location":"services/gpuservice/faq/#pytorch-slow-performance-issues","title":"Pytorch Slow Performance Issues","text":"<p>Pytorch on Kubernetes may operate slower than expected - much slower than an equivalent VM setup.</p> <p>Pytorch defaults to auto-detecting the number of OMP Threads and it will report an incorrect number of potential threads compared to your requested CPU core count. This is a consequence in operating in a container environment, the CPU information is reported by standard libraries and tools will be the node level information rather than your container.</p> <p>To help correct this issue, the environment variable OMP_NUM_THREADS should be set in the job submission file to the number of cores requested or less.</p> <p>This has been tested using:</p> <ul> <li>OMP_NUM_THREADS=1</li> <li>OMP_NUM_THREADS=(number of requested cores).</li> </ul> <p>Example fragment for a Bash command start:</p> <pre><code>  containers:\n    - args:\n        - &gt;\n          export OMP_NUM_THREADS=1;\n          python mypytorchprogram.py;\n      command:\n        - /bin/bash\n        - '-c'\n        - '--'\n</code></pre>"},{"location":"services/gpuservice/faq/#my-large-number-of-gpus-job-takes-a-long-time-to-be-scheduled","title":"My large number of GPUs Job takes a long time to be scheduled","text":"<p>When requesting a large number of GPUs for a job, this may require an entire node to be free. This could take some time to become available, the default scheduling algorithm in the queues in place is Best Effort FIFO - this means that large jobs will not block small jobs from running if there is sufficient quota and space available.</p>"},{"location":"services/gpuservice/kueue/","title":"Kueue","text":""},{"location":"services/gpuservice/kueue/#overview","title":"Overview","text":"<p>Kueue is a native Kubernetes quota and job management system.</p> <p>This is the job queue system for the EIDF GPU Service, starting with April 2025.</p> <p>All users should submit jobs to their local namespace user queue, this queue will have the name <code>eidf project namespace</code>-user-queue.</p>"},{"location":"services/gpuservice/kueue/#changes-to-job-specs","title":"Changes to Job Specs","text":"<p>Jobs can be submitted as before but will require the addition of a metadata label:</p> <pre><code>   labels:\n      kueue.x-k8s.io/queue-name:  &lt;project namespace&gt;-user-queue\n</code></pre> <p>This is the only change required to make Jobs Kueue functional. A policy will be in place that will stop jobs without this label being accepted.</p>"},{"location":"services/gpuservice/kueue/#useful-commands-for-looking-at-your-local-queue","title":"Useful commands for looking at your local queue","text":""},{"location":"services/gpuservice/kueue/#kubectl-get-queue","title":"<code>kubectl get queue</code>","text":"<p>This command will output the high level status of your namespace queue with the number of workloads currently running and the number waiting to start:</p> <pre><code>NAME               CLUSTERQUEUE             PENDING WORKLOADS   ADMITTED WORKLOADS\neidf001-user-queue eidf001-project-gpu-cq   0                   2\n</code></pre>"},{"location":"services/gpuservice/kueue/#kubectl-describe-queue-queue","title":"<code>kubectl describe queue &lt;queue&gt;</code>","text":"<p>This command will output more detailed information on the current resource usage in your queue:</p> <pre><code>Name:         eidf001-user-queue\nNamespace:    eidf001\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  kueue.x-k8s.io/v1beta1\nKind:         LocalQueue\nMetadata:\n  Creation Timestamp:  2024-02-06T13:06:23Z\n  Generation:          1\n  Managed Fields:\n    API Version:  kueue.x-k8s.io/v1beta1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:spec:\n        .:\n        f:clusterQueue:\n    Manager:      kubectl-create\n    Operation:    Update\n    Time:         2024-02-06T13:06:23Z\n    API Version:  kueue.x-k8s.io/v1beta1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        .:\n        f:admittedWorkloads:\n        f:conditions:\n          .:\n          k:{\"type\":\"Active\"}:\n            .:\n            f:lastTransitionTime:\n            f:message:\n            f:reason:\n            f:status:\n            f:type:\n        f:flavorUsage:\n          .:\n          k:{\"name\":\"default-flavor\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"cpu\"}:\n                .:\n                f:name:\n                f:total:\n              k:{\"name\":\"memory\"}:\n                .:\n                f:name:\n                f:total:\n          k:{\"name\":\"gpu-a100\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"nvidia.com/gpu\"}:\n                .:\n                f:name:\n                f:total:\n          k:{\"name\":\"gpu-a100-1g\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"nvidia.com/gpu\"}:\n                .:\n                f:name:\n                f:total:\n          k:{\"name\":\"gpu-a100-3g\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"nvidia.com/gpu\"}:\n                .:\n                f:name:\n                f:total:\n          k:{\"name\":\"gpu-a100-80\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"nvidia.com/gpu\"}:\n                .:\n                f:name:\n                f:total:\n        f:flavorsReservation:\n          .:\n          k:{\"name\":\"default-flavor\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"cpu\"}:\n                .:\n                f:name:\n                f:total:\n              k:{\"name\":\"memory\"}:\n                .:\n                f:name:\n                f:total:\n          k:{\"name\":\"gpu-a100\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"nvidia.com/gpu\"}:\n                .:\n                f:name:\n                f:total:\n          k:{\"name\":\"gpu-a100-1g\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"nvidia.com/gpu\"}:\n                .:\n                f:name:\n                f:total:\n          k:{\"name\":\"gpu-a100-3g\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"nvidia.com/gpu\"}:\n                .:\n                f:name:\n                f:total:\n          k:{\"name\":\"gpu-a100-80\"}:\n            .:\n            f:name:\n            f:resources:\n              .:\n              k:{\"name\":\"nvidia.com/gpu\"}:\n                .:\n                f:name:\n                f:total:\n        f:pendingWorkloads:\n        f:reservingWorkloads:\n    Manager:         kueue\n    Operation:       Update\n    Subresource:     status\n    Time:            2024-02-14T10:54:20Z\n  Resource Version:  333898946\n  UID:               bca097e2-6c55-4305-86ac-d1bd3c767751\nSpec:\n  Cluster Queue:  eidf001-project-gpu-cq\nStatus:\n  Admitted Workloads:  2\n  Conditions:\n    Last Transition Time:  2024-02-06T13:06:23Z\n    Message:               Can submit new workloads to clusterQueue\n    Reason:                Ready\n    Status:                True\n    Type:                  Active\n  Flavor Usage:\n    Name:  gpu-a100\n    Resources:\n      Name:   nvidia.com/gpu\n      Total:  2\n    Name:     gpu-a100-3g\n    Resources:\n      Name:   nvidia.com/gpu\n      Total:  0\n    Name:     gpu-a100-1g\n    Resources:\n      Name:   nvidia.com/gpu\n      Total:  0\n    Name:     gpu-a100-80\n    Resources:\n      Name:   nvidia.com/gpu\n      Total:  0\n    Name:     default-flavor\n    Resources:\n      Name:   cpu\n      Total:  16\n      Name:   memory\n      Total:  256Gi\n  Flavors Reservation:\n    Name:  gpu-a100\n    Resources:\n      Name:   nvidia.com/gpu\n      Total:  2\n    Name:     gpu-a100-3g\n    Resources:\n      Name:   nvidia.com/gpu\n      Total:  0\n    Name:     gpu-a100-1g\n    Resources:\n      Name:   nvidia.com/gpu\n      Total:  0\n    Name:     gpu-a100-80\n    Resources:\n      Name:   nvidia.com/gpu\n      Total:  0\n    Name:     default-flavor\n    Resources:\n      Name:             cpu\n      Total:            16\n      Name:             memory\n      Total:            256Gi\n  Pending Workloads:    0\n  Reserving Workloads:  2\nEvents:                 &lt;none&gt;\n</code></pre>"},{"location":"services/gpuservice/kueue/#kubectl-get-workloads","title":"<code>kubectl get workloads</code>","text":"<p>This command will return the list of workloads in the queue:</p> <pre><code>NAME                QUEUE                ADMITTED BY              AGE\njob-jobtest-366ab   eidf001-user-queue   eidf001-project-gpu-cq   4h45m\njob-jobtest-34ba9   eidf001-user-queue   eidf001-project-gpu-cq   6h48m\n</code></pre>"},{"location":"services/gpuservice/kueue/#kubectl-describe-workload-workload","title":"<code>kubectl describe workload &lt;workload&gt;</code>","text":"<p>This command will return a detailed summary of the workload including status and resource usage:</p> <pre><code>Name:         job-pytorch-job-0b664\nNamespace:    t4\nLabels:       kueue.x-k8s.io/job-uid=33bc1e48-4dca-4252-9387-bf68b99759dc\nAnnotations:  &lt;none&gt;\nAPI Version:  kueue.x-k8s.io/v1beta1\nKind:         Workload\nMetadata:\n  Creation Timestamp:  2024-02-14T15:22:16Z\n  Generation:          2\n  Managed Fields:\n    API Version:  kueue.x-k8s.io/v1beta1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        f:admission:\n          f:clusterQueue:\n          f:podSetAssignments:\n            k:{\"name\":\"main\"}:\n              .:\n              f:count:\n              f:flavors:\n                f:cpu:\n                f:memory:\n                f:nvidia.com/gpu:\n              f:name:\n              f:resourceUsage:\n                f:cpu:\n                f:memory:\n                f:nvidia.com/gpu:\n        f:conditions:\n          k:{\"type\":\"Admitted\"}:\n            .:\n            f:lastTransitionTime:\n            f:message:\n            f:reason:\n            f:status:\n            f:type:\n          k:{\"type\":\"QuotaReserved\"}:\n            .:\n            f:lastTransitionTime:\n            f:message:\n            f:reason:\n            f:status:\n            f:type:\n    Manager:      kueue-admission\n    Operation:    Apply\n    Subresource:  status\n    Time:         2024-02-14T15:22:16Z\n    API Version:  kueue.x-k8s.io/v1beta1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        f:conditions:\n          k:{\"type\":\"Finished\"}:\n            .:\n            f:lastTransitionTime:\n            f:message:\n            f:reason:\n            f:status:\n            f:type:\n    Manager:      kueue-job-controller-Finished\n    Operation:    Apply\n    Subresource:  status\n    Time:         2024-02-14T15:25:06Z\n    API Version:  kueue.x-k8s.io/v1beta1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:labels:\n          .:\n          f:kueue.x-k8s.io/job-uid:\n        f:ownerReferences:\n          .:\n          k:{\"uid\":\"33bc1e48-4dca-4252-9387-bf68b99759dc\"}:\n      f:spec:\n        .:\n        f:podSets:\n          .:\n          k:{\"name\":\"main\"}:\n            .:\n            f:count:\n            f:name:\n            f:template:\n              .:\n              f:metadata:\n                .:\n                f:labels:\n                  .:\n                  f:controller-uid:\n                  f:job-name:\n                f:name:\n              f:spec:\n                .:\n                f:containers:\n                f:dnsPolicy:\n                f:nodeSelector:\n                f:restartPolicy:\n                f:schedulerName:\n                f:securityContext:\n                f:terminationGracePeriodSeconds:\n                f:volumes:\n        f:priority:\n        f:priorityClassSource:\n        f:queueName:\n    Manager:    kueue\n    Operation:  Update\n    Time:       2024-02-14T15:22:16Z\n  Owner References:\n    API Version:           batch/v1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  Job\n    Name:                  pytorch-job\n    UID:                   33bc1e48-4dca-4252-9387-bf68b99759dc\n  Resource Version:        270812029\n  UID:                     8cfa93ba-1142-4728-bc0c-e8de817e8151\nSpec:\n  Pod Sets:\n    Count:  1\n    Name:   main\n    Template:\n      Metadata:\n        Labels:\n          Controller - UID:  33bc1e48-4dca-4252-9387-bf68b99759dc\n          Job - Name:        pytorch-job\n        Name:                pytorch-pod\n      Spec:\n        Containers:\n          Args:\n            /mnt/ceph_rbd/example_pytorch_code.py\n          Command:\n            python3\n          Image:              pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\n          Image Pull Policy:  IfNotPresent\n          Name:               pytorch-con\n          Resources:\n            Limits:\n              Cpu:             4\n              Memory:          4Gi\n              nvidia.com/gpu:  1\n            Requests:\n              Cpu:                     2\n              Memory:                  1Gi\n          Termination Message Path:    /dev/termination-log\n          Termination Message Policy:  File\n          Volume Mounts:\n            Mount Path:  /mnt/ceph_rbd\n            Name:        volume\n        Dns Policy:      ClusterFirst\n        Node Selector:\n          nvidia.com/gpu.product:  NVIDIA-A100-SXM4-40GB\n        Restart Policy:            Never\n        Scheduler Name:            default-scheduler\n        Security Context:\n        Termination Grace Period Seconds:  30\n        Volumes:\n          Name:  volume\n          Persistent Volume Claim:\n            Claim Name:   pytorch-pvc\n  Priority:               0\n  Priority Class Source:\n  Queue Name:             t4-user-queue\nStatus:\n  Admission:\n    Cluster Queue:  project-cq\n    Pod Set Assignments:\n      Count:  1\n      Flavors:\n        Cpu:             default-flavor\n        Memory:          default-flavor\n        nvidia.com/gpu:  gpu-a100\n      Name:              main\n      Resource Usage:\n        Cpu:             2\n        Memory:          1Gi\n        nvidia.com/gpu:  1\n  Conditions:\n    Last Transition Time:  2024-02-14T15:22:16Z\n    Message:               Quota reserved in ClusterQueue project-cq\n    Reason:                QuotaReserved\n    Status:                True\n    Type:                  QuotaReserved\n    Last Transition Time:  2024-02-14T15:22:16Z\n    Message:               The workload is admitted\n    Reason:                Admitted\n    Status:                True\n    Type:                  Admitted\n    Last Transition Time:  2024-02-14T15:25:06Z\n    Message:               Job finished successfully\n    Reason:                JobFinished\n    Status:                True\n    Type:                  Finished\n</code></pre>"},{"location":"services/gpuservice/policies/","title":"GPU Service Policies","text":""},{"location":"services/gpuservice/policies/#namespaces","title":"Namespaces","text":"<p>Each project will be given a namespace which will have an applied quota.</p> <p>Default Quota:</p> <ul> <li>CPU: 100 Cores</li> <li>Memory: 1TiB</li> <li>GPU: 12</li> </ul>"},{"location":"services/gpuservice/policies/#kubeconfig","title":"Kubeconfig","text":"<p>Each project will be assigned a kubeconfig file for access to the service which will allow operation in the assigned namespace and access to exposed service operators, for example the GPU and CephRBD operators.</p>"},{"location":"services/gpuservice/policies/#kubernetes-job-names","title":"Kubernetes Job Names","text":"<p>All Kubernetes Jobs submitted will need to use the <code>metadata.generateName</code> field instead of the <code>metadata.name</code> field. This is to ensure jobs can be identified for purporses of  maintenance and troubleshooting.</p> <p>Submitting jobs with <code>name</code> only would allow several jobs to have the same name, potentially blocking you from submitting the job until the previous one was deleted. Support would have difficulties troubleshooting as the name remains the same, but execution results can be different each time.</p> <p>Important</p> <p>This policy is automated, but users will need to change their job template to use the new field for the submission to work.</p>"},{"location":"services/gpuservice/policies/#kubernetes-job-time-to-live","title":"Kubernetes Job Time to Live","text":"<p>All Kubernetes Jobs submitted to the service will have a Time to Live (TTL) applied via <code>spec.ttlSecondsAfterFinished</code> automatically. The default TTL for jobs using the service will be 1 week (604800 seconds). A completed job (in success or error state) will be deleted from the service once one week has elapsed after execution has completed. This will reduce excessive object accumulation on the service.</p> <p>Important</p> <p>This policy is automated and does not require users to change their job specifications.</p> <p>Important</p> <p>We recommend setting a lower value, unless you absolutely need the job to remain for debugging. Completed jobs serve no other purpose and can potentially make identifying your workloads more difficult.</p>"},{"location":"services/gpuservice/policies/#kubernetes-active-deadline-seconds","title":"Kubernetes Active Deadline Seconds","text":"<p>All Kubernetes User Pods submitted to the service will have an Active Deadline Seconds (ADS) applied via <code>spec.spec.activeDeadlineSeconds</code> automatically. The default ADS for pods using the service will be 5 days (432000 seconds). A pod will be terminated 5 days after execution has begun. This will reduce the number of unused pods remaining on the service.</p> <p>Important</p> <p>This policy is automated and does not require users to change their job or pod specifications.</p> <p>Important</p> <p>The preference would be, that you lower this number unless you are confident you need the workload to run for the maximum duration. Any configuration errors in your code can lead to the container running for the whole duration, but not yielding a result and taking cluster resources away from other users.</p>"},{"location":"services/gpuservice/policies/#kueue","title":"Kueue","text":"<p>All jobs will be managed through the Kueue scheduling system. All pods will be required to be owned by a Kubernetes workload.</p> <p>Each project will have a local user queue in their namespace. This will provide access to their cluster queue. To enable the use of the queue in your job definitions, the following will need to be added to the job specification file as part of the metadata:</p> <pre><code>   labels:\n      kueue.x-k8s.io/queue-name:  &lt;project namespace&gt;-user-queue\n</code></pre> <p>Workloads without this queue name tag will be rejected.</p>"},{"location":"services/gpuservice/shared-cephfs/","title":"Shared Filesystem (CephFS) Persistent Volume Claims (PVCs)","text":"<p>The Shared Filesystem (CephFS) allows multiple EIDF services to access the same file system for easier sharing of data and code.</p> <p>For more information on the Shared Filesystem - please refer to EIDF Storage Overview - Shared Filesystem (CephFS).</p> <p>For how to use the Shared Filesystem on Virtual Machines - please refer to EIDF Shared Filesystem (CephFS).</p>"},{"location":"services/gpuservice/shared-cephfs/#accessing-the-shared-filesystem-cephfs-on-the-gpu-service","title":"Accessing the Shared Filesystem (CephFS) on the GPU Service","text":"<p>To access directories in the shared filesystem, a project will need to create static persistent volumes which point to the Shared Filesystem (CephFS). Creation of these static persistent volumes is done using the EIDF Portal.</p> <p>Portal Creation Operation Only</p> <p>Users cannot create static volumes directly on the service - all static volumes must be created via the EIDF Portal</p>"},{"location":"services/gpuservice/shared-cephfs/#pre-requisites","title":"Pre-requisites","text":"<p>To mount CephFS in your project namespace:</p> <ol> <li>The project must have space allocated on CephFS.</li> <li>A mount key must exist. PI's should contact the EIDF helpdesk to have this created.</li> </ol> <p>Note</p> <p>It is useful to have the Shared Filesystem (CephFS) space mounted to at least one VM, for managing what directories are created and could be shared as static persistent volumes.</p> <p>If the pre-requisites are met, PIs and PMs will be able to see the GPU Service PVC button under CephFS Mounts in the project management page in the EIDF portal.</p> <p> GPU Service PVC button displayed in project management page</p>"},{"location":"services/gpuservice/shared-cephfs/#creating-a-static-persistent-volume","title":"Creating a Static Persistent Volume","text":"<p>To create a static persistent volume, click the GPU Service PVC button, the following form will appear:</p> <p> GPU Service PVC blank form displayed in portal</p> <p>The two parts that the PI or PM will need to complete are:</p> <ul> <li>Volume Root - this is how much of the shared directory is to be made visible to the GPU Service. If it is left blank, everything in the shared folder for the project will be visible. If there is a specific directory to be made visible, enter the path after shared/.</li> <li>PVC Name - this is mandatory - this is how project users will be able to use the static persistent volume in their workloads.</li> </ul> <p>ReadWriteMany</p> <p>These CephFS static persistent volumes are ReadWriteMany - this means that only one PVC is needed per volume root exposed in a project.</p> <p>The following image shows an example of a completed static persistent volume form:</p> <p> GPU Service PVC Example form displayed in portal</p> <p>The filled in fields are as follows:</p> <ul> <li>Volume Root - a directory called gpu-service in the shared space will be the exposed path in the volume. This means the volume will only show the contents of that directory.</li> <li>PVC Name - a PVC called example-pvc will now be available for the project to use in its workloads.</li> </ul>"},{"location":"services/gpuservice/shared-cephfs/#using-a-static-persistent-volume","title":"Using a Static Persistent Volume","text":"<p>Using a static PVC is the same as using a PVC created using <code>kubectl</code> with the standard storage class. Below is an example job to mount the PVC created in the last section. To adapt this to your project, update the user queue name and use your PVC name.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  generateName: test-ceph-pvc-job-\n  labels:\n    kueue.x-k8s.io/queue-name: eidf018ns-user-queue\nspec:\n  completions: 1\n  backoffLimit: 1\n  ttlSecondsAfterFinished: 1800\n  template:\n    metadata:\n      name: test-ceph-pvc-pod\n    spec:\n      containers:\n      - name: pvcreader\n        image: busybox\n        args: [\"sleep\", \"infinity\"]\n        resources:\n          requests:\n            cpu: 2\n            memory: '1Gi'\n          limits:\n            cpu: 2\n            memory: '4Gi'\n        volumeMounts:\n          - mountPath: /mnt/static\n            name: volume\n      restartPolicy: Never\n      volumes:\n        - name: volume\n          persistentVolumeClaim:\n            claimName: example-pvc\n</code></pre> <p>Users can work with the contents of the Shared Filesystem (CephFS) volume using a VM with the project Shared Filesystem (CephFS) mounted to it. More information on this is available at EIDF Shared Filesystem (CephFS).</p>"},{"location":"services/gpuservice/training/L1_getting_started/","title":"Getting started with Kubernetes","text":""},{"location":"services/gpuservice/training/L1_getting_started/#requirements","title":"Requirements","text":"<p>In order to follow this tutorial on the EIDF GPU Cluster you will need to have:</p> <ul> <li> <p>An account on the EIDF Portal.</p> </li> <li> <p>An active EIDF Project on the Portal with access to the EIDF GPU Service.</p> </li> <li> <p>The EIDF GPU Service kubernetes namespace associated with the project, e.g. eidf001ns.</p> </li> <li> <p>The EIDF GPU Service queue name associated with the project, e.g. eidf001ns-user-queue.</p> </li> <li> <p>Downloaded the kubeconfig file to a Project VM along with the kubectl command line tool to interact with the K8s API.</p> </li> </ul> <p>Downloading the kubeconfig file and kubectl</p> <p>Project Leads should use the 'Download kubeconfig' button on the EIDF Portal to complete this step to ensure the correct kubeconfig file and kubectl version is installed.</p>"},{"location":"services/gpuservice/training/L1_getting_started/#introduction","title":"Introduction","text":"<p>Kubernetes (K8s) is a container orchestration system, originally developed by Google, for the deployment, scaling, and management of containerised applications.</p> <p>Nvidia GPUs are supported through K8s native Nvidia GPU Operators.</p> <p>The use of K8s to manage the EIDF GPU Service provides two key advantages:</p> <ul> <li>support for containers enabling reproducible analysis whilst minimising demand on system admin.</li> <li>automated resource allocation management for GPUs and storage volumes that are shared across multiple users.</li> </ul>"},{"location":"services/gpuservice/training/L1_getting_started/#interacting-with-a-k8s-cluster","title":"Interacting with a K8s cluster","text":"<p>An overview of the key components of a K8s container can be seen on the Kubernetes docs website.</p> <p>The primary component of a K8s cluster is a pod.</p> <p>A pod is a set of one or more docker containers (and their storage volumes) that share resources.</p> <p>It is the EIDF GPU Cluster policy that all pods should be wrapped within a K8s job.</p> <p>This allows GPU/CPU/Memory resource requests to be managed by the cluster queue management system, kueue.</p> <p>Pods which attempt to bypass the queue mechanism will affect the experience of other project users.</p> <p>Any pods not associated with a job (or other K8s object) are at risk of being deleted without notice.</p> <p>K8s jobs also provide additional functionality such as parallelism (described later in this tutorial).</p> <p>Users define the resource requirements of a pod (i.e. number/type of GPU) and the containers/code to be ran in the pod by defining a template within a job manifest file written in yaml.</p> <p>The job yaml file is sent to the cluster using the K8s API and is assigned to an appropriate node to be ran.</p> <p>A node is a part of the cluster such as a physical or virtual host which exposes CPU, Memory and GPUs.</p> <p>Users interact with the K8s API using the <code>kubectl</code> (short for kubernetes control) commands.</p> <p>Some of the kubectl commands are restricted on the EIDF cluster in order to ensure project details are not shared across namespaces.</p> <p>Ensure kubectl is interacting with your project namespace.</p> <p>You will need to pass the name of your project namespace to <code>kubectl</code> in order for it to have permission to interact with the cluster.</p> <p><code>kubectl</code> will attempt to interact with the <code>default</code> namespace which will return a permissions error if it is not told otherwise.</p> <p><code>kubectl -n &lt;project-namespace&gt; &lt;command&gt;</code> will tell kubectl to pass the commands to the correct namespace.</p> <p>Useful commands are:</p> <ul> <li><code>kubectl -n &lt;project-namespace&gt; create -f &lt;job definition yaml&gt;</code>: Create a new job with requested resources. Returns an error if a job with the same name already exists.</li> <li><code>kubectl -n &lt;project-namespace&gt; apply -f &lt;job definition yaml&gt;</code>: Create a new job with requested resources. If a job with the same name already exists it updates that job with the new resource/container requirements outlined in the yaml.</li> <li><code>kubectl -n &lt;project-namespace&gt; delete pod &lt;pod name&gt;</code>: Delete a pod from the cluster.</li> <li><code>kubectl -n &lt;project-namespace&gt; get pods</code>: Summarise all pods the namespace has active (or pending).</li> <li><code>kubectl -n &lt;project-namespace&gt; describe pods</code>: Verbose description of all pods the namespace has active (or pending).</li> <li><code>kubectl -n &lt;project-namespace&gt; describe pod &lt;pod name&gt;</code>: Verbose summary of the specified pod.</li> <li><code>kubectl -n &lt;project-namespace&gt; logs &lt;pod name&gt;</code>: Retrieve the log files associated with a running pod.</li> <li><code>kubectl -n &lt;project-namespace&gt; get jobs</code>:  List all jobs the namespace has active (or pending).</li> <li><code>kubectl -n &lt;project-namespace&gt; describe job &lt;job name&gt;</code>: Verbose summary of the specified job.</li> <li><code>kubectl -n &lt;project-namespace&gt; delete job &lt;job name&gt;</code>: Delete a job from the cluster.</li> </ul>"},{"location":"services/gpuservice/training/L1_getting_started/#creating-your-first-pod-template-within-a-job-yaml-file","title":"Creating your first pod template within a job yaml file","text":"<p>To access the GPUs on the service, it is recommended to start with one of the prebuilt container images provided by Nvidia, these images are intended to perform different tasks using Nvidia GPUs.</p> <p>The list of Nvidia images is available on their website.</p> <p>The following example uses their CUDA sample code simulating nbody interactions.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n generateName: jobtest-\n labels:\n  kueue.x-k8s.io/queue-name:  &lt;project-namespace&gt;-user-queue\nspec:\n completions: 1\n backoffLimit: 1\n ttlSecondsAfterFinished: 1800\n template:\n  metadata:\n   name: job-test\n  spec:\n   containers:\n   - name: cudasample\n     image: nvcr.io/nvidia/k8s/cuda-sample:nbody-cuda11.7.1\n     args: [\"-benchmark\", \"-numbodies=512000\", \"-fp64\", \"-fullscreen\"]\n     resources:\n      requests:\n       cpu: 2\n       memory: '1Gi'\n      limits:\n       cpu: 2\n       memory: '4Gi'\n       nvidia.com/gpu: 1\n   restartPolicy: Never\n</code></pre> <p>The pod resources are defined under the <code>resources</code> tags using the <code>requests</code> and <code>limits</code> tags.</p> <p>Resources defined under the <code>requests</code> tags are the reserved resources required for the pod to be scheduled.</p> <p>If a pod is assigned to a node with unused resources then it may burst up to use resources beyond those requested.</p> <p>This may allow the task within the pod to run faster, but it will also throttle back down when further pods are scheduled to the node.</p> <p>The <code>limits</code> tag specifies the maximum resources that can be assigned to a pod.</p> <p>The EIDF GPU Service requires all pods have <code>requests</code> and <code>limits</code> tags for CPU and memory defined in order to be accepted.</p> <p>GPU resources requests are optional and only an entry under the <code>limits</code> tag is needed to specify the use of a GPU, <code>nvidia.com/gpu: 1</code>. Without this no GPU will be available to the pod.</p> <p>The label <code>kueue.x-k8s.io/queue-name</code> specifies the queue you are submitting your job to. This is part of the Kueue system in operation on the service to allow for improved resource management for users.</p>"},{"location":"services/gpuservice/training/L1_getting_started/#submitting-your-first-job","title":"Submitting your first job","text":"<ol> <li>Open an editor of your choice and create the file test_NBody.yml</li> <li>Copy the above job yaml in to the file, filling in <code>&lt;project-namespace&gt;-user-queue</code>, e.g. eidf001ns-user-queue:</li> <li>Save the file and exit the editor</li> <li>Run <code>kubectl -n &lt;project-namespace&gt; create -f test_NBody.yml</code></li> <li> <p>This will output something like:</p> <pre><code>job.batch/jobtest-b92qg created\n</code></pre> <p>The five character code appended to the job name, i.e. <code>b92qg</code>, is randomly generated and will differ from your run.</p> </li> <li> <p>Run <code>kubectl -n &lt;project-namespace&gt; get jobs</code></p> </li> <li> <p>This will output something like:</p> <pre><code>NAME            COMPLETIONS   DURATION   AGE\njobtest-b92qg   1/1           48s        29m\n</code></pre> <p>There may be more than one entry as this displays all the jobs in the current namespace, starting with their name, number of completions against required completions, duration and age.</p> </li> <li> <p>Inspect your job further using the command <code>kubectl -n &lt;project-namespace&gt; describe job jobtest-b92qg</code>, updating the job name with your five character code.</p> </li> <li> <p>This will output something like:</p> <pre><code>Name:             jobtest-b92qg\nNamespace:        t4\nSelector:         controller-uid=d3233fee-794e-466f-9655-1fe32d1f06d3\nLabels:           kueue.x-k8s.io/queue-name=t4-user-queue\nAnnotations:      batch.kubernetes.io/job-tracking:\nParallelism:      1\nCompletions:      3\nCompletion Mode:  NonIndexed\nStart Time:       Wed, 14 Feb 2024 14:07:44 +0000\nCompleted At:     Wed, 14 Feb 2024 14:08:32 +0000\nDuration:         48s\nPods Statuses:    0 Active (0 Ready) / 3 Succeeded / 0 Failed\nPod Template:\n    Labels:  controller-uid=d3233fee-794e-466f-9655-1fe32d1f06d3\n            job-name=jobtest-b92qg\n    Containers:\n        cudasample:\n            Image:      nvcr.io/nvidia/k8s/cuda-sample:nbody-cuda11.7.1\n            Port:       &lt;none&gt;\n            Host Port:  &lt;none&gt;\n            Args:\n                -benchmark\n                -numbodies=512000\n                -fp64\n                -fullscreen\n            Limits:\n                cpu:             2\n                memory:          4Gi\n                nvidia.com/gpu:  1\n            Requests:\n                cpu:        2\n                memory:     1Gi\n            Environment:  &lt;none&gt;\n            Mounts:       &lt;none&gt;\n    Volumes:        &lt;none&gt;\nEvents:\nType    Reason            Age    From                        Message\n----    ------            ----   ----                        -------\nNormal  Suspended         8m1s   job-controller              Job suspended\nNormal  CreatedWorkload   8m1s   batch/job-kueue-controller  Created Workload: t4/job-jobtest-b92qg-3b890\nNormal  Started           8m1s   batch/job-kueue-controller  Admitted by clusterQueue project-cq\nNormal  SuccessfulCreate  8m     job-controller              Created pod: jobtest-b92qg-lh64s\nNormal  Resumed           8m     job-controller              Job resumed\nNormal  SuccessfulCreate  7m44s  job-controller              Created pod: jobtest-b92qg-xhvdm\nNormal  SuccessfulCreate  7m28s  job-controller              Created pod: jobtest-b92qg-lvmrf\nNormal  Completed         7m12s  job-controller              Job completed\n</code></pre> </li> <li> <p>Run <code>kubectl -n &lt;project-namespace&gt; get pods</code></p> </li> <li> <p>This will output something like:</p> <pre><code>NAME                  READY   STATUS      RESTARTS   AGE\njobtest-b92qg-lh64s   0/1     Completed   0          11m\n</code></pre> <p>Again, there may be more than one entry as this displays all the jobs in the current namespace. Also, each pod within a job is given another unique 5 character code appended to the job name.</p> </li> <li> <p>View the logs of a pod from the job you ran <code>kubectl -n &lt;project-namespace&gt; logs jobtest-b92qg-lh64s</code> - again update with you run's pod and job five letter code.</p> </li> <li> <p>This will output something like:</p> <pre><code>Run \"nbody -benchmark [-numbodies=&lt;numBodies&gt;]\" to measure performance.\n    -fullscreen       (run n-body simulation in fullscreen mode)\n    -fp64             (use double precision floating point values for simulation)\n    -hostmem          (stores simulation data in host memory)\n    -benchmark        (run benchmark to measure performance)\n    -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)\n    -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)\n    -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)\n    -compare          (compares simulation results running once on the default GPU and once on the CPU)\n    -cpu              (run n-body simulation on the CPU)\n    -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)\n\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n\n&gt; Fullscreen mode\n&gt; Simulation data stored in video memory\n&gt; Double precision floating point simulation\n&gt; 1 Devices used for simulation\nGPU Device 0: \"Ampere\" with compute capability 8.0\n\n&gt; Compute 8.0 CUDA device: [NVIDIA A100-SXM4-40GB]\nnumber of bodies = 512000\n512000 bodies, total time for 10 iterations: 10570.778 ms\n= 247.989 billion interactions per second\n= 7439.679 double-precision GFLOP/s at 30 flops per interaction\n</code></pre> </li> <li> <p>Delete your job with <code>kubectl -n &lt;project-namespace&gt; delete job jobtest-b92qg</code> - this will delete the associated pods as well.</p> </li> </ol>"},{"location":"services/gpuservice/training/L1_getting_started/#specifying-gpu-requirements","title":"Specifying GPU requirements","text":"<p>If you create multiple jobs with the same definition file and compare their log files you may notice the CUDA device may differ from <code>Compute 8.0 CUDA device: [NVIDIA A100-SXM4-40GB]</code>.</p> <p>The GPU Operator on K8s is allocating the pod to the first node with a GPU free that matches the other resource specifications irrespective of the type of GPU present on the node.</p> <p>The GPU resource requests can be made more specific by adding the type of GPU product the pod template is requesting to the node selector:</p> <ul> <li><code>nvidia.com/gpu.product: 'NVIDIA-A100-SXM4-80GB'</code></li> <li><code>nvidia.com/gpu.product: 'NVIDIA-A100-SXM4-40GB'</code></li> <li><code>nvidia.com/gpu.product: 'NVIDIA-A100-SXM4-40GB-MIG-3g.20gb'</code></li> <li><code>nvidia.com/gpu.product: 'NVIDIA-A100-SXM4-40GB-MIG-1g.5gb'</code></li> <li><code>nvidia.com/gpu.product: 'NVIDIA-H100-80GB-HBM3'</code></li> <li><code>nvidia.com/gpu.product: 'NVIDIA-H200'</code></li> </ul>"},{"location":"services/gpuservice/training/L1_getting_started/#example-yaml-file-with-gpu-type-specified","title":"Example yaml file with GPU type specified","text":"<p>The <code>nodeSelector:</code> key at the bottom of the pod template states the pod should be ran on a node with a 1g.5gb MIG GPU.</p> <p>Exact GPU product names only</p> <p>K8s will fail to assign the pod if you misspell the GPU type.</p> <p>Be especially careful when requesting a full 80Gb or 40Gb A100 GPU as attempting to load GPUs with more data than its memory can handle can have unexpected consequences.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  generateName: jobtest-\n  labels:\n    kueue.x-k8s.io/queue-name:  &lt;project-namespace&gt;-user-queue\nspec:\n  completions: 1\n  backoffLimit: 1\n  ttlSecondsAfterFinished: 1800\n  template:\n    metadata:\n      name: job-test\n    spec:\n      containers:\n      - name: cudasample\n        image: nvcr.io/nvidia/k8s/cuda-sample:nbody-cuda11.7.1\n        args: [\"-benchmark\", \"-numbodies=512000\", \"-fp64\", \"-fullscreen\"]\n        resources:\n          requests:\n            cpu: 2\n            memory: '1Gi'\n          limits:\n            cpu: 2\n            memory: '4Gi'\n            nvidia.com/gpu: 1\n      restartPolicy: Never\n      nodeSelector:\n        nvidia.com/gpu.product: NVIDIA-A100-SXM4-40GB-MIG-1g.5gb\n</code></pre>"},{"location":"services/gpuservice/training/L1_getting_started/#running-multiple-pods-with-k8s-jobs","title":"Running multiple pods with K8s jobs","text":"<p>Wrapping a pod within a job provides additional functionality on top of accessing the queuing system.</p> <p>Firstly, the restartPolicy within a job enables the self-healing mechanism within K8s so that if a node dies with the job's pod on it then the job will find a new node to automatically restart the pod.</p> <p>Jobs also allow users to define multiple pods that can run in parallel or series and will continue to spawn pods until a specific number of pods successfully terminate.</p> <p>See below for an example K8s job that requires three pods to successfully complete the example CUDA code before the job itself ends.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n generateName: jobtest-\n labels:\n    kueue.x-k8s.io/queue-name:  &lt;project-namespace&gt;-user-queue\nspec:\n completions: 1\n backoffLimit: 1\n ttlSecondsAfterFinished: 1800\n parallelism: 1\n template:\n  metadata:\n   name: job-test\n  spec:\n   containers:\n   - name: cudasample\n     image: nvcr.io/nvidia/k8s/cuda-sample:nbody-cuda11.7.1\n     args: [\"-benchmark\", \"-numbodies=512000\", \"-fp64\", \"-fullscreen\"]\n     resources:\n      requests:\n       cpu: 2\n       memory: '1Gi'\n      limits:\n       cpu: 2\n       memory: '4Gi'\n       nvidia.com/gpu: 1\n   restartPolicy: Never\n</code></pre>"},{"location":"services/gpuservice/training/L1_getting_started/#change-the-default-kubectl-namespace-in-the-project-kubeconfig-file","title":"Change the default kubectl namespace in the project kubeconfig file","text":"<p>Passing the <code>-n &lt;project-namespace&gt;</code> flag every time you want to interact with the cluster can be cumbersome.</p> <p>You can alter the kubeconfig on your VM to send commands to your project namespace by default.</p> <p>Only users with sudo privileges can change the root kubectl config file.</p> <ol> <li> <p>Open the command line on your EIDF VM with access to the EIDF GPU Service.</p> </li> <li> <p>Open the root kubeconfig file with sudo privileges.</p> <pre><code>sudo nano /kubernetes/config\n</code></pre> </li> <li> <p>Add the namespace line with your project's kubernetes namespace to the \"eidf-general-prod\" context entry in your copy of the config file.</p> <pre><code>*** MORE CONFIG ***\n\ncontexts:\n- name: \"eidf-general-prod\"\n  context:\n    user: \"eidf-general-prod\"\n    namespace: \"&lt;project-namespace&gt;\" # INSERT LINE\n    cluster: \"eidf-general-prod\"\n\n*** MORE CONFIG ***\n</code></pre> </li> <li> <p>Check kubectl connects to the cluster. If this does not work you delete and re-download the kubeconfig file using the button on the project page of the EIDF portal.</p> <pre><code>kubectl get pods\n</code></pre> </li> </ol>"},{"location":"services/gpuservice/training/L2_requesting_persistent_volumes/","title":"Requesting persistent volumes With Kubernetes","text":""},{"location":"services/gpuservice/training/L2_requesting_persistent_volumes/#requirements","title":"Requirements","text":"<p>It is recommended that users complete Getting started with Kubernetes before proceeding with this tutorial.</p>"},{"location":"services/gpuservice/training/L2_requesting_persistent_volumes/#overview","title":"Overview","text":"<p>Ceph RBD &amp; Ceph FS - ReadWriteMany volume recommendation</p> <p>Prior to April 2025, the default storage class in the GPU cluster was Ceph RBD, which was a ReadWriteOnce type volume.</p> <p>This meant, that cluster could only mount the volume to a single active workload. Since the service migration, the CephFS storage class is the default, which allows ReadWriteMany and functionally supersedes the Ceph RBD volumes.</p> <p>Please consider migrating your data onto CephFS volumes (if you have not already), and change your PVC definitions to use the new storage class afterwards.</p> <p>Pods in the K8s EIDF GPU Service are intentionally ephemeral.</p> <p>They only last as long as required to complete the task that they were created for.</p> <p>Keeping pods ephemeral ensures the cluster resources are released for other users to request.</p> <p>However, this means the default storage volumes within a pod are temporary.</p> <p>If multiple pods require access to the same large data set or they output large files, then computationally costly file transfers need to be included in every pod instance.</p> <p>K8s allows you to request persistent volumes that can be mounted to multiple pods to share files or collate outputs.</p> <p>These persistent volumes will remain even if the pods they are mounted to are deleted, are updated or crash.</p>"},{"location":"services/gpuservice/training/L2_requesting_persistent_volumes/#submitting-a-persistent-volume-claim","title":"Submitting a Persistent Volume Claim","text":"<p>Before a persistent volume can be mounted to a pod, the required storage resources need to be requested and reserved to your namespace.</p> <p>A PersistentVolumeClaim (PVC) needs to be submitted to K8s to request the storage resources.</p> <p>The storage resources are held on a Ceph server which can accept requests up to 100 TiB. The Ceph FS storage class allows for multiple workloads to mount the volume.</p> <p>Example PVCs can be seen on the Kubernetes documentation page.</p> <p>All PVCs on the EIDF GPU Service should use the <code>csi-cephfs-sc</code> storage class.</p>"},{"location":"services/gpuservice/training/L2_requesting_persistent_volumes/#example-persistentvolumeclaim","title":"Example PersistentVolumeClaim","text":"<pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n name: test-ceph-pvc\nspec:\n accessModes:\n  - ReadWriteMany\n resources:\n  requests:\n   storage: 2Gi\n storageClassName: csi-cephfs-sc\n</code></pre> <p>ReadWriteMany volumes</p> <p>A RWM volume allows several jobs and pods to use the same disk at the same time. The obvious benefit to this is allowing parallel workloads to work with the same dataset at the same time, or use a single volume for multiple datasets, without the need to manage each individually. The new Ceph FS storage class facilitates this.</p> <p>You create a persistent volume by passing the yaml file to kubectl like a pod specification yaml <code>kubectl -n &lt;project-namespace&gt; create -f &lt;PVC specification yaml&gt;</code> Once you have successfully created a persistent volume you can interact with it using the standard kubectl commands:</p> <ul> <li><code>kubectl -n &lt;project-namespace&gt; delete pvc &lt;PVC name&gt;</code></li> <li><code>kubectl -n &lt;project-namespace&gt; get pvc &lt;PVC name&gt;</code></li> <li><code>kubectl -n &lt;project-namespace&gt; apply -f &lt;PVC specification yaml&gt;</code></li> </ul>"},{"location":"services/gpuservice/training/L2_requesting_persistent_volumes/#mounting-a-persistent-volume-to-a-pod","title":"Mounting a persistent Volume to a Pod","text":"<p>Introducing a persistent volume to a pod requires the addition of a volumeMount option to the container and a volume option linking to the PVC in the pod specification yaml.</p>"},{"location":"services/gpuservice/training/L2_requesting_persistent_volumes/#example-pod-specification-yaml-with-mounted-persistent-volume","title":"Example pod specification yaml with mounted persistent volume","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  generateName: test-ceph-pvc-job-\n  labels:\n    kueue.x-k8s.io/queue-name: &lt;project namespace&gt;-user-queue\nspec:\n  completions: 1\n  backoffLimit: 1\n  ttlSecondsAfterFinished: 1800\n  template:\n    metadata:\n      name: test-ceph-pvc-pod\n    spec:\n      containers:\n      - name: cudasample\n        image: busybox\n        args: [\"sleep\", \"infinity\"]\n        resources:\n          requests:\n            cpu: 2\n            memory: '1Gi'\n          limits:\n            cpu: 2\n            memory: '4Gi'\n        volumeMounts:\n          - mountPath: /mnt/ceph\n            name: volume\n      restartPolicy: Never\n      volumes:\n        - name: volume\n          persistentVolumeClaim:\n            claimName: test-ceph-pvc\n</code></pre>"},{"location":"services/gpuservice/training/L2_requesting_persistent_volumes/#accessing-the-persistent-volume-outside-a-pod","title":"Accessing the persistent volume outside a pod","text":"<p>To move files in/out of the persistent volume from outside a pod you can use the kubectl cp command.</p> <pre><code>*** On Login Node - replacing pod name with your pod name ***\nkubectl -n &lt;project-namespace&gt; cp /home/data/test_data.csv test-ceph-pvc-job-8c9cc:/mnt/ceph_rbd\n</code></pre> <p>For more complex file transfers and synchronisation, create a low resource pod with the persistent volume mounted.</p> <p>The bash command rsync can be amended to manage file transfers into the mounted PV following this GitHub repo.</p>"},{"location":"services/gpuservice/training/L2_requesting_persistent_volumes/#clean-up","title":"Clean up","text":"<pre><code>kubectl -n &lt;project-namespace&gt; delete job test-ceph-pvc-job\n\nkubectl -n &lt;project-namespace&gt; delete pvc test-ceph-pvc\n</code></pre>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/","title":"Running a PyTorch task","text":""},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#requirements","title":"Requirements","text":"<p>It is recommended that users complete Getting started with Kubernetes and Requesting persistent volumes With Kubernetes before proceeding with this tutorial.</p>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#overview","title":"Overview","text":"<p>In the following lesson, we'll build a CNN neural network and train it using the EIDF GPU Service.</p> <p>The model was taken from the PyTorch Tutorials.</p> <p>The lesson will be split into three parts:</p> <ul> <li>Requesting a persistent volume and transferring code/data to it</li> <li>Creating a pod with a PyTorch container downloaded from DockerHub</li> <li>Submitting a job to the EIDF GPU Service and retrieving the results</li> </ul>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#load-training-data-and-ml-code-into-a-persistent-volume","title":"Load training data and ML code into a persistent volume","text":""},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#create-a-persistent-volume","title":"Create a persistent volume","text":"<p>Request memory from the Ceph server by submitting a PVC to K8s (example pvc spec yaml below).</p> <pre><code>kubectl -n &lt;project-namespace&gt; create -f &lt;pvc-spec-yaml&gt;\n</code></pre>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#example-pytorch-persistentvolumeclaim","title":"Example PyTorch PersistentVolumeClaim","text":"<pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n name: pytorch-pvc\nspec:\n accessModes:\n  - ReadWriteMany\n resources:\n  requests:\n   storage: 2Gi\n storageClassName: csi-cephfs-sc\n</code></pre>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#transfer-codedata-to-persistent-volume","title":"Transfer code/data to persistent volume","text":"<ol> <li> <p>Check PVC has been created</p> <pre><code>kubectl -n &lt;project-namespace&gt; get pvc &lt;pv-name&gt;\n</code></pre> </li> <li> <p>Create a lightweight job with pod with PV mounted (example job below)</p> <pre><code>kubectl -n &lt;project-namespace&gt; create -f lightweight-pod-job.yaml\n</code></pre> </li> <li> <p>Download the PyTorch code</p> <pre><code>wget https://github.com/EPCCed/eidf-docs/raw/main/docs/services/gpuservice/training/resources/example_pytorch_code.py\n</code></pre> </li> <li> <p>Copy the Python script into the PV</p> <pre><code>kubectl -n &lt;project-namespace&gt; cp example_pytorch_code.py lightweight-job-&lt;identifier&gt;:/mnt/ceph_rbd/\n</code></pre> </li> <li> <p>Check whether the files were transferred successfully</p> <pre><code>kubectl -n &lt;project-namespace&gt; exec lightweight-job-&lt;identifier&gt; -- ls /mnt/ceph\n</code></pre> </li> <li> <p>Delete the lightweight job</p> <pre><code>kubectl -n &lt;project-namespace&gt; delete job lightweight-job-&lt;identifier&gt;\n</code></pre> </li> </ol>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#example-lightweight-job-specification","title":"Example lightweight job specification","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  generateName: lightweight-job-\n  labels:\n    kueue.x-k8s.io/queue-name: &lt;project namespace&gt;-user-queue\nspec:\n  completions: 1\n  backoffLimit: 1\n  ttlSecondsAfterFinished: 1800\n  template:\n    metadata:\n      name: lightweight-pod\n    spec:\n      containers:\n      - name: data-loader\n        image: busybox\n        args: [\"sleep\", \"infinity\"]\n        resources:\n          requests:\n            cpu: 1\n            memory: '1Gi'\n          limits:\n            cpu: 1\n            memory: '1Gi'\n        volumeMounts:\n          - mountPath: /mnt/ceph\n            name: volume\n      restartPolicy: Never\n      volumes:\n        - name: volume\n          persistentVolumeClaim:\n            claimName: pytorch-pvc\n</code></pre>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#creating-a-job-with-a-pytorch-container","title":"Creating a Job with a PyTorch container","text":"<p>We will use the pre-made PyTorch Docker image available on Docker Hub to run the PyTorch ML model.</p> <p>The PyTorch container will be held within a pod that has the persistent volume mounted and access a MIG GPU.</p> <p>Submit the specification file below to K8s to create the job, replacing the queue name with your project namespace queue name.</p> <pre><code>kubectl -n &lt;project-namespace&gt; create -f &lt;pytorch-job-yaml&gt;\n</code></pre>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#example-pytorch-job-specification-file","title":"Example PyTorch Job Specification File","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  generateName: pytorch-job-\n  labels:\n    kueue.x-k8s.io/queue-name: &lt;project namespace&gt;-user-queue\nspec:\n  completions: 1\n  backoffLimit: 1\n  ttlSecondsAfterFinished: 1800\n  template:\n    metadata:\n      name: pytorch-pod\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: pytorch-con\n        image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\n        command: [\"python3\"]\n        args: [\"/mnt/ceph/example_pytorch_code.py\"]\n        volumeMounts:\n          - mountPath: /mnt/ceph\n            name: volume\n        resources:\n          requests:\n            cpu: 2\n            memory: \"1Gi\"\n          limits:\n            cpu: 4\n            memory: \"4Gi\"\n            nvidia.com/gpu: 1\n      nodeSelector:\n        nvidia.com/gpu.product: NVIDIA-A100-SXM4-40GB-MIG-1g.5gb\n      volumes:\n        - name: volume\n          persistentVolumeClaim:\n            claimName: pytorch-pvc\n</code></pre>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#reviewing-the-results-of-the-pytorch-model","title":"Reviewing the results of the PyTorch model","text":"<p>This is not intended to be an introduction to PyTorch, please see the online tutorial for details about the model.</p> <ol> <li> <p>Check that the model ran to completion</p> <pre><code>kubectl -n &lt;project-namespace&gt; logs &lt;pytorch-pod-name&gt;\n</code></pre> </li> <li> <p>Spin up a lightweight pod to retrieve results</p> <pre><code>kubectl -n &lt;project-namespace&gt; create -f lightweight-pod-job.yaml\n</code></pre> </li> <li> <p>Copy the trained model back to your access VM</p> <pre><code>kubectl -n &lt;project-namespace&gt; cp lightweight-job-&lt;identifier&gt;:mnt/ceph_rbd/model.pth model.pth\n</code></pre> </li> </ol>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#using-a-kubernetes-job-to-train-the-pytorch-model-multiple-times","title":"Using a Kubernetes job to train the pytorch model multiple times","text":"<p>A common ML training workflow may consist of training multiple iterations of a model: such as models with different hyperparameters or models trained on multiple different data sets.</p> <p>A Kubernetes job can create and manage multiple pods with identical or different initial parameters.</p> <p>NVIDIA provide a detailed tutorial on how to conduct a ML hyperparameter search with a Kubernetes job.</p> <p>Below is an example job yaml for running the pytorch model which will continue to create pods until three have successfully completed the task of training the model.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  generateName: pytorch-job-\n  labels:\n    kueue.x-k8s.io/queue-name: &lt;project namespace&gt;-user-queue\nspec:\n  ttlSecondsAfterFinished: 3600\n  completions: 3\n  backoffLimit: 4\n  template:\n    metadata:\n      name: pytorch-pod\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: pytorch-con\n        image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\n        command: [\"python3\"]\n        args: [\"/mnt/ceph/example_pytorch_code.py\"]\n        volumeMounts:\n          - mountPath: /mnt/ceph\n            name: volume\n        resources:\n          requests:\n            cpu: 2\n            memory: \"1Gi\"\n          limits:\n            cpu: 4\n            memory: \"4Gi\"\n            nvidia.com/gpu: 1\n      nodeSelector:\n          nvidia.com/gpu.product: NVIDIA-A100-SXM4-40GB-MIG-1g.5gb\n      volumes:\n          - name: volume\n            persistentVolumeClaim:\n              claimName: pytorch-pvc\n</code></pre>"},{"location":"services/gpuservice/training/L3_running_a_pytorch_task/#clean-up","title":"Clean up","text":"<pre><code>kubectl -n &lt;project-namespace&gt; delete pod pytorch-job\n\nkubectl -n &lt;project-namespace&gt; delete pvc pytorch-pvc\n</code></pre>"},{"location":"services/gpuservice/training/L4_template_workflow/","title":"Template workflow","text":""},{"location":"services/gpuservice/training/L4_template_workflow/#requirements","title":"Requirements","text":"<p>It is recommended that users complete Getting started with Kubernetes and Requesting persistent volumes With Kubernetes before proceeding with this tutorial.</p>"},{"location":"services/gpuservice/training/L4_template_workflow/#overview","title":"Overview","text":"<p>An example workflow for code development using K8s is outlined below.</p> <p>In theory, users can create docker images with all the code, software and data included to complete their analysis.</p> <p>In practice, docker images with the required software can be several gigabytes in size which can lead to unacceptable download times when ~100GB of data and code is then added.</p> <p>Therefore, it is recommended to separate code, software, and data preparation into distinct steps:</p> <ol> <li> <p>Data Loading: Loading large data sets asynchronously.</p> </li> <li> <p>Developing a Docker environment: Manually or automatically building Docker images.</p> </li> <li> <p>Code development with K8s: Iteratively changing and testing code in a job.</p> </li> </ol> <p>The workflow describes different strategies to tackle the three common stages in code development and analysis using the EIDF GPU Service.</p> <p>The three stages are interchangeable and may not be relevant to every project.</p> <p>Some strategies in the workflow require a GitHub account and Docker Hub account for automatic building (this can be adapted for other platforms such as GitLab).</p>"},{"location":"services/gpuservice/training/L4_template_workflow/#data-loading","title":"Data loading","text":"<p>The EIDF GPU service contains GPUs with 40Gb/80Gb of on board memory and it is expected that data sets of &gt; 100 Gb will be loaded onto the service to utilise this hardware.</p> <p>Persistent volume claims need to be of sufficient size to hold the input data, any expected output data and a small amount of additional empty space to facilitate IO.</p> <p>Read the requesting persistent volumes with Kubernetes lesson to learn how to request and mount persistent volumes to pods.</p> <p>It often takes several hours or days to download data sets of 1/2 TB or more to a persistent volume.</p> <p>Therefore, the data download step needs to be completed asynchronously as maintaining a contention to the server for long periods of time can be unreliable.</p>"},{"location":"services/gpuservice/training/L4_template_workflow/#asynchronous-data-downloading-with-a-lightweight-job","title":"Asynchronous data downloading with a lightweight job","text":"<ol> <li> <p>Check a PVC has been created.</p> <pre><code>kubectl -n &lt;project-namespace&gt; get pvc template-workflow-pvc\n</code></pre> </li> <li> <p>Write a job yaml with PV mounted and a command to download the data. Change the curl URL to your data set of interest.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n generateName: lightweight-job-\n labels:\n  kueue.x-k8s.io/queue-name: &lt;project-namespace&gt;-user-queue\nspec:\n completions: 1\n backoffLimit: 1\n parallelism: 1\n ttlSecondsAfterFinished: 1800\n template:\n  metadata:\n   name: lightweight-job\n  spec:\n   restartPolicy: Never\n   containers:\n   - name: data-loader\n     image: alpine/curl:latest\n     command: ['sh', '-c', \"cd /mnt/ceph; curl https://archive.ics.uci.edu/static/public/53/iris.zip -o iris.zip\"]\n     resources:\n      requests:\n       cpu: 1\n       memory: \"1Gi\"\n      limits:\n       cpu: 1\n       memory: \"1Gi\"\n     volumeMounts:\n     - mountPath: /mnt/ceph\n       name: volume\n   volumes:\n   - name: volume\n     persistentVolumeClaim:\n      claimName: template-workflow-pvc\n</code></pre> </li> <li> <p>Run the data download job.</p> <pre><code>kubectl -n &lt;project-namespace&gt; create -f lightweight-pod.yaml\n</code></pre> </li> <li> <p>Check if the download has completed.</p> <pre><code>kubectl -n &lt;project-namespace&gt; get jobs\n</code></pre> </li> <li> <p>Delete the lightweight job once completed.</p> <pre><code>kubectl -n &lt;project-namespace&gt; delete job lightweight-job\n</code></pre> </li> </ol>"},{"location":"services/gpuservice/training/L4_template_workflow/#asynchronous-data-downloading-within-a-screen-session","title":"Asynchronous data downloading within a screen session","text":"<p>Screen is a window manager available in Linux that allows you to create multiple interactive shells and swap between then.</p> <p>Screen has the added benefit that if your remote session is interrupted the screen session persists and can be reattached when you manage to reconnect.</p> <p>This allows you to start a task, such as downloading a data set, and check in on it asynchronously.</p> <p>Once you have started a screen session, you can create a new window with <code>ctrl-a c</code>, swap between windows with <code>ctrl-a 0-9</code> and exit screen (but keep any task running) with <code>ctrl-a d</code>.</p> <p>Using screen rather than a single download job can be helpful if downloading multiple data sets or if you intend to do some simple QC or tidying up before/after downloading.</p> <ol> <li> <p>Start a screen session.</p> <pre><code>screen\n</code></pre> </li> <li> <p>Create an interactive lightweight job session.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n generateName: lightweight-job-\n labels:\n  kueue.x-k8s.io/queue-name: &lt;project-namespace&gt;-user-queue\nspec:\n completions: 1\n backoffLimit: 1\n parallelism: 1\n ttlSecondsAfterFinished: 1800\n template:\n  metadata:\n   name: lightweight-pod\n  spec:\n   restartPolicy: Never\n   containers:\n   - name: data-loader\n     image: alpine/curl:latest\n     command: ['sleep','infinity']\n     resources:\n      requests:\n       cpu: 1\n       memory: \"1Gi\"\n      limits:\n       cpu: 1\n       memory: \"1Gi\"\n     volumeMounts:\n     - mountPath: /mnt/ceph\n       name: volume\n   volumes:\n   - name: volume\n     persistentVolumeClaim:\n      claimName: template-workflow-pvc\n</code></pre> </li> <li> <p>Download data set. Change the curl URL to your data set of interest.</p> <pre><code>kubectl -n &lt;project-namespace&gt; exec &lt;lightweight-pod-name&gt; -- curl https://archive.ics.uci.edu/static/public/53/iris.zip -o /mnt/ceph_rbd/iris.zip\n</code></pre> </li> <li> <p>Exit the remote session by either ending the session or <code>ctrl-a d</code>.</p> </li> <li> <p>Reconnect at a later time and reattach the screen window.</p> <pre><code>screen -list\n\nscreen -r &lt;session-name&gt;\n</code></pre> </li> <li> <p>Check the download was successful and delete the job.</p> <pre><code>kubectl -n &lt;project-namespace&gt; exec &lt;lightweight-pod-name&gt; -- ls /mnt/ceph_rbd/\n\nkubectl -n &lt;project-namespace&gt; delete job lightweight-job\n</code></pre> </li> <li> <p>Exit the screen session.</p> <pre><code>exit\n</code></pre> </li> </ol>"},{"location":"services/gpuservice/training/L4_template_workflow/#preparing-a-custom-docker-image","title":"Preparing a custom Docker image","text":"<p>Kubernetes requires Docker images to be pre-built and available for download from a container repository such as Docker Hub.</p> <p>It does not provide functionality to build images and create pods from docker files.</p> <p>However, use cases may require some custom modifications of a base image, such as adding a python library.</p> <p>These custom images need to be built locally (using docker) or online (using a GitHub/GitLab worker) and pushed to a repository such as Docker Hub.</p> <p>This is not an introduction to building docker images, please see the Docker tutorial  for a general overview.</p>"},{"location":"services/gpuservice/training/L4_template_workflow/#manually-building-a-docker-image-locally","title":"Manually building a Docker image locally","text":"<ol> <li> <p>Select a suitable base image (The Nvidia container catalog is often a useful starting place for GPU accelerated tasks). We'll use the base RAPIDS image.</p> </li> <li> <p>Create a Dockerfile to add any additional packages required to the base image.</p> <pre><code>FROM nvcr.io/nvidia/rapidsai/base:23.12-cuda12.0-py3.10\nRUN pip install pandas\nRUN pip install plotly\n</code></pre> </li> <li> <p>Build the Docker container locally (You will need to install Docker)</p> <pre><code>cd &lt;dockerfile-folder&gt;\n\ndocker build . -t &lt;docker-hub-username&gt;/template-docker-image:latest\n</code></pre> </li> </ol> <p>Building images for different CPU architectures</p> <p>Be aware that docker images built for Apple ARM64 architectures will not function optimally on the EIDFGPU Service's AMD64 based architecture.</p> <p>If building docker images locally on an Apple device you must tell the docker daemon to use AMD64 based images by passing the <code>--platform linux/amd64</code> flag to the build function.</p> <ol> <li> <p>Create a repository to hold the image on Docker Hub (You will need to create and setup an account).</p> </li> <li> <p>Push the Docker image to the repository.</p> <pre><code>docker push &lt;docker-hub-username&gt;/template-docker-image:latest\n</code></pre> </li> <li> <p>Finally, specify your Docker image in the <code>image:</code> tag of the job specification yaml file.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n generateName: template-workflow-job\n labels:\n  kueue.x-k8s.io/queue-name: &lt;project-namespace&gt;-user-queue\nspec:\n completions: 1\n parallelism: 1\n template:\n  spec:\n   restartPolicy: Never\n   containers:\n   - name: template-docker-image\n     image: &lt;docker-hub-username&gt;/template-docker-image:latest\n     command: [\"sleep\", \"infinity\"]\n     resources:\n      requests:\n       cpu: 1\n       memory: \"4Gi\"\n      limits:\n       cpu: 1\n       memory: \"8Gi\"\n</code></pre> </li> </ol>"},{"location":"services/gpuservice/training/L4_template_workflow/#automatically-building-docker-images-using-github-actions","title":"Automatically building docker images using GitHub Actions","text":"<p>In cases where the Docker image needs to be built and tested iteratively (i.e. to check for comparability issues), git version control and GitHub Actions can simplify the build process.</p> <p>A GitHub action can build and push a Docker image to Docker Hub whenever it detects a git push that changes the docker file in a git repo.</p> <p>This process requires you to already have a GitHub and Docker Hub account.</p> <ol> <li> <p>Create an access token on your Docker Hub account to allow GitHub to push changes to the Docker Hub image repo.</p> </li> <li> <p>Create two GitHub secrets to securely provide your Docker Hub username and access token.</p> </li> <li> <p>Add the dockerfile to a code/docker folder within an active GitHub repo.</p> </li> <li> <p>Add the GitHub action yaml file below to the .github/workflow folder to automatically push a new image to Docker Hub if any changes to files in the code/docker folder is detected.</p> <pre><code>name: ci\non:\n  push:\n    paths:\n      - 'code/docker/**'\n\njobs:\n  docker:\n    runs-on: ubuntu-latest\n    steps:\n      -\n        name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: \"{{defaultContext}}:code/docker\"\n          push: true\n          tags: &lt;target-dockerhub-image-name&gt;\n</code></pre> </li> <li> <p>Push a change to the dockerfile and check the Docker Hub image is updated.</p> </li> </ol>"},{"location":"services/gpuservice/training/L4_template_workflow/#code-development-with-k8s","title":"Code development with K8s","text":"<p>Production code can be included within a Docker image to aid reproducibility as the specific software versions required to run the code are packaged together.</p> <p>However, binding the code to the docker image during development can delay the testing cycle as re-downloading all of the software for every change in a code block can take time.</p> <p>If the docker image is consistent across tests, then it can be cached locally on the EIDFGPU Service instead of being re-downloaded (this occurs automatically although the cache is node specific and is not shared across nodes).</p> <p>A pod yaml file can be defined to automatically pull the latest code version before running any tests.</p> <p>Reducing the download time to fractions of a second allows rapid testing to be completed on the cluster with just the <code>kubectl create</code> command.</p> <p>You must already have a GitHub account to follow this process.</p> <p>This process allows code development to be conducted on any device/VM with access to the repo (GitHub/GitLab).</p> <p>A template GitHub repo with sample code, k8s yaml files and a Docker build Github Action is available here.</p>"},{"location":"services/gpuservice/training/L4_template_workflow/#create-a-job-that-downloads-and-runs-the-latest-code-version-at-runtime","title":"Create a job that downloads and runs the latest code version at runtime","text":"<ol> <li> <p>Write a standard yaml file for a k8s job with the required resources and custom docker image (example below)</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n generateName: template-workflow-job-\n labels:\n  kueue.x-k8s.io/queue-name: &lt;project-namespace&gt;-user-queue\nspec:\n completions: 1\n parallelism: 1\n template:\n  spec:\n   restartPolicy: Never\n   containers:\n   - name: template-docker-image\n     image: &lt;docker-hub-username&gt;/template-docker-image:latest\n     command: [\"sleep\", \"infinity\"]\n     resources:\n      requests:\n       cpu: 1\n       memory: \"4Gi\"\n      limits:\n       cpu: 1\n       memory: \"8Gi\"\n     volumeMounts:\n     - mountPath: /mnt/ceph\n       name: volume\n   volumes:\n   - name: volume\n     persistentVolumeClaim:\n      claimName: template-workflow-pvc\n</code></pre> </li> <li> <p>Add an initial container that runs before the main container to download the latest version of the code.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n generateName: template-workflow-job-\n labels:\n  kueue.x-k8s.io/queue-name: &lt;project-namespace&gt;-user-queue\nspec:\n completions: 1\n parallelism: 1\n template:\n  spec:\n   restartPolicy: Never\n   containers:\n   - name: template-docker-image\n     image: &lt;docker-hub-username&gt;/template-docker-image:latest\n     command: [\"sleep\", \"infinity\"]\n     resources:\n      requests:\n       cpu: 1\n       memory: \"4Gi\"\n      limits:\n       cpu: 1\n       memory: \"8Gi\"\n     volumeMounts:\n     - mountPath: /mnt/ceph\n       name: volume\n     - mountPath: /code\n       name: github-code\n   initContainers:\n   - name: lightweight-git-container\n     image: cicirello/alpine-plus-plus\n     command: ['sh', '-c', \"cd /code; git clone &lt;target-repo&gt;\"]\n     resources:\n      requests:\n       cpu: 1\n       memory: \"4Gi\"\n      limits:\n       cpu: 1\n       memory: \"8Gi\"\n     volumeMounts:\n     - mountPath: /code\n       name: github-code\n   volumes:\n   - name: volume\n     persistentVolumeClaim:\n      claimName: template-workflow-pvc\n   - name: github-code\n     emptyDir:\n      sizeLimit: 1Gi\n</code></pre> </li> <li> <p>Change the command argument in the main container to run the code once started. Add the URL of the GitHub repo of interest to the <code>initContainers: command:</code> tag.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n generateName: template-workflow-job-\n labels:\n  kueue.x-k8s.io/queue-name: &lt;project-namespace&gt;-user-queue\nspec:\n completions: 1\n parallelism: 1\n template:\n  spec:\n   restartPolicy: Never\n   containers:\n   - name: template-docker-image\n     image: &lt;docker-hub-username&gt;/template-docker-image:latest\n     command: ['sh', '-c', \"python3 /code/&lt;python-script&gt;\"]\n     resources:\n      requests:\n       cpu: 10\n       memory: \"40Gi\"\n      limits:\n       cpu: 10\n       memory: \"80Gi\"\n       nvidia.com/gpu: 1\n     volumeMounts:\n     - mountPath: /mnt/ceph\n       name: volume\n     - mountPath: /code\n       name: github-code\n   initContainers:\n   - name: lightweight-git-container\n     image: cicirello/alpine-plus-plus\n     command: ['sh', '-c', \"cd /code; git clone &lt;target-repo&gt;\"]\n     resources:\n      requests:\n       cpu: 1\n       memory: \"4Gi\"\n      limits:\n       cpu: 1\n       memory: \"8Gi\"\n     volumeMounts:\n     - mountPath: /code\n       name: github-code\n   volumes:\n   - name: volume\n     persistentVolumeClaim:\n      claimName: template-workflow-pvc\n   - name: github-code\n     emptyDir:\n      sizeLimit: 1Gi\n</code></pre> </li> <li> <p>Submit the yaml file to kubernetes</p> <pre><code>kubectl -n &lt;project-namespace&gt; create -f &lt;job-yaml-file&gt;\n</code></pre> </li> </ol>"},{"location":"services/graphcore/","title":"Overview","text":"<p>EIDF hosts a Graphcore Bow Pod64 system for AI acceleration.</p> <p>The specification of the Bow Pod64 is:</p> <ul> <li>16x Bow-2000 machines</li> <li>64x Bow IPUs (4 IPUs per Bow-2000)</li> <li>94,208 IPU cores (1472 cores per IPU)</li> <li>57.6GB of In-Processor-Memory (0.9GB per IPU)</li> </ul> <p>For more details about the IPU architecture, see documentation from Graphcore.</p> <p>The smallest unit of compute resource that can be requested is a single IPU.</p> <p>Similarly to the EIDF GPU Service, usage of the Graphcore is managed using Kubernetes.</p>"},{"location":"services/graphcore/#service-access","title":"Service Access","text":"<p>Access to the Graphcore accelerator is provisioning through the EIDF GPU Service.</p> <p>Users should apply for access to Graphcore via the EIDF GPU Service.</p>"},{"location":"services/graphcore/#project-quotas","title":"Project Quotas","text":"<p>Currently there is no active quota mechanism on the Graphcore accelerator. IPUJobs should be actively using partitions on the Graphcore.</p>"},{"location":"services/graphcore/#graphcore-tutorial","title":"Graphcore Tutorial","text":"<p>The following tutorial teaches users how to submit tasks to the Graphcore system. This tutorial assumes basic familiary with submitting jobs via Kubernetes. For a tutorial on using Kubernetes, see the GPU service tutorial. For more in-depth lessons about developing applications for Graphcore, see the general documentation and guide for creating IPU jobs via Kubernetes.</p> Lesson Objective Getting started with IPU jobs a. How to send an IPUJob.b. Monitoring and Cancelling your IPUJob. Multi-IPU Jobs a. Using multiple IPUs for distributed training. Profiling with PopVision a. Enabling profiling in your code.b. Downloading the profile reports. Other Frameworks a. Using Tensorflow and PopART.b. Writing IPU programs with PopLibs (C++)."},{"location":"services/graphcore/#further-reading-and-help","title":"Further Reading and Help","text":"<ul> <li> <p>The Graphcore documentation provides information about using the Graphcore system.</p> </li> <li> <p>The Graphcore examples repository on GitHub provides a catalogue of application examples that have been optimised to run on Graphcore IPUs for both training and inference. It also contains tutorials for using various frameworks.</p> </li> </ul>"},{"location":"services/graphcore/faq/","title":"Graphcore FAQ","text":""},{"location":"services/graphcore/faq/#graphcore-questions","title":"Graphcore Questions","text":""},{"location":"services/graphcore/faq/#how-do-i-delete-a-runningterminated-pod","title":"How do I delete a running/terminated pod?","text":"<p><code>IPUJobs</code> manages the launcher and worker <code>pods</code>, therefore the pods will be deleted when the <code>IPUJob</code> is deleted, using <code>kubectl delete ipujobs &lt;IPUJob-name&gt;</code>. If only the <code>pod</code> is deleted via <code>kubectl delete pod</code>, the <code>IPUJob</code> may respawn the <code>pod</code>.</p> <p>To see running or terminated <code>IPUJobs</code>, run <code>kubectl get ipujobs</code>.</p>"},{"location":"services/graphcore/faq/#my-ipujob-died-with-a-message-poptorch_cpp_error-failed-to-acquire-x-ipus-why","title":"My IPUJob died with a message: <code>'poptorch_cpp_error': Failed to acquire X IPU(s)</code>. Why?","text":"<p>This error may appear when the IPUJob name is too long.</p> <p>We have identified that for IPUJobs with <code>metadata:name</code> length over 36 characters, this error may appear. A solution is to reduce the name to under 36 characters.</p>"},{"location":"services/graphcore/training/L1_getting_started/","title":"Getting started with Graphcore IPU Jobs","text":"<p>This guide assumes basic familiarity with Kubernetes (K8s) and usage of <code>kubectl</code>. See GPU service tutorial to get started.</p>"},{"location":"services/graphcore/training/L1_getting_started/#introduction","title":"Introduction","text":"<p>Graphcore provides prebuilt docker containers (full lists here) which contain the required libraries (pytorch, tensorflow, poplar etc.) and can be used directly within the K8s to run on the Graphcore IPUs.</p> <p>In this tutorial we will cover running training with a single IPU. The subsequent tutorial will cover using multiple IPUs, which can be used for distrubed training jobs.</p>"},{"location":"services/graphcore/training/L1_getting_started/#creating-your-first-ipu-job","title":"Creating your first IPU job","text":"<p>For our first IPU job, we will be using the Graphcore PyTorch (PopTorch) container image (<code>graphcore/pytorch:3.3.0</code>) to run a simple example of training a neural network for classification on the MNIST dataset, which is provided here. More applications can be found in the repository https://github.com/graphcore/examples.</p> <p>To get started:</p> <ol> <li>to specify the job - create the file <code>mnist-training-ipujob.yaml</code>, then copy and save the following content into the file:</li> </ol> <pre><code>apiVersion: graphcore.ai/v1alpha1\nkind: IPUJob\nmetadata:\n  generateName: mnist-training-\nspec:\n  # jobInstances defines the number of job instances.\n  # More than 1 job instance is usually useful for inference jobs only.\n  jobInstances: 1\n  # ipusPerJobInstance refers to the number of IPUs required per job instance.\n  # A separate IPU partition of this size will be created by the IPU Operator\n  # for each job instance.\n  ipusPerJobInstance: \"1\"\n  workers:\n    template:\n      spec:\n        containers:\n        - name: mnist-training\n          image: graphcore/pytorch:3.3.0\n          command: [/bin/bash, -c, --]\n          args:\n            - |\n              cd;\n              mkdir build;\n              cd build;\n              git clone https://github.com/graphcore/examples.git;\n              cd examples/tutorials/simple_applications/pytorch/mnist;\n              python -m pip install -r requirements.txt;\n              python mnist_poptorch_code_only.py --epochs 1\n          resources:\n            limits:\n              cpu: 32\n              memory: 200Gi\n          securityContext:\n            capabilities:\n              add:\n              - IPC_LOCK\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: devshm\n        restartPolicy: Never\n        hostIPC: true\n        volumes:\n        - emptyDir:\n            medium: Memory\n            sizeLimit: 10Gi\n          name: devshm\n</code></pre> <ol> <li> <p>to submit the job - run <code>kubectl create -f mnist-training-ipujob.yaml</code>, which will give the following output:</p> <pre><code>ipujob.graphcore.ai/mnist-training-&lt;random string&gt; created\n</code></pre> </li> <li> <p>to monitor progress of the job - run <code>kubectl get pods</code>, which will give the following output</p> <pre><code>NAME                      READY   STATUS      RESTARTS   AGE\nmnist-training-&lt;random string&gt;-worker-0   0/1     Completed   0          2m56s\n</code></pre> </li> <li> <p>to read the result - run <code>kubectl logs mnist-training-&lt;random string&gt;-worker-0</code>, which will give the following output (or similar)</p> </li> </ol> <pre><code>...\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:23&lt;00:00]\nEpochs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:34&lt;00:00, 34.18s/it]\n...\nAccuracy on test set: 97.08%\n</code></pre>"},{"location":"services/graphcore/training/L1_getting_started/#monitoring-and-cancelling-your-ipu-job","title":"Monitoring and Cancelling your IPU job","text":"<p>An IPU job creates an IPU Operator, which manages the required worker or launcher pods. To see running or complete <code>IPUjobs</code>, run <code>kubectl get ipujobs</code>, which will show:</p> <pre><code>NAME             STATUS      CURRENT   DESIRED   LASTMESSAGE          AGE\nmnist-training   Completed   0         1         All instances done   10m\n</code></pre> <p>To delete the <code>IPUjob</code>, run <code>kubectl delete ipujobs &lt;job-name&gt;</code>, e.g. <code>kubectl delete ipujobs mnist-training-&lt;random string&gt;</code>. This will also delete the associated worker pod <code>mnist-training-&lt;random string&gt;-worker-0</code>.</p> <p>Note: simply deleting the pod via <code>kubectl delete pods mnist-training-&lt;random-string&gt;-worker-0</code> does not delete the IPU job, which will need to be deleted separately.</p> <p>Note: you can list all pods via <code>kubectl get all</code> or <code>kubectl get pods</code>, but they do not show the ipujobs. These can be obtained using <code>kubectl get ipujobs</code>.</p> <p>Note: <code>kubectl describe &lt;pod-name&gt;</code> provides verbose description of a specific pod.</p>"},{"location":"services/graphcore/training/L1_getting_started/#description","title":"Description","text":"<p>The Graphcore IPU Operator (Kubernetes interface) extends the Kubernetes API by introducing a custom resource definition (CRD) named <code>IPUJob</code>, which can be seen at the beginning of the included yaml file:</p> <pre><code>apiVersion: graphcore.ai/v1alpha1\nkind: IPUJob\n</code></pre> <p>An <code>IPUJob</code> allows users to defineworkloads that can use IPUs. There are several fields specific to an <code>IPUJob</code>:</p> <p>job instances : This defines the number of jobs. In the case of training it should be 1.</p> <p>ipusPerJobInstance : This defines the size of IPU partition that will be created for each job instance.</p> <p>workers : This defines a Pod specification that will be used for <code>Worker</code> Pods, including the container image and commands.</p> <p>These fields have been populated in the example .yaml file. For distributed training (with multiple IPUs), additional fields need to be included, which will be described in the next lesson.</p>"},{"location":"services/graphcore/training/L1_getting_started/#additional-information","title":"Additional Information","text":"<p>It is possible to further specify the restart policy (<code>Always</code>/<code>OnFailure</code>/<code>Never</code>/<code>ExitCode</code>) and clean up policy (<code>Workers</code>/<code>All</code>/<code>None</code>); see here.</p>"},{"location":"services/graphcore/training/L2_multiple_IPU/","title":"Distributed training on multiple IPUs","text":"<p>In this tutorial, we will cover how to run larger models, including examples provided by Graphcore on https://github.com/graphcore/examples. These may require distributed training on multiple IPUs.</p> <p>The number of IPUs requested must be in powers of two, i.e. 1, 2, 4, 8, 16, 32, or 64.</p>"},{"location":"services/graphcore/training/L2_multiple_IPU/#first-example","title":"First example","text":"<p>As an example, we will use 4 IPUs to perform the pre-training step of BERT, an NLP transformer model. The code is available from https://github.com/graphcore/examples/tree/master/nlp/bert/pytorch.</p> <p>To get started, save and create an IPUJob with the following <code>.yaml</code> file:</p> <pre><code>apiVersion: graphcore.ai/v1alpha1\nkind: IPUJob\nmetadata:\n  generateName: bert-training-multi-ipu-\nspec:\n  jobInstances: 1\n  ipusPerJobInstance: \"4\"\n  workers:\n    template:\n      spec:\n        containers:\n        - name: bert-training-multi-ipu\n          image: graphcore/pytorch:3.3.0\n          command: [/bin/bash, -c, --]\n          args:\n            - |\n              cd ;\n              mkdir build;\n              cd build ;\n              git clone https://github.com/graphcore/examples.git;\n              cd examples/nlp/bert/pytorch;\n              apt update ;\n              apt upgrade -y;\n              DEBIAN_FRONTEND=noninteractive TZ='Europe/London' apt install $(&lt; required_apt_packages.txt) -y ;\n              pip3 install -r requirements.txt ;\n              python3 run_pretraining.py --dataset generated --config pretrain_base_128_pod4 --training-steps 1\n          resources:\n            limits:\n              cpu: 32\n              memory: 200Gi\n          securityContext:\n            capabilities:\n              add:\n              - IPC_LOCK\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: devshm\n        restartPolicy: Never\n        hostIPC: true\n        volumes:\n        - emptyDir:\n            medium: Memory\n            sizeLimit: 10Gi\n          name: devshm\n</code></pre> <p>Running the above IPUJob and querying the log via <code>kubectl logs pod/bert-training-multi-ipu-&lt;random string&gt;-worker-0</code> should give:</p> <pre><code>...\nData loaded in 8.559805537108332 secs\n-----------------------------------------------------------\n-------------------- Device Allocation --------------------\nEmbedding  --&gt; IPU 0\nEncoder 0  --&gt; IPU 1\nEncoder 1  --&gt; IPU 1\nEncoder 2  --&gt; IPU 1\nEncoder 3  --&gt; IPU 1\nEncoder 4  --&gt; IPU 2\nEncoder 5  --&gt; IPU 2\nEncoder 6  --&gt; IPU 2\nEncoder 7  --&gt; IPU 2\nEncoder 8  --&gt; IPU 3\nEncoder 9  --&gt; IPU 3\nEncoder 10 --&gt; IPU 3\nEncoder 11 --&gt; IPU 3\nPooler     --&gt; IPU 0\nClassifier --&gt; IPU 0\n-----------------------------------------------------------\n---------- Compilation/Loading from Cache Started ---------\n\n...\n\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [08:02&lt;00:00]\nCompiled/Loaded model in 500.756152929971 secs\n-----------------------------------------------------------\n--------------------- Training Started --------------------\nStep: 0 / 0 - LR: 0.00e+00 - total loss: 10.817 - mlm_loss: 10.386 - nsp_loss: 0.432 - mlm_acc: 0.000 % - nsp_acc: 1.000 %:   0%|          | 0/1 [00:16&lt;?, ?it/s, throughput: 4035.0 samples/sec]\n-----------------------------------------------------------\n-------------------- Training Metrics ---------------------\nglobal_batch_size: 65536\ndevice_iterations: 1\ntraining_steps: 1\nTraining time: 16.245 secs\n-----------------------------------------------------------\n</code></pre>"},{"location":"services/graphcore/training/L2_multiple_IPU/#details","title":"Details","text":"<p>In this example, we have requested 4 IPUs:</p> <pre><code>ipusPerJobInstance: \"4\"\n</code></pre> <p>The python flag <code>--config pretrain_base_128_pod4</code> uses one of the preset configurations for this model with 4 IPUs. Here we also use the <code>--datset generated</code> flag to generate data rather than download the required dataset.</p> <p>To provided sufficient shm for the IPU pod, it may be necessary to mount <code>/dev/shm</code> as follows:</p> <pre><code>          volumeMounts:\n          - mountPath: /dev/shm\n            name: devshm\n        volumes:\n        - emptyDir:\n            medium: Memory\n            sizeLimit: 10Gi\n          name: devshm\n</code></pre> <p>It is also required to set <code>spec.hostIPC</code> to <code>true</code>:</p> <pre><code>  hostIPC: true\n</code></pre> <p>and add a <code>securityContext</code> to the container definition than enables the <code>IPC_LOCK</code> capability:</p> <pre><code>    securityContext:\n      capabilities:\n        add:\n        - IPC_LOCK\n</code></pre> <p>Note: <code>IPC_LOCK</code> allows for the RDMA software stack to use pinned memory \u2014 which is particularly useful for PyTorch dataloaders, which can be very memory hungry. This is since all data going to the IPUs go via the network interfaces (via 100Gbps ethernet).</p>"},{"location":"services/graphcore/training/L2_multiple_IPU/#memory-usage","title":"Memory usage","text":"<p>In general, the graph compilation phase of running large models can require significant memory, and far less during the execution phase.</p> <p>In the example above, it is possible to explicitly request the memory via:</p> <pre><code>          resources:\n            limits:\n              memory: \"128Gi\"\n            requests:\n              memory: \"128Gi\"\n</code></pre> <p>which will succeed. (The graph compilation fails if only <code>32Gi</code> is requested.)</p> <p>As a general guideline, 128GB memory should be enough for the majority of tasks, and rarely exceed 200GB even for jobs with high IPU count. In the example <code>.yaml</code> script, we do not specifically request the memory.</p>"},{"location":"services/graphcore/training/L2_multiple_IPU/#scaling-up-ipu-count-and-using-poprun","title":"Scaling up IPU count and using Poprun","text":"<p>In the example above, python is launched directly in the pod. When scaling up the number of IPUs (e.g. above 8 IPUs), it may be possible to run into a CPU bottleneck. This may be observed when the throughput scales sub-linearly with the number of data-parallel replicas (i.e. when doubling the IPU count, the performance does not double). This can also be verified by profiling the application and observing a significant proportion of runtime spent on host CPU workload.</p> <p>In this case, Poprun can be used launch multiple instances. As an example, we will save the following .yaml configuratoin and run:</p> <pre><code>apiVersion: graphcore.ai/v1alpha1\nkind: IPUJob\nmetadata:\n  generateName: bert-poprun-64ipus-\nspec:\n  jobInstances: 1\n  modelReplicasPerWorker: \"16\"\n  ipusPerJobInstance: \"64\"\n  workers:\n    template:\n      spec:\n        containers:\n        - name: bert-poprun-64ipus\n          image: graphcore/pytorch:3.3.0\n          command: [/bin/bash, -c, --]\n          args:\n            - |\n              cd ;\n              mkdir build;\n              cd build ;\n              git clone https://github.com/graphcore/examples.git;\n              cd examples/nlp/bert/pytorch;\n              apt update ;\n              apt upgrade -y;\n              DEBIAN_FRONTEND=noninteractive TZ='Europe/London' apt install $(&lt; required_apt_packages.txt) -y ;\n              pip3 install -r requirements.txt ;\n              OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1 OMPI_ALLOW_RUN_AS_ROOT=1 \\\n              poprun \\\n              --allow-run-as-root 1 \\\n              --vv \\\n              --num-instances 1 \\\n              --num-replicas 16 \\\n               --mpi-global-args=\"--tag-output\" \\\n              --ipus-per-replica 4 \\\n              python3 run_pretraining.py \\\n              --config pretrain_large_128_POD64 \\\n              --dataset generated --training-steps 1\n          resources:\n            limits:\n              cpu: 32\n              memory: 200Gi\n          securityContext:\n            capabilities:\n              add:\n              - IPC_LOCK\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: devshm\n        restartPolicy: Never\n        hostIPC: true\n        volumes:\n        - emptyDir:\n            medium: Memory\n            sizeLimit: 10Gi\n          name: devshm\n</code></pre> <p>Inspecting the log via <code>kubectl logs &lt;pod-name&gt;</code> should produce:</p> <pre><code>...\n ===========================================================================================\n|                                      poprun topology                                      |\n|===========================================================================================|\n10:10:50.154 1 POPRUN [D] Done polling, final state of p-bert-poprun-64ipus-gc-dev-0: PS_ACTIVE\n10:10:50.154 1 POPRUN [D] Target options from environment: {}\n| hosts     |                                   localhost                                   |\n|-----------|-------------------------------------------------------------------------------|\n| ILDs      |                                       0                                       |\n|-----------|-------------------------------------------------------------------------------|\n| instances |                                       0                                       |\n|-----------|-------------------------------------------------------------------------------|\n| replicas  | 0  | 1  | 2  | 3  | 4  | 5  | 6  | 7  | 8  | 9  | 10 | 11 | 12 | 13 | 14 | 15 |\n -------------------------------------------------------------------------------------------\n10:10:50.154 1 POPRUN [D] Target options from V-IPU partition: {\"ipuLinkDomainSize\":\"64\",\"ipuLinkConfiguration\":\"slidingWindow\",\"ipuLinkTopology\":\"torus\",\"gatewayMode\":\"true\",\"instanceSize\":\"64\"}\n10:10:50.154 1 POPRUN [D] Using target options: {\"ipuLinkDomainSize\":\"64\",\"ipuLinkConfiguration\":\"slidingWindow\",\"ipuLinkTopology\":\"torus\",\"gatewayMode\":\"true\",\"instanceSize\":\"64\"}\n10:10:50.203 1 POPRUN [D] No hosts specified; ignoring host-subnet setting\n10:10:50.203 1 POPRUN [D] Default network/RNIC for host communication: None\n10:10:50.203 1 POPRUN [I] Running command: /opt/poplar/bin/mpirun '--tag-output' '--bind-to' 'none' '--tag-output'\n'--allow-run-as-root' '-np' '1' '-x' 'POPDIST_NUM_TOTAL_REPLICAS=16' '-x' 'POPDIST_NUM_IPUS_PER_REPLICA=4' '-x'\n'POPDIST_NUM_LOCAL_REPLICAS=16' '-x' 'POPDIST_UNIFORM_REPLICAS_PER_INSTANCE=1' '-x' 'POPDIST_REPLICA_INDEX_OFFSET=0' '-x'\n'POPDIST_LOCAL_INSTANCE_INDEX=0' '-x' 'IPUOF_VIPU_API_HOST=10.21.21.129' '-x' 'IPUOF_VIPU_API_PORT=8090' '-x'\n'IPUOF_VIPU_API_PARTITION_ID=p-bert-poprun-64ipus-gc-dev-0' '-x' 'IPUOF_VIPU_API_TIMEOUT=120' '-x' 'IPUOF_VIPU_API_GCD_ID=0'\n'-x' 'IPUOF_LOG_LEVEL=WARN' '-x' 'PATH' '-x' 'LD_LIBRARY_PATH' '-x' 'PYTHONPATH' '-x' 'POPLAR_TARGET_OPTIONS=\n{\"ipuLinkDomainSize\":\"64\",\"ipuLinkConfiguration\":\"slidingWindow\",\"ipuLinkTopology\":\"torus\",\"gatewayMode\":\"true\",\n\"instanceSize\":\"64\"}' 'python3' 'run_pretraining.py' '--config' 'pretrain_large_128_POD64' '--dataset' 'generated' '--training-steps' '1'\n10:10:50.204 1 POPRUN [I] Waiting for mpirun (PID 4346)\n[1,0]&lt;stderr&gt;:    Registered metric hook: total_compiling_time with object: &lt;function get_results_for_compile_time at 0x7fe0a6e8af70&gt;\n[1,0]&lt;stderr&gt;:Using config: pretrain_large_128_POD64\n...\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [10:11&lt;00:00][1,0]&lt;stderr&gt;:\n[1,0]&lt;stderr&gt;:Compiled/Loaded model in 683.6591004971415 secs\n[1,0]&lt;stderr&gt;:-----------------------------------------------------------\n[1,0]&lt;stderr&gt;:--------------------- Training Started --------------------\nStep: 0 / 0 - LR: 0.00e+00 - total loss: 11.260 - mlm_loss: 10.397 - nsp_loss: 0.863 - mlm_acc: 0.000 % - nsp_acc: 0.052 %:   0%|          | 0/1 [00:03&lt;?, ?itStep: 0 / 0 - LR: 0.00e+00 - total loss: 11.260 - mlm_loss: 10.397 - nsp_loss: 0.863 - mlm_acc: 0.000 % - nsp_acc: 0.052 %:   0%|          | 0/1 [00:03&lt;?, ?itStep: 0 / 0 - LR: 0.00e+00 - total loss: 11.260 - mlm_loss: 10.397 - nsp_loss: 0.863 - mlm_acc: 0.000 % - nsp_acc: 0.052 %:   0%|          | 0/1 [00:03&lt;?, ?it/s, throughput: 17692.1 samples/sec][1,0]&lt;stderr&gt;:\n[1,0]&lt;stderr&gt;:-----------------------------------------------------------\n[1,0]&lt;stderr&gt;:-------------------- Training Metrics ---------------------\n[1,0]&lt;stderr&gt;:global_batch_size: 65536\n[1,0]&lt;stderr&gt;:device_iterations: 1\n[1,0]&lt;stderr&gt;:training_steps: 1\n[1,0]&lt;stderr&gt;:Training time: 3.718 secs\n[1,0]&lt;stderr&gt;:-----------------------------------------------------------\n</code></pre>"},{"location":"services/graphcore/training/L2_multiple_IPU/#notes-on-using-the-examples-respository","title":"Notes on using the examples respository","text":"<p>Graphcore provides examples of a variety of models on Github https://github.com/graphcore/examples. When following the instructions, note that since we are using a container within a Kubernetes pod, there is no need to enable the Poplar/PopART SDK, set up a virtual python environment, or install the PopTorch wheel.</p>"},{"location":"services/graphcore/training/L3_profiling/","title":"Profiling with PopVision","text":"<p>Graphcore provides various tools for profiling, debugging, and instrumenting programs run on IPUs. In this tutorial we will briefly demonstrate an example using the PopVision Graph Analyser. For more information, see Profiling and Debugging and PopVision Graph Analyser User Guide.</p> <p>We will reuse the same PyTorch MNIST example from lesson 1 (from https://github.com/graphcore/examples/tree/master/tutorials/simple_applications/pytorch/mnist).</p> <p>To enable profiling and create IPU reports, we need to add the following line to the training script <code>mnist_poptorch_code_only.py</code> :</p> <pre><code>training_opts = training_opts.enableProfiling()\n</code></pre> <p>(for details the API, see API reference)</p> <p>Save and run <code>kubectl create -f &lt;yaml-file&gt;</code> on the following:</p> <pre><code>apiVersion: graphcore.ai/v1alpha1\nkind: IPUJob\nmetadata:\n  generateName: mnist-training-profiling-\nspec:\n  jobInstances: 1\n  ipusPerJobInstance: \"1\"\n  workers:\n    template:\n      spec:\n        containers:\n        - name: mnist-training-profiling\n          image: graphcore/pytorch:3.3.0\n          command: [/bin/bash, -c, --]\n          args:\n            - |\n              cd;\n              mkdir build;\n              cd build;\n              git clone https://github.com/graphcore/examples.git;\n              cd examples/tutorials/simple_applications/pytorch/mnist;\n              python -m pip install -r requirements.txt;\n              sed -i '131i training_opts = training_opts.enableProfiling()' mnist_poptorch_code_only.py;\n              python mnist_poptorch_code_only.py --epochs 1;\n              echo 'RUNNING ls ./training';\n              ls training\n          resources:\n            limits:\n              cpu: 32\n              memory: 200Gi\n          securityContext:\n            capabilities:\n              add:\n              - IPC_LOCK\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: devshm\n        restartPolicy: Never\n        hostIPC: true\n        volumes:\n        - emptyDir:\n            medium: Memory\n            sizeLimit: 10Gi\n          name: devshm\n</code></pre> <p>After completion, using <code>kubectl logs &lt;pod-name&gt;</code>, we can see the following result</p> <pre><code>...\nAccuracy on test set: 96.69%\nRUNNING ls ./training\narchive.a\nprofile.pop\n</code></pre> <p>We can see that the training has created two Poplar report files: <code>archive.a</code> which is an archive of the ELF executable files, one for each tile; and <code>profile.pop</code>, the poplar profile, which contains compile-time and execution information about the Poplar graph.</p>"},{"location":"services/graphcore/training/L3_profiling/#downloading-the-profile-reports","title":"Downloading the profile reports","text":"<p>To download the traing profiles to your local environment, you can use <code>kubectl cp</code>. For example, run</p> <pre><code>kubectl cp &lt;pod-name&gt;:/root/build/examples/tutorials/simple_applications/pytorch/mnist/training .\n</code></pre> <p>Once you have downloaded the profile report files, you can view the contents locally using the PopVision Graph Analyser tool, which is available for download here https://www.graphcore.ai/developer/popvision-tools.</p> <p>From the Graph Analyser, you can analyse information including memory usage, execution trace and more.</p>"},{"location":"services/graphcore/training/L4_other_frameworks/","title":"Other Frameworks","text":"<p>In this tutorial we'll briefly cover running tensorflow and PopART for Machine Learning, and writing IPU programs directly via the PopLibs library in C++. Extra links and resources will be provided for more in-depth information.</p>"},{"location":"services/graphcore/training/L4_other_frameworks/#terminology","title":"Terminology","text":"<p>Within Graphcore, <code>Poplar</code> refers to the tools (e.g. Poplar <code>Graph Engine</code> or Poplar <code>Graph Compiler</code>) and libraries (<code>PopLibs</code>)  for programming on IPUs.</p> <p>The <code>Poplar SDK</code> is a package of software development tools, including</p> <ul> <li>TensorFlow 1 and 2 for the IPU</li> <li>PopTorch (Wrapper around PyTorch for running on IPU)</li> <li>PopART (Poplar Advanced Run-Time, provides support for importing, creating, and running ONNX graphs on the IPU)</li> <li>Poplar and PopLibs</li> <li>PopDist (Poplar Distributed Configuration Library) and PopRun (Command line utility to launch distributed applications)</li> <li>Device drivers and command line tools for managing the IPU</li> </ul> <p>For more details see here.</p>"},{"location":"services/graphcore/training/L4_other_frameworks/#other-ml-frameworks-tensorflow-and-popart","title":"Other ML frameworks: Tensorflow and PopART","text":"<p>Besides being able to run PyTorch code, as demonstrated in the previous lessons, the Poplar SDK also supports running ML learning applications with tensorflow or PopART.</p>"},{"location":"services/graphcore/training/L4_other_frameworks/#tensorflow","title":"Tensorflow","text":"<p>The Poplar SDK includes implementation of TensorFlow and Keras for the IPU.</p> <p>For more information, refer to Targeting the IPU from TensorFlow 2 and TensorFlow 2 Quick Start.</p> <p>These are available from the image <code>graphcore/tensorflow:2</code>.</p> <p>For a quick example, we will run an example script from https://github.com/graphcore/examples/tree/master/tutorials/simple_applications/tensorflow2/mnist. To get started, save the following yaml and run <code>kubectl create -f &lt;yaml-file&gt;</code> to create the IPUJob:</p> <pre><code>apiVersion: graphcore.ai/v1alpha1\nkind: IPUJob\nmetadata:\n  generateName: tensorflow-example-\nspec:\n  jobInstances: 1\n  ipusPerJobInstance: \"1\"\n  workers:\n    template:\n      spec:\n        containers:\n        - name: tensorflow-example\n          image: graphcore/tensorflow:2\n          command: [/bin/bash, -c, --]\n          args:\n            - |\n              apt update;\n              apt upgrade -y;\n              apt install git -y;\n              cd;\n              mkdir build;\n              cd build;\n              git clone https://github.com/graphcore/examples.git;\n              cd examples/tutorials/simple_applications/tensorflow2/mnist;\n              python -m pip install -r requirements.txt;\n              python mnist_code_only.py --epochs 1\n          resources:\n            limits:\n              cpu: 32\n              memory: 200Gi\n          securityContext:\n            capabilities:\n              add:\n              - IPC_LOCK\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: devshm\n        restartPolicy: Never\n        hostIPC: true\n        volumes:\n        - emptyDir:\n            medium: Memory\n            sizeLimit: 10Gi\n          name: devshm\n</code></pre> <p>Running <code>kubectl logs &lt;pod&gt;</code> should show the results similar to the following</p> <pre><code>...\n2023-10-25 13:21:40.263823: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 3.2.0 (1513789a51) Poplar package: b82480c629\n2023-10-25 13:21:42.203515: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1619] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n11501568/11490434 [==============================] - 0s 0us/step\n2023-10-25 13:21:43.789573: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2023-10-25 13:21:44.164207: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2023-10-25 13:21:57.935339: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nEpoch 1/4\n2000/2000 [==============================] - 17s 8ms/step - loss: 0.6188\nEpoch 2/4\n2000/2000 [==============================] - 1s 427us/step - loss: 0.3330\nEpoch 3/4\n2000/2000 [==============================] - 1s 371us/step - loss: 0.2857\nEpoch 4/4\n2000/2000 [==============================] - 1s 439us/step - loss: 0.2568\n</code></pre>"},{"location":"services/graphcore/training/L4_other_frameworks/#popart","title":"PopART","text":"<p>The Poplar Advanced Run Time (PopART) enables importing and constructing ONNX graphs, and running graphs in inference, evaluation or training modes. PopART provides both a C++ and Python API.</p> <p>For more information, see the PopART User Guide</p> <p>PopART is available from the image <code>graphcore/popart</code>.</p> <p>For a quick example, we will run an example script from https://github.com/graphcore/tutorials/tree/sdk-release-3.1/simple_applications/popart/mnist. To get started, save the following yaml and run <code>kubectl create -f &lt;yaml-file&gt;</code> to create the IPUJob:</p> <pre><code>apiVersion: graphcore.ai/v1alpha1\nkind: IPUJob\nmetadata:\n  generateName: popart-example-\nspec:\n  jobInstances: 1\n  ipusPerJobInstance: \"1\"\n  workers:\n    template:\n      spec:\n        containers:\n        - name: popart-example\n          image: graphcore/popart:3.3.0\n          command: [/bin/bash, -c, --]\n          args:\n            - |\n              cd ;\n              mkdir build;\n              cd build ;\n              git clone https://github.com/graphcore/tutorials.git;\n              cd tutorials;\n              git checkout sdk-release-3.1;\n              cd simple_applications/popart/mnist;\n              python3 -m pip install -r requirements.txt;\n              ./get_data.sh;\n              python3 popart_mnist.py --epochs 1\n          resources:\n            limits:\n              cpu: 32\n              memory: 200Gi\n          securityContext:\n            capabilities:\n              add:\n              - IPC_LOCK\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: devshm\n        restartPolicy: Never\n        hostIPC: true\n        volumes:\n        - emptyDir:\n            medium: Memory\n            sizeLimit: 10Gi\n          name: devshm\n</code></pre> <p>Running <code>kubectl logs &lt;pod&gt;</code> should show the results similar to the following</p> <pre><code>...\nCreating ONNX model.\nCompiling the training graph.\nCompiling the validation graph.\nRunning training loop.\nEpoch #1\n   Loss=16.2605\n   Accuracy=88.88%\n</code></pre>"},{"location":"services/graphcore/training/L4_other_frameworks/#writing-ipu-programs-directly-with-poplibs","title":"Writing IPU programs directly with PopLibs","text":"<p>The Poplar libraries are a set of C++ libraries consisting of the Poplar graph library and the open-source PopLibs libraries.</p> <p>The Poplar graph library provides direct access to the IPU by code written in C++. You can write complete programs using Poplar, or use it to write functions to be called from your application written in a higher-level framework such as TensorFlow.</p> <p>The PopLibs libraries are a set of application libraries that implement operations commonly required by machine learning applications, such as linear algebra operations, element-wise tensor operations, non-linearities and reductions. These provide a fast and easy way to create programs that run efficiently using the parallelism of the IPU.</p> <p>For more information, see Poplar Quick Start and Poplar and PopLibs User Guide.</p> <p>These are available from the image <code>graphcore/poplar</code>.</p> <p>When using the PopLibs libraries, you will have to include the include files in the <code>include/popops</code> directory, e.g.</p> <pre><code>#include &lt;include/popops/ElementWise.hpp&gt;\n</code></pre> <p>and to link the relevant PopLibs libraries, in addition to the Poplar library, e.g.</p> <pre><code>g++ -std=c++11 my-program.cpp -lpoplar -lpopops\n</code></pre> <p>For a quick example, we will run an example from https://github.com/graphcore/examples/tree/master/tutorials/simple_applications/poplar/mnist. To get started, save the following yaml and run <code>kubectl create -f &lt;yaml-file&gt;</code> to create the IPUJob:</p> <pre><code>apiVersion: graphcore.ai/v1alpha1\nkind: IPUJob\nmetadata:\n  generateName: poplib-example-\nspec:\n  jobInstances: 1\n  ipusPerJobInstance: \"1\"\n  workers:\n    template:\n      spec:\n        containers:\n        - name: poplib-example\n          image: graphcore/poplar:3.3.0\n          command: [\"bash\"]\n          args: [\"-c\", \"cd &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; git clone https://github.com/graphcore/examples.git &amp;&amp; cd examples/tutorials/simple_applications/poplar/mnist/ &amp;&amp; ./get_data.sh &amp;&amp; make &amp;&amp;  ./regression-demo -IPU 1 50\"]\n          resources:\n            limits:\n              cpu: 32\n              memory: 200Gi\n          securityContext:\n            capabilities:\n              add:\n              - IPC_LOCK\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: devshm\n        restartPolicy: Never\n        hostIPC: true\n        volumes:\n        - emptyDir:\n            medium: Memory\n            sizeLimit: 10Gi\n          name: devshm\n</code></pre> <p>Running <code>kubectl logs &lt;pod&gt;</code> should show the results similar to the following</p> <pre><code>...\nUsing the IPU\nTrying to attach to IPU\nAttached to IPU 0\nTarget:\n  Number of IPUs:         1\n  Tiles per IPU:          1,472\n  Total Tiles:            1,472\n  Memory Per-Tile:        624.0 kB\n  Total Memory:           897.0 MB\n  Clock Speed (approx):   1,850.0 MHz\n  Number of Replicas:     1\n  IPUs per Replica:       1\n  Tiles per Replica:      1,472\n  Memory per Replica:     897.0 MB\n\nGraph:\n  Number of vertices:            5,466\n  Number of edges:              16,256\n  Number of variables:          41,059\n  Number of compute sets:           20\n\n...\n\nEpoch 1 (99%), accuracy 76%\n</code></pre>"},{"location":"services/jhub/","title":"EIDF Notebook Service","text":"<p>The EIDF Notebook Service is a scalable Jupyterhub deployment in the EIDF Data Science Cloud.</p> <p>The Notebook Service is open to all EIDF users and offers a selection of data science environments and user interfaces, including Jupyter notebooks, Jupyter Lab and RStudio.</p> <p>Follow Quickstart to start using the EIDF Notebook Service.</p>"},{"location":"services/jhub/quickstart/","title":"Quickstart","text":""},{"location":"services/jhub/quickstart/#accessing","title":"Accessing","text":"<p>Access the EIDF Notebooks in your browser by opening https://notebook.eidf.ac.uk/. You must be a member of an active EIDF project and have a user account to use the EIDF Notebook Service.</p> <p></p> <p>Click on \"Sign In with SAFE\". You will be redirected to the SAFE login page.</p> <p>Log into the SAFE if you're not logged in already. If you have more than one account you will be presented with the form \"Approve Token\" and a choice of user accounts for the Notebook Service. This account is the user in your notebooks and you can share data with your DSC VMs within the same project.</p> <p>Select the account you would like to use from the dropdown \"User Account\" at the end of the form. Then press \"Accept\" to return to the EIDF Notebook Service where you can select a server environment.</p> <p></p> <p>Select the environment that you would like to use for your notebooks and press \"Start\". Now your notebook container will be launched. This may take a little while.</p> <p></p>"},{"location":"services/jhub/quickstart/#first-notebook","title":"First Notebook","text":"<p>You will be presented with the JupyterLab dashboard view when the container has started.</p> <p></p> <p>The availability of launchers depends on the environment that you selected.</p> <p>For example launch a Python 3 notebook or an R notebook from the dashboard. You can also launch a terminal session.</p>"},{"location":"services/jhub/quickstart/#python-packages","title":"Python packages","text":"<p>Note that Python packages are installed into the system space of your container by default. However this means that they are not available after a restart of your notebook container which may happen when your session was idle for a while. We recommend specifying <code>--user</code> to install packages into your user directory to preserve installations across sessions.</p> <p>To install python packages in a notebook use the command:</p> <pre><code>!pip install &lt;package&gt; --user\n</code></pre> <p>or run the command in a terminal:</p> <pre><code>pip install &lt;package&gt; --user\n</code></pre>"},{"location":"services/jhub/quickstart/#data","title":"Data","text":"<p>There is a project space mounted in <code>/project_data</code>. Only project accounts have permissions to view and write to their project folder in this space. Here you can share data with other notebook users in your project. Data placed in <code>/project_data/shared</code> is shared with other notebook users outside your project.</p> <p>You can also share data with DSC VMs in your project. Please contact the helpdesk if you would like to mount this project space to one of your VMs.</p>"},{"location":"services/jhub/quickstart/#limits","title":"Limits","text":"<p>Note that there are limited amounts of memory and cores available per user. Users do not have sudo permissions in the containers so you cannot install any system packages.</p> <p>Currently there is no access to GPUs. You can submit jobs to the EIDF GPU Service but you cannot run your notebooks on a GPU.</p>"},{"location":"services/mft/","title":"MFT","text":""},{"location":"services/mft/quickstart/","title":"Managed File Transfer","text":""},{"location":"services/mft/quickstart/#getting-to-the-mft","title":"Getting to the MFT","text":"<p>The EIDF MFT can be accessed at https://eidf-mft.epcc.ed.ac.uk</p>"},{"location":"services/mft/quickstart/#how-it-works","title":"How it works","text":"<p>The MFT provides a 'drop zone' for the project. All users in a given project will have access to the same shared transfer area. They will have the ability to upload, download, and delete files from the project's transfer area. This area is linked to a directory within the projects space on the shared backend storage.</p> <p>Files which are uploaded are owned by the Linux user 'nobody' and the group ID of whatever project the file is being uploaded to. They have the permissions:  Owner = rw  Group =   r  Others = r</p> <p>Once the file is opened on the VM, the user that opened it will become the owner and they can make further changes.</p>"},{"location":"services/mft/quickstart/#gaining-access-to-the-mft","title":"Gaining access to the MFT","text":"<p>By default a project won't have access to the MFT, this has to be enabled. Currently this can be done by the PI sending a request to the EIDF Helpdesk. Once the project is enabled within the MFT, every user with the project will be able to log into the MFT using their usual EIDF credentials.</p> <p>Once MFT access has been enabled for a project, PIs can give a project user access to the MFT. A new 'eidf-mft' machine option will be available for each user within the portal, which the PI can select to grant the user access to the MFT.</p>"},{"location":"services/mft/using-the-mft/","title":"Using the MFT Web Portal","text":""},{"location":"services/mft/using-the-mft/#logging-in-to-the-web-browser","title":"Logging in to the web browser","text":"<p>When you reach the MFT home page you can log in using your usual VM project credentials.</p> <p>You will then be asked what type of session you would like to start. Select New Web Client or Web Client and continue.</p>"},{"location":"services/mft/using-the-mft/#file-ingress","title":"File Ingress","text":"<p>Once logged in, all files currently in the projects transfer directory will be displayed. Click the 'Upload' button under the 'Home' title to open the dialogue for file upload. You can then drag and drop files in, or click 'Browse' to find them locally.</p> <p>Once uploaded, the file will be immediately accessible from the project area, and can be used within any EIDF service which has the filesystem mounted.</p>"},{"location":"services/mft/using-the-mft/#file-egress","title":"File Egress","text":"<p>File egress can be done in the reverse way. By placing the file into the project transfer directory, it will become available in the MFT portal.</p>"},{"location":"services/mft/using-the-mft/#file-management","title":"File Management","text":"<p>Directories can be created within the project transfer directory, for example with 'Import' and 'Export' to allow for better file management. Files deleted from either the MFT portal or from the VM itself will remove it from the other, as both locations point at the same file. It's only stored in one place, so modifications made from either place will remove the file.</p>"},{"location":"services/mft/using-the-mft/#sftp","title":"SFTP","text":"<p>Once a project and user have access to the MFT, they can connect to it using SFTP as well as through the web browser.</p> <p>This can be done by logging into the MFT URL with the user's project account:</p> <pre><code>    sftp [EIDF username]@eidf-mft.epcc.ed.ac.uk\n</code></pre>"},{"location":"services/mft/using-the-mft/#scp","title":"SCP","text":"<p>Files can be scripted to be upload to the MFT using SCP.</p> <p>To copy a file to the project MFT area using SCP:</p> <pre><code>    scp /path/to/file [EIDF username]@eidf-mft.epcc.ed.ac.uk:/\n</code></pre>"},{"location":"services/s3/","title":"Overview","text":"<p>The EIDF S3 Service is an object store with an interface that is compatible with a subset of the Amazon S3 RESTful API.</p>"},{"location":"services/s3/#service-access","title":"Service Access","text":"<p>Users should have an EIDF account as described in EIDF Accounts.</p> <p>Project leads can request an object store allocation through a request to the EIDF helpdesk.</p>"},{"location":"services/s3/#access-keys","title":"Access keys","text":"<p>Select your project at https://portal.eidf.ac.uk/project/. Your access keys are displayed in the table at the top of the page.</p> <p></p> <p>For each account, the quota and the number of buckets that it is permitted to create is shown, as well as the access keys. Click on \"Secret\" to view the access secret. You will need the access key, the corresponding access secret and the endpoint <code>https://s3.eidf.ac.uk</code> to connect to the EIDF S3 Service with an S3 client.</p>"},{"location":"services/s3/#further-information","title":"Further information","text":"<p>Access management: Project management guide to managing accounts and access permissions for your S3 allocation.</p> <p>Tutorial: Examples using EIDF S3</p>"},{"location":"services/s3/manage/","title":"Manage EIDF S3 access","text":"<p>Access keys and accounts for the object store are managed by project managers via the EIDF Portal.</p>"},{"location":"services/s3/manage/#request-an-allocation","title":"Request an allocation","text":"<p>An object store allocation for a project may be requested by contacting the EIDF helpdesk.</p>"},{"location":"services/s3/manage/#object-store-accounts","title":"Object store accounts","text":"<p>Select your project at https://portal.eidf.ac.uk/project/ and jump to \"S3 Allocation\" on the project page to manage access keys and accounts.</p> <p></p> <p>S3 buckets and objects are owned by an account. Each account has a quota for storage and the number of buckets that it can create. The sum of all account quotas is limited by the total storage quota of the project object store allocation shown at the top.</p> <p>An account with the minimum storage quota (1B) and zero buckets is effectively read only as it may not create new buckets and so cannot upload files.</p> <p>To create an account:</p> <ol> <li>On the project page scroll to \"S3 Allocation\"</li> <li>Click \"Add Account\"</li> <li>Enter:<ul> <li>an account name (letters, numbers and underscore <code>_</code> only)</li> <li>a description</li> <li>a quota: a number with a unit B, kB, MiB (MB), GiB (GB), or TiB (TB), the minimum is 1B (1 Byte).</li> <li>the number of buckets that the account may create</li> </ul> </li> <li>Click \"Create Account\"</li> </ol> <p>You will not be allowed to create an account with the quota greater than the available storage quota of the project.</p> <p>It may take a little while for the account to become available. Refresh the project page to update the list of accounts.</p>"},{"location":"services/s3/manage/#access-keys","title":"Access keys","text":"<p>To use S3 (listing or creating buckets, listing objects or uploading and downloading files) you need an access key and a secret. An account can own any number of access keys. These keys share the account's quota and have access to the same buckets.</p> <p>To create an access key:</p> <ol> <li>Select an account and click \"Add Key\"</li> <li>Click \"Create Access Key\"</li> </ol> <p>It can take a little while for the access keys to become available. Refresh the project page to update the list of keys.</p>"},{"location":"services/s3/manage/#access-key-permissions","title":"Access key permissions","text":"<p>You can control which project members are allowed to view an access key and secret in the EIDF Portal or the SAFE. Project managers and the PI have access to all S3 accounts and can view associated access keys and secrets in the project management view.</p> <p>To grant view permissions for an access key to a project member:</p> <ol> <li>Click on the \"Edit\" icon next to the key.</li> <li>Select the project members that will have view permissions for this access key.</li> <li>Press \"Update Permissions\"</li> </ol> <p>It can take a little while for the permissions update to complete.</p> <p>Note</p> <p>Anyone who knows an access key and secret will be able to perform the associated activities via the S3 API regardless of the view permissions.</p>"},{"location":"services/s3/manage/#delete-an-access-key","title":"Delete an access key","text":"<p>Click on the \"Bin\" icon next to a key and press \"Delete\" on the form.</p>"},{"location":"services/s3/tutorial/","title":"Tutorial","text":""},{"location":"services/s3/tutorial/#aws-cli","title":"AWS CLI","text":"<p>The following examples use the AWS Command Line Interface (AWS CLI) to connect to EIDF S3.</p>"},{"location":"services/s3/tutorial/#setup","title":"Setup","text":"<p>Install with pip</p> <pre><code>python -m pip install awscli\n</code></pre> <p>Installers are available for various platforms if you are not using Python: see https://aws.amazon.com/cli/</p>"},{"location":"services/s3/tutorial/#configure","title":"Configure","text":"<p>Set your access key and secret as environment variables or configure a credentials file at <code>~/.aws/credentials</code> on Linux or <code>%USERPROFILE%\\.aws\\credentials</code> on Windows.</p> <p>Credentials file:</p> <pre><code>[default]\naws_access_key_id=&lt;key&gt;\naws_secret_access_key=&lt;secret&gt;\nendpoint_url=https://s3.eidf.ac.uk\n\nrequest_checksum_calculation=when_required\nresponse_checksum_validation=when_required\n</code></pre> <p>The last two lines are required since boto3 version 1.36 when a breaking change was introduced that adopts new default integrity protections which is not currently supported by EIDF S3.</p> <p>Environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=&lt;key&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;secret&gt;\nexport AWS_ENDPOINT_URL=https://s3.eidf.ac.uk\n</code></pre>"},{"location":"services/s3/tutorial/#commands","title":"Commands","text":"<p>List the buckets in your account:</p> <pre><code>aws s3 ls\n</code></pre> <p>Create a bucket:</p> <pre><code>aws s3api create-bucket --bucket &lt;bucketname&gt;\n</code></pre> <p>Upload a file:</p> <pre><code>aws s3 cp &lt;filename&gt; s3://&lt;bucketname&gt;\n</code></pre> <p>Check that the file above was uploaded successfully by listing objects in the bucket:</p> <pre><code>aws s3 ls s3://&lt;bucketname&gt;\n</code></pre> <p>To read from a public bucket without providing credentials, add the option <code>--no-sign-request</code> to the call:</p> <pre><code>aws s3 ls s3://&lt;bucketname&gt; --no-sign-request\n</code></pre> <p>You can get help on the options for any command using:</p> <pre><code>aws s3 help\n</code></pre> <p>or for particular commands</p> <pre><code>aws s3 ls help\n</code></pre> <p>Alternatively, there are many graphical clients that act as a file browser for S3, for example Cyberduck - to use Cyberduck you may need to enable connections using deprecated path style requests otherwise you may have problems listing the contents of a bucket. For more details please consult the Cyberduck documentation.</p> <p>If you want to use another programming language have a look at the Ceph S3 API interfaces (Ceph is the underlying platform used).</p>"},{"location":"services/s3/tutorial/#examples","title":"Examples","text":"<p>You want to upload all the files in a subdirectory to your S3 bucket</p> <pre><code>aws s3 cp ./mydir s3://mybucket --recursive --exclude \"*\" --include \"*.dat\"\n</code></pre> <p>Here all <code>*.dat</code>  files only in <code>mydir</code> will be uploaded to <code>s3://mybucket</code>.</p> <p>You can check your upload using:</p> <pre><code>aws s3 ls --summarize --human-readable --recursive s3://mybucket/\n</code></pre> <p>For public S3 buckets, such as those provided for the data publishing service,  you can construct a downloadable https link  to download files from an S3 link, e.g. taking:</p> <pre><code>s3://eidfXXX-my-dataset/mydatafile.csv\n</code></pre> <p>and by making the following transformation:</p> <pre><code>https://s3.eidf.ac.uk/eidfXXX-my-dataset/mydatafile.csv\n</code></pre> <p>Now you can open this link in a browser to download the file.</p> <p>Alternatively, you can use the aws client to download an entire data set:</p> <pre><code>aws s3 cp --recursive s3://eidfXXX-my-dataset/ ./mydataset --no-sign-request\n</code></pre> <p>will copy the entire content of the S3 bucket to your <code>mydataset</code> subdirectory. Note that you must use <code>--no-sign-request</code> when accessing public buckets.</p>"},{"location":"services/s3/tutorial/#python-using-boto3","title":"Python using <code>boto3</code>","text":"<p>The following examples use the Python library <code>boto3</code>.</p>"},{"location":"services/s3/tutorial/#install","title":"Install","text":"<p>Installation:</p> <pre><code>python -m pip install boto3\n</code></pre>"},{"location":"services/s3/tutorial/#usage","title":"Usage","text":""},{"location":"services/s3/tutorial/#connect","title":"Connect","text":"<p>Create an S3 client resource:</p> <pre><code>import boto3\ns3 = boto3.resource('s3')\n</code></pre>"},{"location":"services/s3/tutorial/#list-buckets","title":"List buckets","text":"<pre><code>for bucket in s3.buckets.all():\n    print(f'{bucket.name}')\n</code></pre>"},{"location":"services/s3/tutorial/#list-objects-in-a-bucket","title":"List objects in a bucket","text":"<pre><code>bucket_name = 'somebucket'\nbucket = s3.Bucket(bucket_name)\nfor obj in bucket.objects.all():\n    print(f'{obj.key}')\n</code></pre>"},{"location":"services/s3/tutorial/#uploading-files-to-a-bucket","title":"Uploading files to a bucket","text":"<pre><code>bucket = s3.Bucket(bucket_name)\nbucket.upload_file('./somedata.csv', 'somedata.csv')\n</code></pre> <p>In boto3 version 1.36, a breaking change was introduced that adopts new default integrity protections which is not currently supported by EIDF S3 (see https://github.com/boto/boto3/issues/4392). If you see this error</p> <pre><code>botocore.exceptions.ClientError: An error occurred (XAmzContentSHA256Mismatch) when calling the PutObject operation: None\n</code></pre> <p>use the following configuration for your client:</p> <pre><code>from botocore.config import Config\nconfig = Config(\n    request_checksum_calculation=\"when_required\",\n    response_checksum_validation=\"when_required\",\n)\ns3 = boto3.resource('s3', config=config)\n</code></pre> <p>Then upload your files as shown above.</p>"},{"location":"services/s3/tutorial/#accessing-buckets-in-other-projects","title":"Accessing buckets in other projects","text":"<p>Buckets owned by an EIDF project are placed in a tenancy in the EIDF S3 Service. The project code is a prefix on the bucket name, separated by a colon (<code>:</code>), for example <code>eidfXX1:somebucket</code>.</p> <p>This is only relevant when accessing buckets outside your project tenancy - if you access buckets in your own project you can ignore this section.</p> <p>By default, the <code>boto3</code> Python library raises an error that bucket names with a colon <code>:</code> (as used by the EIDF S3 Service) are invalid.</p> <p>When accessing a bucket with the project code prefix, switch off the bucket name validation:</p> <pre><code>import boto3\nfrom botocore.handlers import validate_bucket_name\n\ns3 = boto3.resource('s3', endpoint_url='https://s3.eidf.ac.uk')\ns3.meta.client.meta.events.unregister('before-parameter-build.s3', validate_bucket_name)\n</code></pre>"},{"location":"services/s3/tutorial/#access-policies","title":"Access policies","text":"<p>Buckets owned by an EIDF project are placed in a tenancy in the EIDF S3 Service. The project code is a prefix on the bucket name, separated by a colon (<code>:</code>), for example <code>eidfXX1:somebucket</code>. Note that some S3 client libraries do not accept bucket names in this format.</p> <p>Bucket permissions use IAM (Identity Access Management) policies. You can grant other accounts (within the same project or from other projects) read or write access to your buckets.</p> <p>For example to grant permissions to put, get, delete and list objects in bucket <code>eidfXX1:somebucket</code> to the account <code>account2</code> in project <code>eidfXX2</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAccessToBucket\",\n            \"Principal\": {\n              \"AWS\": [\n                \"arn:aws:iam::eidfXX2:user/account2\",\n              ]\n            },\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\",\n                \"s3:DeleteObject\",\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::/*\",\n                \"arn:aws:s3::eidfXX1:somebucket\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>You can chain multiple policies in the statement array:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Principal\": { ... }\n            \"Effect\": \"Allow\",\n            \"Action\": [ ... ],\n            \"Resource\": [ ... ]\n        },\n        {\n            \"Principal\": { ... }\n            \"Effect\": \"Allow\",\n            \"Action\": [ ... ],\n            \"Resource\": [ ... ]\n        }\n    ]\n}\n</code></pre> <p>Give public read access to a bucket (listing and downloading files):</p> <pre><code>{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n  {\n    \"Effect\": \"Allow\",\n    \"Principal\": \"*\",\n    \"Action\": [\"s3:ListBucket\"],\n    \"Resource\": [\n      f\"arn:aws:s3::eidfXX1:somebucket\"\n    ]\n  },\n  {\n    \"Effect\": \"Allow\",\n    \"Principal\": \"*\",\n    \"Action\": [\"s3:GetObject\"],\n    \"Resource\": [\n      f\"arn:aws:s3::eidfXX1:somebucket/*\"\n    ]\n   }\n ]\n}\n</code></pre>"},{"location":"services/s3/tutorial/#set-policy-using-aws-cli","title":"Set policy using AWS CLI","text":"<p>Grant permissions stored in an IAM policy file:</p> <pre><code>aws put-bucket-policy --bucket &lt;bucketname&gt; --policy \"$(cat bucket-policy.json)\"\n</code></pre>"},{"location":"services/s3/tutorial/#set-policy-using-python-boto3","title":"Set policy using Python <code>boto3</code>","text":"<p>Grant permissions to another account: In this example we grant <code>ListBucket</code> and <code>GetObject</code> permissions to account <code>account1</code> in project <code>eidfXX1</code> and <code>account2</code> in project <code>eidfXX2</code>.</p> <pre><code>import json\n\nbucket_policy = {\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n  {\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n      \"AWS\": [\n        \"arn:aws:iam::eidfXX1:user/account1\",\n        \"arn:aws:iam::eidfXX2:user/account2\",\n      ]\n    },\n    \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetObject\"\n    ],\n    \"Resource\": [\n      f\"arn:aws:s3::eidfXX1:{bucket_name}\"\n      f\"arn:aws:s3::eidfXX1:{bucket_name}/*\"\n    ]\n  }\n ]\n}\n\npolicy = bucket.Policy()\npolicy.put(Policy=json.dumps(bucket_policy))\n</code></pre>"},{"location":"services/ultra2/","title":"Ultra2 Large Memory System","text":"<p>Overview</p> <p>Connect</p> <p>Running jobs</p>"},{"location":"services/ultra2/access/","title":"Overview","text":"<p>Ultra2 is a single logical CPU system based at EPCC. It is suitable for running jobs which require large volumes of non-distributed memory (as opposed to a cluster).</p>"},{"location":"services/ultra2/access/#specifications","title":"Specifications","text":"<p>The system is a HPE SuperDome Flex containing 576 individual cores in a SMT-1 arrangement (1 thread per core). The system has 18TB of memory available to users. Home directories are network mounted from the EIDF e1000 Lustre filesystem, although some local NVMe storage is available for temporary file storage during runs.</p>"},{"location":"services/ultra2/access/#getting-access","title":"Getting Access","text":"<p>Access to Ultra2 is currently by arrangement with EIDF. Please apply for a project on the EIDF Portal.</p>"},{"location":"services/ultra2/connect/","title":"Login","text":"<p>The hostname for SSH access to the system is <code>ultra2.eidf.ac.uk</code></p>"},{"location":"services/ultra2/connect/#access-credentials","title":"Access credentials","text":"<p>To access Ultra2, you need to use two credentials: your SSH key pair protected by a passphrase and a Time-based one-time password (TOTP).</p>"},{"location":"services/ultra2/connect/#ssh-key","title":"SSH Key","text":"<p>You must upload the public part of your SSH key pair to the SAFE by following the instructions from the SAFE documentation</p>"},{"location":"services/ultra2/connect/#time-based-one-time-password-totp","title":"Time-based one-time password (TOTP)","text":"<p>You must set up your TOTP token by following the instructions from the SAFE documentation</p>"},{"location":"services/ultra2/connect/#ssh-login-example","title":"SSH Login example","text":"<p>To login to Ultra2, you will need to use the SSH Key and TOTP token as noted above. With the appropriate key loaded<code>ssh &lt;username&gt;@ultra2.eidf.ac.uk</code> will then prompt you, roughly once per day, for your TOTP code.</p>"},{"location":"services/ultra2/run/","title":"Running jobs","text":""},{"location":"services/ultra2/run/#software","title":"Software","text":""},{"location":"services/ultra2/run/#oneapi","title":"OneAPI","text":"<p>The primary HPC software provided is Intel's OneAPI suite containing mpi compilers and runtimes, debuggers and the vTune performance analyser. Standard GNU compilers are also available. The OneAPI suite can be loaded by sourcing the shell script:</p> <pre><code>source  /opt/intel/oneapi/setvars.sh\n</code></pre>"},{"location":"services/ultra2/run/#queue-system","title":"Queue system","text":"<p>All jobs must be run via SLURM to avoid inconveniencing other users of the system. Users should not run jobs directly. Note that the system has one logical processor with a large number of threads and thus appears to SLURM as a single node. This is intentional.</p>"},{"location":"services/ultra2/run/#queue-limits","title":"Queue limits","text":"<p>We kindly request that users limit their maximum total running job size to 288 cores and 4TB of memory, whether that be a divided into a single job, or a number of jobs. This may be enforced via SLURM in the future.</p>"},{"location":"services/ultra2/run/#example-mpi-job","title":"Example MPI job","text":"<p>An example script to run a multi-process MPI \"Hello world\" example is shown.</p> <pre><code>#!/usr/bin/env bash\n#SBATCH -J HelloWorld\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=4\n#SBATCH --nodelist=sdf-cs1\n#SBATCH --partition=standard\n##SBATCH --exclusive\n\n\necho \"Running on host ${HOSTNAME}\"\necho \"Using ${SLURM_NTASKS_PER_NODE} tasks per node\"\necho \"Using ${SLURM_CPUS_PER_TASK} cpus per task\"\nlet mpi_threads=${SLURM_NTASKS_PER_NODE}*${SLURM_CPUS_PER_TASK}\necho \"Using ${mpi_threads} MPI threads\"\n\n# Source oneAPI to ensure mpirun available\nif [[ -z \"${SETVARS_COMPLETED}\" ]]; then\nsource /opt/intel/oneapi/setvars.sh\nfi\n\n# mpirun invocation for Intel suite.\nmpirun -n ${mpi_threads} ./helloworld.exe\n</code></pre>"},{"location":"services/virtualmachines/","title":"Overview","text":"<p>The EIDF Virtual Machine (VM) Service is the underlying infrastructure upon which the EIDF Data Science Cloud (DSC) is built.</p> <p>The service currently has a mixture of hardware node types which host VMs of various flavours:</p> <ul> <li>The mcomp nodes which host general flavour VMs are based upon AMD EPYC 7702 CPUs (128 Cores) with 1TB of DRAM</li> <li>The hcomp nodes which host capability flavour VMs are based upon 4x Intel Xeon Platinum 8280L CPUs (224 Threads, 112 cores with HT) with 3TB of DRAM</li> <li>The GPU nodes which host GPU flavour VMs are based upon 2x Intel Xeon Platinum 8260 CPUs (96 Cores) with 4x Nvidia Tesla V100S 32GB and 1.5TB of DRAM</li> </ul> <p>The shapes and sizes of the flavours are based on subdivisions of this hardware, noting that CPUs are 4x oversubscribed for mcomp nodes (general VM flavours).</p>"},{"location":"services/virtualmachines/#service-access","title":"Service Access","text":"<p>Users should have an EIDF account - EIDF Accounts.</p> <p>Project Leads will be able to have access to the DSC added to their project during the project application process or through a request to the EIDF helpdesk.</p>"},{"location":"services/virtualmachines/#additional-service-policy-information","title":"Additional Service Policy Information","text":"<p>Additional information on service policies can be found here.</p>"},{"location":"services/virtualmachines/docs/","title":"Service Documentation","text":""},{"location":"services/virtualmachines/docs/#project-management-guide","title":"Project Management Guide","text":""},{"location":"services/virtualmachines/docs/#create-a-vm","title":"Create a VM","text":"<p>To create a new VM:</p> <ol> <li>Open the projects page in the portal at https://portal.eidf.ac.uk/project/    or select 'Your Projects' from the Projects menu.</li> <li>Select the project from the list of your projects, e.g. <code>eidfxxx</code></li> <li>Click on the 'New Machine' button</li> <li> <p>Complete the 'Create Machine' form as follows:</p> <ol> <li>Provide an appropriate name, e.g. <code>dev-01</code>. The project code will be prepended automatically     to your VM name, in this case your VM would be named <code>eidfxxx-dev-01</code>.</li> <li>Select a suitable operating system</li> <li>Select a machine specification that is suitable</li> <li>Choose the required disk size (in GB) or leave blank for the default</li> <li>Tick the checkbox \"Configure RDP access\" if you would like to install RDP    and configure VDI connections via RDP for your VM.</li> <li>Select the package installations from the software catalogue drop-down list,    or \"None\" if you don't require any pre-installed packages</li> </ol> </li> <li> <p>Click on 'Create'</p> </li> <li>You should see the new VM listed under the 'Machines' table on the project page and the status as 'Creating'</li> <li>Wait while the job to launch the VM completes.    This may take up to 10 minutes, depending on the configuration you requested.    You have to reload the page to see updates.</li> <li>Once the job has completed successfully the status shows as 'Active' in the list of machines.</li> </ol> <p>You may wish to ensure that the machine size selected (number of CPUs and RAM) does not exceed your remaining quota before you press Create, otherwise the request will fail.</p> <p>In the list of 'Machines' in the project page in the portal, click on the name of new VM to see the configuration and properties, including the machine specification, its <code>10.24.*.*</code> IP address and any configured VDI connections.</p>"},{"location":"services/virtualmachines/docs/#quota-and-usage","title":"Quota and Usage","text":"<p>Each project has a quota for the number of instances, total number of vCPUs, total RAM and storage. You will not be able to create a VM if it exceeds the quota.</p> <p>You can view and refresh the project usage compared to the quota in a table near the bottom of the project page. This table will be updated automatically when VMs are created or removed, and you can refresh it manually by pressing the \"Refresh\" button at the top of the table.</p> <p>Please contact the helpdesk if your quota requirements have changed.</p>"},{"location":"services/virtualmachines/docs/#add-a-user-account","title":"Add a user account","text":"<p>User accounts allow project members to log in to the VMs in a project. The Project PI and project managers manage user accounts for each member of the project. Users usually use one account (username and password) to log in to all the VMs in the same project that they can access, however a user may have multiple accounts in a project, for example for different roles.</p> <ol> <li>Select your project at https://portal.eidf.ac.uk/project/.</li> <li>From the project page in the portal click on the 'Create account' button under the 'Project Accounts' table at the bottom</li> <li> <p>Complete the 'Create User Account' form as follows:</p> <ol> <li>Choose 'Account user name': this could be something sensible like the first and last names concatenated (or initials) together with the project name. The username is unique across all EPCC systems so the user will not be able to reuse this name in another project once it has been assigned.</li> <li>Select the project member from the 'Account owner' drop-down field</li> <li>Click 'Create'</li> </ol> </li> </ol> <p>The user can now set the password for their new account on the account details page.</p>"},{"location":"services/virtualmachines/docs/#adding-access-to-the-vm-for-a-user","title":"Adding Access to the VM for a User","text":"<p>User accounts can be granted or denied access to existing VMs.</p> <ol> <li>Click 'Manage' next to an existing user account in the 'Project Accounts' table on the project page, or click on the account name and then 'Manage' on the account details page</li> <li>Select the checkboxes in the column \"Access\" for the VMs to which this account should have access or uncheck the ones without access</li> <li>Click the 'Update' button</li> <li>After a few minutes, the job to give them access to the selected VMs will complete and the account status will show as \"Active\".</li> </ol> <p>If a user is logged in already to the VDI at https://eidf-vdi.epcc.ed.ac.uk/vdi newly added connections may not appear in their connections list immediately. They must log out and log in again to refresh the connection information, or wait until the login token expires and is refreshed automatically - this might take a while.</p> <p>If a user only has one connection available in the VDI they will be automatically directed to the VM with the default connection.</p>"},{"location":"services/virtualmachines/docs/#sudo-permissions","title":"Sudo permissions","text":"<p>A project manager or PI may also grant sudo permissions to users on selected VMs. Management of sudo permissions must be requested in the project application - if it was not requested or the request was denied the functionality described below is not available.</p> <ol> <li>Click 'Manage' next to an existing user account in the 'Project Accounts' table on the project page</li> <li>Select the checkboxes in the column \"Sudo\" for the VMs on which this account is granted sudo permissions or uncheck to remove permissions</li> <li>Make sure \"Access\" is also selected for the sudo VMs to allow login</li> <li>Click the 'Update' button</li> </ol> <p>After a few minutes, the job to give the user account sudo permissions on the selected VMs will complete. On the account detail page a \"sudo\" badge will appear next to the selected VMs.</p> <p>Please contact the helpdesk if sudo permission management is required but is not available in your project.</p>"},{"location":"services/virtualmachines/docs/#first-login","title":"First login","text":"<p>A new user account must reset the password before they can log in for the first time. To do this:</p> <ol> <li>The user can log into the Portal and select their project from the 'Projects' drop down.</li> <li>From the project page, they can select their account from the 'Your Accounts' table</li> <li>Finally, click the 'Set Password' button from the 'User Account Info' table.</li> </ol>"},{"location":"services/virtualmachines/docs/#updating-an-existing-machine","title":"Updating an existing machine","text":""},{"location":"services/virtualmachines/docs/#adding-rdp-access","title":"Adding RDP Access","text":"<p>If you did not select RDP access when you created the VM you can add it later:</p> <ol> <li>Open the VM details page by selecting the name on the project page</li> <li>Click on 'Configure RDP'</li> <li>The configuration job runs for a few minutes.</li> </ol> <p>Once the RDP job is completed, all users that are allowed to access the VM will also be permitted to use the RDP connection.</p>"},{"location":"services/virtualmachines/docs/#software-catalogue","title":"Software catalogue","text":"<p>You can install packages from the software catalogue at a later time, even if you didn't select a package when first creating the machine.</p> <ol> <li>Open the VM details page by selecting the name on the project page</li> <li>Click on 'Software Catalogue'</li> <li>Select the configuration you wish to install and press 'Submit'</li> <li>The configuration job runs for a few minutes.</li> </ol>"},{"location":"services/virtualmachines/docs/#resize-a-machine","title":"Resize a machine","text":"<p>An existing machine can be resized within the allowance of the project. This requires a reboot of your machine.</p> <ol> <li>Open the VM details page</li> <li>Click on 'Resize'</li> <li>In the form (as shown below), select a suitable machine specification.    The form will indicate the maximum number of cores and memory size that is available within your quota.      Example page to resize a machine from 2 cores and 4 GB RAM to 8 cores and 16GB RAM</li> <li>Click 'Submit'</li> </ol> <p>Please wait for the job to complete: Resizing takes a few minutes. After the automated reboot the machine will be available with the new specifications.</p> <p>If your quota does not allow resizing the machine to the desired number of cores or memory you can request additional resources through the EIDF Helpdesk. Within the form above please select the category 'EIDF Project extension: duration and quota'.</p>"},{"location":"services/virtualmachines/docs/#resize-the-boot-volume-of-a-machine","title":"Resize the boot volume of a machine","text":"<p>The disk of an existing machine can be resized within the allowance of the project. This requires a reboot of your machine.</p> <ol> <li>Open the VM details page</li> <li>Click on 'Expand Disk'</li> <li>In the form, enter the new disk size for your machine.    The form will indicate the maximum size that is available within your quota.</li> <li>Click 'Submit'</li> </ol> <p>Please wait for the job to complete: Expanding the disk takes a few minutes. After the job completes, the machine needs to be rebooted to expand the file system. You can do this by executing <code>sudo reboot now</code> from a terminal on the VM, or click 'Reboot' on the machine details page in the Portal.</p> <p>Please note that you cannot decrease the disk size.</p> <p>You can request additional storage through the EIDF Helpdesk. Within the form above please select the category 'EIDF Project extension: duration and quota'.</p>"},{"location":"services/virtualmachines/docs/#patching-and-updating","title":"Patching and updating","text":"<p>It is the responsibility of project PIs to keep the VMs in their projects up to date as stated in the policy.</p>"},{"location":"services/virtualmachines/docs/#ubuntu","title":"Ubuntu","text":"<p>To patch and update packages on Ubuntu run the following commands (requires sudo permissions):</p> <pre><code>sudo apt update\nsudo apt upgrade\n</code></pre> <p>Your system might require a restart after installing updates.</p>"},{"location":"services/virtualmachines/docs/#rocky","title":"Rocky","text":"<p>To patch and update packages on Rocky run the following command (requires sudo permissions):</p> <pre><code>sudo dnf update\n</code></pre> <p>Your system might require a restart after installing updates.</p>"},{"location":"services/virtualmachines/docs/#reboot","title":"Reboot","text":"<p>When logged in you can reboot a VM with this command (requires sudo permissions):</p> <pre><code>sudo reboot now\n</code></pre> <p>or use the reboot button in the EIDF Portal (requires project manager permissions).</p>"},{"location":"services/virtualmachines/docs/#required-member-permissions","title":"Required Member Permissions","text":"<p>VMs and user accounts can only be managed by project members with Cloud Admin permissions. This includes the principal investigator (PI) of the project and all project managers (PM). Through SAFE the PI can designate project managers and the PI and PMs can grant a project member the Cloud Admin role:</p> <ol> <li>Click \"Manage Project in SAFE\" at the bottom of the project page (opens a new tab)</li> <li>On the project management page in SAFE, scroll down to \"Manage Members\"</li> <li>Click Add project manager or Set member permissions</li> </ol> <p>For details please refer to the SAFE documentation: How can I designate a user as a project manager?</p>"},{"location":"services/virtualmachines/flavours/","title":"Flavours","text":"<p>These are the current Virtual Machine (VM) flavours (configurations) available on the the Virtual Desktop cloud service. Note that all VMs are built and configured using the EIDF Portal by PIs/Cloud Admins of projects, except GPU flavours which must be requested via the helpdesk or the support request form.</p> Flavour Name vCPUs DRAM in GB Pinned Cores GPU general.v2.tiny 1 2 No No general.v2.small 2 4 No No general.v2.medium 4 8 No No general.v2.large 8 16 No No general.v2.xlarge 16 32 No No capability.v2.8cpu 8 112 Yes No capability.v2.16cpu 16 224 Yes No capability.v2.32cpu 32 448 Yes No capability.v2.48cpu 48 672 Yes No capability.v2.64cpu 64 896 Yes No gpu.v1.8cpu 8 128 Yes Yes gpu.v1.16cpu 16 256 Yes Yes gpu.v1.32cpu 32 512 Yes Yes gpu.v1.48cpu 48 768 Yes Yes"},{"location":"services/virtualmachines/policies/","title":"EIDF Data Science Cloud Policies","text":""},{"location":"services/virtualmachines/policies/#end-of-life-policy-for-user-accounts-and-projects","title":"End of Life Policy for User Accounts and Projects","text":""},{"location":"services/virtualmachines/policies/#what-happens-when-an-account-or-project-is-no-longer-required-or-a-user-leaves-a-project","title":"What happens when an account or project is no longer required, or a user leaves a project","text":"<p>These situations are most likely to come about during one of the following scenarios:</p> <ol> <li>The retirement of project (usually one month after project end)</li> <li>A Principal Investigator (PI) tidying up a project requesting the removal of user(s) no longer working on the project</li> <li>A user wishing their own account to be removed</li> <li>A failure by a user to respond to the annual request to verify their email address held in the SAFE</li> </ol> <p>For each user account involved, assuming the relevant consent is given, the next step can be summarised as one of the following actions:</p> <ul> <li>Removal of the EIDF account</li> <li>The re-owning of the EIDF account within an EIDF project (typically to PI)</li> <li>In addition, the corresponding SAFE account may be retired under scenario 4</li> </ul> <p>It will be possible to have the account re-activated up until resources are removed (as outlined above); after this time it will be necessary to re-apply.</p> <p>A user's right to use EIDF is granted by a project. Our policy is to treat the account and associated data as the property of the PI as the owner of the project and its resources. It is the user's responsibility to ensure that any data they store on the EIDF DSC is handled appropriately and to copy off anything that they wish to keep to an appropriate location.</p> <p>A project manager or the PI can revoke a user's access accounts within their project at any time, by locking, removing or re-owning the account as appropriate.</p> <p>A user may give up access to an account and return it to the control of the project at any time.</p> <p>When a project is due to end, the PI will receive notification of the closure of the project and its accounts one month before all project accounts and DSC resources (VMs, data volumes) are closed and cleaned or removed.</p>"},{"location":"services/virtualmachines/policies/#backup-policies","title":"Backup policies","text":"<p>The current policy is:</p> <ul> <li>The content of VM disk images is not backed up</li> <li>The VM disk images are not backed up</li> </ul> <p>We strongly advise that you keep copies of any critical data on an alternative system that is fully backed up.</p>"},{"location":"services/virtualmachines/policies/#patching-of-user-vms","title":"Patching of User VMs","text":"<p>The EIDF team updates and patches the hypervisors and the cloud management software as part of the EIDF Maintenance sessions. It is the responsibility of project PIs to keep the VMs in their projects up to date. VMs running the Ubuntu and Rocky operating systems automatically install security patches and alert users at log-on (via SSH) to reboot as necessary for the changes to take effect. They also encourage users to update packages.</p>"},{"location":"services/virtualmachines/policies/#customer-run-outward-facing-web-services","title":"Customer-run outward facing web services","text":"<p>PIs can apply to run an outward-facing service; that is a webservice on port 443, running on a project-owned VM. The policy requires the customer to accept the following conditions:</p> <ul> <li>Agreement that the customer will automatically apply security patches, run regular maintenance, and have named contacts who can act should we require it.</li> <li>Agreement that should EPCC detect any problematic behaviour (of users or code), we reserve the right to remove web access.</li> <li>Agreement that the customer understands all access is filtered and gated by EPCC\u2019s Firewalls and NGINX (or other equivalent software) server such that there is no direct exposure to the internet of their application.</li> <li>Agreement that the customer owns the data, has permission to expose it, and that it will not bring UoE into disrepute.</li> </ul> <p>Pis can apply for such a service on application and also at any time by contacing the EIDF Service Desk.</p>"},{"location":"services/virtualmachines/quickstart/","title":"Quickstart","text":"<p>Projects using the Virtual Desktop cloud service are accessed via the EIDF Portal.</p> <p>Authentication is provided by SAFE, so if you do not have an active web browser session in SAFE, you will be redirected to the SAFE log on page. If you do not have a SAFE account follow the instructions in the SAFE documentation how to register and receive your password.</p>"},{"location":"services/virtualmachines/quickstart/#accessing-your-projects","title":"Accessing your projects","text":"<ol> <li> <p>Log into the portal at https://portal.eidf.ac.uk/.    The login will redirect you to the SAFE.</p> </li> <li> <p>View the projects that you have access to    at https://portal.eidf.ac.uk/project/</p> </li> </ol>"},{"location":"services/virtualmachines/quickstart/#joining-a-project","title":"Joining a project","text":"<ol> <li> <p>Navigate to https://portal.eidf.ac.uk/project/    and click the link to \"Request access\", or choose \"Request Access\" in the \"Project\" menu.</p> </li> <li> <p>Select the project that you want to join in the \"Project\" dropdown list -    you can search for the project name or the project code, e.g. \"eidf0123\".</p> </li> </ol> <p>Now you have to wait for your PI or project manager to accept your request to join.</p>"},{"location":"services/virtualmachines/quickstart/#accessing-a-vm","title":"Accessing a VM","text":"<ol> <li> <p>Select a project and view your user accounts on the project page.</p> </li> <li> <p>Click on an account name to view details of the VMs that are you allowed to access    with this account, and to change the password for this account.</p> </li> <li> <p>Before you log in for the first time with a new user account, you must change your password as described    below.</p> </li> <li> <p>Follow the link to the Guacamole login or    log in directly at https://eidf-vdi.epcc.ed.ac.uk/vdi/.    Please see the VDI guide for more information.</p> </li> <li> <p>You can also log in via the EIDF Gateway Jump Host    if this is available in your project.</p> </li> </ol> <p>Warning</p> <p>You must set a password for a new account before you log in for the first time.</p>"},{"location":"services/virtualmachines/quickstart/#set-or-change-the-password-for-a-user-account","title":"Set or change the password for a user account","text":"<p>Follow these instructions to set a password for a new account before you log in for the first time. If you have forgotten your password you may reset the password as described here.</p> <ol> <li> <p>Select a project and click the account name in the project page to view the account details.</p> </li> <li> <p>In the user account detail page, press the button \"Set Password\"    and follow the instructions in the form.</p> </li> </ol> <p>There may be a short delay while the change is implemented before the new password becomes usable.</p>"},{"location":"services/virtualmachines/quickstart/#further-information","title":"Further information","text":"<p>Managing VMs: Project management guide to creating, configuring and removing VMs and managing user accounts in the portal.</p> <p>Virtual Desktop Interface: Working with the VDI interface.</p> <p>EIDF Gateway: SSH access to VMs via the EIDF SSH Gateway jump host.</p>"},{"location":"services/virtualmachines/rdp-tunnelling/","title":"RDP tunnelling over SSH","text":"<p>RDP tunneling is a technique that relies on SSH to create a secure channel that forwards local traffic to a remote server's RDP port. The content of the local traffic in RDP tunneling includes:</p> <ul> <li>User Input: Keyboard and mouse actions.</li> <li>Display Data: Screen updates and graphical interface data.</li> <li>Clipboard Contents: Text or files copied and pasted between the local and remote systems.</li> <li>File Transfers: Transferred files if drive redirection is enabled.</li> </ul> <p>By setting up an SSH tunnel, a local port is forwarded to the remote server's RDP port (3389). The RDP client is then connected to <code>localhost:&lt;local_port&gt;</code>, ensuring that the RDP session is encrypted.</p>"},{"location":"services/virtualmachines/rdp-tunnelling/#ssh-commands","title":"SSH Commands","text":""},{"location":"services/virtualmachines/rdp-tunnelling/#steps","title":"Steps","text":"<ol> <li> <p>Jump host (-J): connect first to the eidf_gateway as an intermediary before reaching the target VM. Authentication is achieved using the identity file provided by the host (-i).</p> </li> <li> <p>Local Port Forwarding (-L): Forwards local port 12345 to localhost:3389 on the remote machine, allowing RDP access via localhost:12345.</p> </li> <li> <p>Configure a RDP client (for example 'Windows App') to connect to <code>localhost:&lt;local_forwarded_port&gt;</code> instead of directly accessing the remote machine's IP. Then connect on the remote server as 'username'.</p> </li> </ol>"},{"location":"services/virtualmachines/rdp-tunnelling/#example","title":"Example","text":"<p>Connect to eidf666 (IP address 10.24.2.224) as user u666</p> <pre><code>ssh -J u666@eidf-gateway.epcc.ed.ac.uk -L 23001:localhost:3389 u666@10.24.2.224\n</code></pre> <p>Defaults like PubkeyAuthentication=yes, PasswordAuthentication=yes, ForwardAgent=yes, ForwardX11=yes, and ForwardX11Trusted=yes are omitted.</p> <p>Once the connection is established, on Windows App or similar use pc name: <code>localhost:23001</code>. When prompted, the username will be 'u666' with corresponding password.</p>"},{"location":"services/virtualmachines/rdp-tunnelling/#ssh-configuration","title":"SSH Configuration","text":"<p>All of the above can also be achieved adding the following to the SSH <code>.config</code> file:</p> <pre><code>host eidf666_gateway\n    Hostname eidf-gateway.epcc.ed.ac.uk\n    User u666\n    IdentityFile ~/.ssh/eidf666-vm\n\nhost eidf666_rdp\n    Hostname 10.24.2.224\n    User u666\n    PubKeyAuthentication yes\n    PasswordAuthentication yes\n    IdentityFile ~/.ssh/eidf666-vm\n    ProxyJump eidf666_gateway\n    ServerAliveInterval 900\n    ForwardAgent yes\n    ForwardX11 yes\n    ForwardX11Trusted yes\n    LocalForward 23001 localhost:3389\n</code></pre> <p>Can then use the command 'ssh eidf666_rdp' to create the connection.</p>"},{"location":"services/virtualmachines/rdp-tunnelling/#using-rdp-tunnelling-to-connect-to-vms-remote-desktops-via-a-rdp-client","title":"Using RDP Tunnelling to connect to VMs Remote Desktops via a RDP client","text":"<p>The Guacamole web interface makes it easy to connect to a remote desktop. This page details how to connect with a dedicated remote desktop client application, which can be useful for longer work. Dedicated remote desktop viewer programs allow better support for copy and pasting, file sharing and other features that may not be available on the web interface.</p> <p>Prerequisites</p> <p>It is recommended to connect to the host machine via Guacamole first as this process is simpler see instructions in Virtual Machines (VMs) and the EIDF Virtual Desktop Interface (VDI)</p> <p>We refer to a Host machine as the machine with an RDP server running on it, within EIDF infrastructure this is the Virtual machine.</p> <p>The Client machine is any machine that you are using to connect to the host machine via the RDP protocol and remote desktop viewing software.</p> <p>Warning</p> <p>Opening a remote connection means having full access to the host machines as though it was unlocked on your desk. You should not provide access credentials to other users you would not want using the machine or leave your client machine unattended and unlocked whilst connected as this risks the EIDF Virtual Machine being accessed by unauthorized users!</p> <p>Using RDP tunnelling allows you to connect to a remote desktop machine securely through an SSH tunnel. This is particularly useful when accessing Virtual Desktop Infrastructure (VDI) machines on the EIDF.</p> <p>Note</p> <p>Port forwarding is needed to get through the EIDF gateway. Whilst some remote desktop viewers support gateways they do not always support all the required features for connection to the EIDF gateway (e.g. the Windows app for macOS does not support SSH keys)</p> <p>Instructions specific to the operating system in use are given in the below sections. If you have your own preferred remote desktop viewing software or cannot use the below instructions for whatever reason then the key details to be configured are:</p> <pre><code>PC or server name and port: localhost:23001\nVDI account credentials: We recommend leaving this blank or as 'Ask when\nrequired' until you have successfully connected to the machine. If prompted for\ncredentials you will need those which you have for VDI connections, the same as\nin Guacamole\nProtocol: RDP\nFriendly Name: &lt;ProjectID&gt; Remote Connection via local port\n</code></pre>"},{"location":"services/virtualmachines/rdp-tunnelling/#windows","title":"Windows","text":"<p>The Microsoft Windows documentation Remote Desktop Connection is relevant from step 2 \"Use Remote Desktop to connect to the PC you set up\" onwards. The Remote Desktop Connection program is preinstalled on Windows machines.</p> <ol> <li>Open the 'Remote Desktop Connection' program (<code>mstsc</code> in Run)</li> <li>Input <code>localhost:&lt;port forwarded to&gt;</code> e.g. <code>localhost:23001</code> as the connection name</li> <li>Click 'Connect'</li> <li>Input VDI login credentials once prompted at the login screen</li> </ol>"},{"location":"services/virtualmachines/rdp-tunnelling/#macos","title":"macOS","text":"<p>These instructions use the Microsoft Windows App a free program available from the Apple App Store.</p> <ol> <li>Open the Windows App -&gt; Click '+' -&gt; Add PC</li> <li>Input into 'PC Name' the forwarded port on the local machine <code>localhost:&lt;port forwarded to&gt;</code> e.g. <code>localhost:23001</code></li> <li>Also add a 'friendly name' to describe the device being connected to</li> <li>Leave all other options as defaults at this stage</li> <li>Click Add</li> <li>Double-click on the newly created PC to connect</li> <li>You will be prompted for the VDI username and password</li> </ol>"},{"location":"services/virtualmachines/rdp-tunnelling/#linux","title":"Linux","text":"<p>The following uses Remmina, a stable Linux RDP client available via most package managers.</p> <ol> <li>Install and open Remmina</li> <li>Select \"Add a new connection profile\" in the top right for the connection profile window to appear</li> <li>Ensure that 'Protocol' is set to RDP</li> <li>In the 'Server' field input the localhost and forwarded port <code>localhost:&lt;port forwarded to&gt;</code> e.g. <code>localhost:23001</code></li> <li>Enter a 'Friendly Name' to describe the device being connected to</li> <li>Leave the 'Username' and 'Password' fields blank at this stage. After your first successful connection, save your VDI credentials here for easier connection in the future</li> <li>Click Save and Connect to start the remote desktop connection. You will be able to double-click on the friendly name in the main Remmina pane the next time you want to connect to the machine</li> </ol>"},{"location":"services/virtualmachines/sharedfs/","title":"Shared Filesystem (CephFS)","text":""},{"location":"services/virtualmachines/sharedfs/#introduction","title":"Introduction","text":"<p>EIDF allows projects with storage space allocated on our CephFS filesystem to access that space on different compute resources such as the EIDF Virtual Machines Service (VMS).</p> <p>On VMs, mounting is done at a project level, i.e. the project subtree part of CephFS is mounted. For users who have used our Cerebras or Cirrus services, <code>/home/eidf124</code> on Cirrus is the same as <code>/home/eidf124</code> on the VM, and users can manipulate files in that directory. However, such VMs would not typically have access to <code>/home/eidf421</code> or other project directories.</p> <p>Note</p> <p>Changing file properties on CephFS on a VM will cause the effect to be visible wherever the file is visible, ie on other systems.</p> <p>If you set a file stored on CephFS to be world readable on your VM, it will also be world readable (i.e. viewable by any user) on shared-access systems such as Cerebras or Cirrus. This also applies to files in the top-level shared directory. For example, files in <code>/home/eidf124/shared</code> are visible wherever <code>/home/eidf124</code> is mounted.</p>"},{"location":"services/virtualmachines/sharedfs/#pre-requisites","title":"Pre-requisites","text":"<p>Warning</p> <p>It is not possible to mount CephFS on VMs running Ubuntu 20.04 LTS due to incompatible driver packages. We recommend upgrading to Ubuntu 22.04 LTS, or later, before trying to mount CephFS.</p> <p>To mount CephFS on a VM:</p> <ol> <li>The project must have space allocated on CephFS.</li> <li>A mount key must be created exist. PI's should contact the EIDF helpdesk to have this created.</li> </ol> <p>If both pre-requisites are met, PIs and PMs will be able to see the Mount button under CephFS Mounts in the project management page in the EIDF portal.</p> <p> Mount button displayed in project management page</p>"},{"location":"services/virtualmachines/sharedfs/#mounting-cephfs-on-a-vm","title":"Mounting CephFS on a VM","text":"<p>When CephFS is mounted on a VM, it can become the home directory for users logging into that VM. PIs and PMs can enable this mounting for individual VMs via the management page in the Portal.</p> <p>Warning</p> <p>Mounting CephFS on a VM of a project where project users have superuser (<code>sudo</code>) permissions will allow these project users to see and manipulate files on the CephFS belonging to all other project users.</p> <p>Note</p> <p>If there is existing data in the VM-local <code>/home</code>, it will be moved to <code>/local-home</code> before mounting. You will have to manually move this data to the mounted home if you wish it accessible by users without superuser permissions. If your VM runs Ubuntu and you don't mount at <code>/home</code> the target location MUST be empty. A non-empty mountpoint will cause the mount to fail.</p> <p>To mount CephFS on a VM:</p> <ol> <li>Click the Mount button in the project management page under CephFS (see screenshot above).</li> <li>Follow the instructions, selecting either to mount as <code>/home</code> or specifying an alternative mount location</li> <li>Select the VMs on which to mount CephFS at the specified location.</li> <li>Click Submit</li> </ol> <p> Example mounting page for project eidf124</p>"},{"location":"status/","title":"EIDF Service Status","text":"<p>The table below represents the broad status of each EIDF service.</p> Service Status EIDF Portal VM SSH Gateway VM VDI Gateway Virtual Desktops Ultra2"},{"location":"status/#advanced-computing-facility-acf-power-outage-friday-29th-august-monday-15th-september","title":"Advanced Computing Facility (ACF) Power Outage: Friday 29th August - Monday 15th September","text":"<p>Due to a significant Health and Safety risk, associated with our power supply to the site, that requires action at the ACF, there will be a full power outage to the site from Friday 29th August - Monday 15th September.  Specialised external contractors will be working on a 24/7 basis for the outage period replacing switchgear. The EIDF Services are hosted at the ACF.</p>"},{"location":"status/#eidf-user-impact","title":"EIDF User Impact","text":"<p>The EIDF Services will be completely powered off for the duration of this period.</p> <p>The EIDF Services impacted are:</p> <ul> <li>EIDF Cerebras Service</li> <li>EIDF Data Catalogue</li> <li>EIDF Data Publishing Service</li> <li>EIDF GPU Service</li> <li>EIDF Jupyter Notebook</li> <li>EIDF Portal</li> <li>EIDF S3 Service</li> <li>EIDF Ultra2 Service</li> <li>EIDF Virtual Desktops</li> </ul> <p>Users will not be able to connect to any of the EIDF Services. Data will be stored safely but users will not be able to access data during the work.  Where appropriate, services will be drained of jobs ahead of the power outage and jobs will not run during this period. Any queued jobs will remain in the queue during the outage and jobs will start once the service is returned.</p> <p>The EIDF website will be available during the outage period and updates will be provided during the outage at https://docs.eidf.ac.uk/status/.</p> <p>The site will be handed back to EPCC on Monday 15th September and we will work to return services thereafter. We will notify users once services are available.</p> <p>We apologise for the inconvenience of this essential outage. Please contact eidf@epcc.ed.ac.uk if you have any questions.</p>"},{"location":"status/#maintenance-sessions","title":"Maintenance Sessions","text":"<p>There will be a service outage on the 3rd Thursday of every month from 9am to 5pm. We keep maintenance downtime to a minimum on the service but do occasionally need to perform essential work on the system. Maintenance sessions are used to ensure that:</p> <ul> <li>software versions are kept up to date;</li> <li>firmware levels on the underlying hardware are kept up to date;</li> <li>essential security patches are applied;</li> <li>failed/suspect hardware can be replaced;</li> <li>new software can be installed; periodic essential maintenance on electrical and mechanical support equipment (cooling systems and power distribution units) can be undertaken safely.</li> </ul> <p>The service will be returned to service ahead of 5pm if all the work is completed early.</p>"},{"location":"storage/faq/","title":"Storage FAQs","text":""},{"location":"storage/faq/#i-have-deleted-my-important-file-what-can-i-do","title":"I have deleted my important file, what can I do?","text":"<p>The EIDF does not offer a backup service and only replicates Ultra2-local storage.</p> <p>Whilst EIDF Ultra2 storage is replicated to tape in case of system failure or data loss related to the running of the service, we do not offer or plan to offer the ability to restore files from this medium in the case of user error.</p> <p>Users are responsible for backing up their data against user error.</p>"},{"location":"storage/faq/#i-want-to-access-storage-from-across-all-eidf-services","title":"I want to access storage from across all EIDF services","text":"<p>The Shared Filesystem (CephFS) is available throughout EIDF services.</p> <p>We are actively working to improve cross-service accessibility of S3. Currently the only remaining issue is that S3 is not accessible from the Cerebras processing unit.</p>"},{"location":"storage/faq/#i-want-to-make-my-data-available-outside-eidf-what-storage-should-i-use","title":"I want to make my data available outside EIDF, what storage should I use?","text":"<p>The Data Publishing Service provides public read access for data to be shared outside of the EIDF infrastructure.</p> <p>S3 storage is sharable outside EIDF infrastructure. The EIDF S3 service is accessible from anywhere in the world via S3-compatible workflows. General purpose usage outside of EIDF is documented on Amazon's S3 documentation.</p>"},{"location":"storage/faq/#i-want-to-process-multi-terabyte-data-on-multiple-vms-and-potentially-other-eidf-services-what-storage-should-i-use","title":"I want to process multi-terabyte data on multiple VMs and potentially other EIDF services, what storage should I use?","text":"<p>You should use the Shared Filesystem (CephFS) or S3. We do not recommend using large, specific attached storage for VMs. This type of storage is not sharable between machines in the EIDF and relies on the same underlying system.</p> <p>PIs can request access to Shared Filesystem (CephFS) and can configure it to be available on their VMs by following the documentation in the Virtual Machines section.</p>"},{"location":"storage/faq/#i-want-to-use-the-gpu-service-what-storage-do-i-need","title":"I want to use the GPU Service, what storage do I need?","text":"<p>The GPU Service has persistent storage attached, requested computationally by the user. Instructions on the format of this request can be found on the GPU Service Tutorials page</p> <p>S3 storage can be used from the GPU Service. This is useful when data is to be shared across EIDF Services and beyond.</p> <p>The GPU Service is also able to use the EIDF Shared Filesystem (CephFS), which can share data across EIDF services, but not beyond. For more information on this please refer to Shared Filesystem (CephFS) PVCs</p>"},{"location":"storage/faq/#i-want-to-use-cerebras-what-storage-do-i-need","title":"I want to use Cerebras, what storage do I need?","text":"<p>Cerebras only uses Shared Filesystem (CephFS). PIs can request access to Shared Filesystem (CephFS) and can configure it to be available on their VMs by following the documentation in the Virtual Machines section.</p>"},{"location":"storage/faq/#i-want-to-use-more-storage","title":"I want to use more storage?","text":"<p>If you need more storage than what is available by default, you can request additional storage through the EIDF Helpdesk. Please provide details about your requirements and the intended use of the storage. Within the form above please select the category 'EIDF Project extension: duration and quota'.</p>"},{"location":"storage/faq/#i-want-to-use-notebooks-and-a-vm-what-storage-do-i-need","title":"I want to use Notebooks and a VM, what storage do I need?","text":"<p>Both the Notebook Service and VM Service have their own local storage, but these local storage areas are not directly accessible between services. If you need to share data between Notebooks, VMs, and other EIDF services, you should use either the Shared Filesystem (CephFS) or S3 storage. These are the only storage options that allow seamless data sharing across different EIDF services.</p> <p>See more details about the Notebook Service.</p>"},{"location":"storage/faq/#i-have-an-additional-question-not-covered-what-do-i-do","title":"I have an additional question not covered, what do I do?","text":"<p>If you are an existing user of EIDF or other EPCC systems, please submit your queries via our EIDF Helpdesk. Alternatively, you may send your query via email to eidf@epcc.ed.ac.uk.</p>"},{"location":"storage/overview/","title":"EIDF Storage Overview","text":"<p>The EIDF offers the following storage options:</p> <ul> <li>Virtual Machine (VM) local storage</li> <li>GPU local storage</li> <li>Ultra2 local storage</li> <li>S3</li> <li>Shared Filesystem (CephFS)</li> <li>Data Publishing Service</li> </ul> <p>These storage options are backed by the hardware described on this page</p>"},{"location":"storage/overview/#summary-of-storage-options-and-access","title":"Summary of Storage Options and Access","text":"Service VM Local GPU Local Ultra2 Local S3 Shared Filesystem (CephFS) Data Publishing Service VMs Yes No No Yes Yes Yes GPU Service No Yes No Yes Yes Yes Ultra2 No No Yes Yes No Yes Cerebras No No No No Yes No Notebooks No No No Yes Yes Yes"},{"location":"storage/overview/#persistent-volumes-attached-to-specific-services","title":"Persistent Volumes attached to specific services","text":""},{"location":"storage/overview/#virtual-machine-vm-local","title":"Virtual Machine (VM) local","text":"<p>Local storage on VMs is provisioned over CephRBD.  This type of storage is only accessible from the VMs.</p> <p>We recommend using 50 GB of local storage on VMs to ensure sufficient space for the boot partition and software installations. You can request more local storage, but we recommend using either the Shared Filesystem (CephFS) or S3 for storing user data. We will ask you to justify your resource requests for disk space in the multi-terabyte ranges.</p> <p>See more general details about the EIDF Virtual Desktop service.</p>"},{"location":"storage/overview/#gpu-service-local","title":"GPU service local","text":"<p>The EIDF GPU service uses persistent volumes provisioned with CephFS. This is not the same storage as the Shared Filesystem (CephFS) and hence the GPU service persistent volumes are only accessible from the GPU service. Data stored in these volumes outlive the execution of jobs.</p> <p>Currently GPU Service persistent volumes are included in the GPU Service costs.</p> <p>See more details about the GPU service.</p>"},{"location":"storage/overview/#ultra2-local","title":"Ultra2 local","text":"<p>Ultra2 uses our e1000 Lustre parallel filesystem; this storage is not accessible from anywhere else on the EIDF.</p> <p>See more general details about Ultra2.</p>"},{"location":"storage/overview/#storage-available-for-use-on-multiple-eidf-services","title":"Storage available for use on multiple EIDF Services","text":""},{"location":"storage/overview/#s3","title":"S3","text":"<p>The EIDF S3 is directly accessible from the VM, GPU and Ultra2 services.</p> <p>We are working to make S3 accessible from the Cerebras processing unit.</p> <p>S3 is accessible from anywhere in the world via S3-compatible workflows. General purpose usage outside of EIDF is documented on Amazon's S3 documentation</p> <p>See more details about S3 at EIDF.</p>"},{"location":"storage/overview/#shared-filesystem-cephfs","title":"Shared Filesystem (CephFS)","text":"<p>Our Shared Filesystem (CephFS) provides a parallel file system accessible across EIDF services. It is directly accessible from the VM service, GPU service, Ultra2 and Cerebras.</p> <p>Cerebras only uses Shared Filesystem (CephFS). PIs can make it available on their VMs; see how on the documentation mounting CephFS on VMs.</p> <p>The Shared Filesystem (CephFS) is provisioned through CephFS.</p> <p>See more information about the EIDF Shared Filesystem (CephFS).</p>"},{"location":"storage/overview/#data-publishing-service","title":"\u2060Data Publishing Service","text":"<p>This is accessible from all EIDF services and has public read-only access, implemented through Ceph-based S3.</p> <p>The Data Publishing Service is visible from all systems with external network access except the EIDF Cerebras processing-nodes.</p> <p>See more information about the Data Publishing service.</p>"},{"location":"storage/overview/#disaster-recovery","title":"Disaster Recovery","text":"<p>The Ultra2-local storage is replicated onto tape for recovery from disaster. Disaster recovery is triggered by disk hardware failure, not user error. Restoring data due to user actions, such as accidental file deletion, is not supported or planned as a service.</p> <p>There is no user interface for the Disaster Recovery and any action will be taken by the EIDF team in response to issues. If you suspect disk failures on Ultra2, please raise a ticket through the EIDF Portal facility. The EIDF Disaster Recovery is implemented through the HPE Data Management Framework.</p>"},{"location":"tutorial-videos/","title":"Tutorial Video Index","text":"Video Tutorial Series Video Topics Using the EIDF VM Service Accessing the EIDF portal  Introduction to the project dashboard  Interfacing with the VM via the VDI (Remote Desktop)  Interfacing with the VM via the VDI (SSH) SSH-ing into an EIDF VM (Windows) SSH-ing into an EIDF VM (Mac/Linux) Generating an SSH key (Mac/Linux and Windows)  Registering an SSH public key with an EIDF project account  Setting up Multi-Factor Authentication  Verifying SSH access (Mac/Linux and Windows)"}]}